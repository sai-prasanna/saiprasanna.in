<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Learning Longer-term Dependencies in RNNs with Auxiliary Losses (ACL 2018) | λf.(λg.f (g g)) (λg.f (g g)) Sai</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Learning Longer-term Dependencies in RNNs with Auxiliary Losses (ACL 2018)" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Paper by Trieu H. Trinh, Andrew M. Dai, Minh-Thang Luong, Quoc V. Le TLDR; RNNs can learn very long sequences when auxiliary loss for predicting input sequence (forward and backward) from different timesteps is added. Problem being addressed" />
<meta property="og:description" content="Paper by Trieu H. Trinh, Andrew M. Dai, Minh-Thang Luong, Quoc V. Le TLDR; RNNs can learn very long sequences when auxiliary loss for predicting input sequence (forward and backward) from different timesteps is added. Problem being addressed" />
<link rel="canonical" href="https://saiprasanna.in/posts/learning-long-term-dependencies-rnn/" />
<meta property="og:url" content="https://saiprasanna.in/posts/learning-long-term-dependencies-rnn/" />
<meta property="og:site_name" content="λf.(λg.f (g g)) (λg.f (g g)) Sai" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-06-05T03:00:00-05:00" />
<script type="application/ld+json">
{"url":"https://saiprasanna.in/posts/learning-long-term-dependencies-rnn/","@type":"BlogPosting","headline":"Learning Longer-term Dependencies in RNNs with Auxiliary Losses (ACL 2018)","dateModified":"2018-06-05T03:00:00-05:00","datePublished":"2018-06-05T03:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://saiprasanna.in/posts/learning-long-term-dependencies-rnn/"},"description":"Paper by Trieu H. Trinh, Andrew M. Dai, Minh-Thang Luong, Quoc V. Le TLDR; RNNs can learn very long sequences when auxiliary loss for predicting input sequence (forward and backward) from different timesteps is added. Problem being addressed","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://saiprasanna.in/feed.xml" title="λf.(λg.f (g g)) (λg.f (g g)) Sai" /><link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">λf.(λg.f (g g)) (λg.f (g g)) Sai</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About Me</a><a class="page-link" href="/search/">Search</a><a class="page-link" href="/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Learning Longer-term Dependencies in RNNs with Auxiliary Losses (ACL 2018)</h1><p class="post-meta post-meta-title"><time class="dt-published" datetime="2018-06-05T03:00:00-05:00" itemprop="datePublished">
        Jun 5, 2018
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      1 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/categories/#Research Paper Summary">Research Paper Summary</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#Deep learning">Deep learning</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#RNN">RNN</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p><a href="https://arxiv.org/pdf/1803.00144.pdf">Paper</a> by Trieu H. Trinh, Andrew M. Dai,  Minh-Thang Luong,  Quoc V. Le</p>

<p><strong>TLDR;</strong> RNNs can learn very long sequences when auxiliary loss for predicting input sequence (forward and backward) from different timesteps is added.</p>

<h3 id="problem-being-addressed">Problem being addressed</h3>

<p>Recurrent neural nets in theory can learn arbitrarily long sequences, but in practice suffer from problems like vanishing gradients etc. Techniques to reduce vanishing gradients problem like LSTM alone don’t work for very long sequences.</p>

<p>RNN has a better tradeoff when it comes to memory requirements compared to CNN based networks or vanilla <a href="https://arxiv.org/abs/1706.03762">Transformer</a> nets.
When processing very large sequences this becomes important.</p>

<h3 id="proposed-method">Proposed Method</h3>

<p>Take the hidden state of main RNN used for a given task at sampled timesteps, use another RNN at sampled intervals, and try to predict the input sequence to certain time steps. Truncated BPTT (Back propagation through time) to few timesteps would give a new loss.</p>

<h3 id="evaluation-and-results">Evaluation and results</h3>

<p>Evaluation is done on MNIST, CIFAR-10, Stanford dog dataset is given as sequence of pixels to an RNN with classification being the target. Since the pixels are flattened to sequential input, spatial location information is now across whole range of the sequence, requiring long dependencies to be formed to get good results. 
The authors also test it on character based classification on dbpedia. 
This technique achieves very significant results on long sequences compared to existing LSTMs, Transformers etc.</p>

<h3 id="opinions">Opinions</h3>

<p>This paper makes a significant experiment to improve a crucial behavior of RNNs on long sequences. The ablation study is well done. This auxiliary loss reminded me of the <a href="https://worldmodels.github.io/">World models</a>  paper where the task of predicting future states improves current tasks output.</p>


  </div><a class="u-url" href="/posts/learning-long-term-dependencies-rnn/" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Blog on machine learning, NLP, Computer Science, philosophy, economics, animal rights, music, programming languages and endless other things that pique my curiosity.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/sai-prasanna" title="sai-prasanna"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/sai_prasanna" title="sai_prasanna"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
