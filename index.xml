<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>λf.(λg.f (g g)) (λg.f (g g)) Sai</title>
<link>saiprasanna.in/index.html</link>
<atom:link href="saiprasanna.in/index.xml" rel="self" type="application/rss+xml"/>
<description></description>
<generator>quarto-1.1.251</generator>
<lastBuildDate>Sat, 08 Jan 2022 00:00:00 GMT</lastBuildDate>
<item>
  <title>When BERT plays the lottery, All Tickets are winning!</title>
  <link>saiprasanna.in/posts/2022-10-when-bert-plays-the-lottery/index.html</link>
  <description><![CDATA[ 




<p>In 2020, I was working in <a href="zoho.com">Zoho</a> focusing on natural language processing (NLP). It was around time when there was an explosion of self-supervised models such as BERT, RoBERTA etc in NLP. I had some research questions about these pre-trained models. I approached Dr.&nbsp;Anna Rogers, an amazing researcher working on interpretability in twitter. I asked her about some research directions in NLP I was considering. As our interests coincided, she graciously offered to mentor me.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="saiprasanna.in/posts/2022-10-when-bert-plays-the-lottery/bert-lottery.webp" class="img-fluid figure-img" alt="A cartoon of BERT sesame street along with a bandit machine."></p>
<p></p><figcaption class="figure-caption">BERT plays the lottery</figcaption><p></p>
</figure>
</div>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>Twitter is a powerful tool to reach out to top researchers in your field of interest.</p>
</div>
</div>
<p>This resulted in our work <span class="citation" data-cites="prasanna-etal-2020-bert">(Prasanna, Rogers, and Rumshisky 2020)</span>. We analyzed pruned sub-networks in the BERT model in the context of fine-tuning. We used pruning as an approach for interpretability. For a summary about our paper, checkout Anna’s <a href="https://thegradient.pub/when-bert-plays-the-lottery-all-tickets-are-winning/">article</a> in the gradient.pub.</p>
<p>This work also helped me to find my long-term research interest. It shifted from NLP. When I finished submitting our paper for review, I found that in the long-term I would enjoy research in robotics. In the research towards better intelligence, I found myself more fascinated by non-human intelligence in animals, birds and micro-organisms. I think building embodied agents that interact with the world is essential to make progress towards building better agents and even understanding what intelligence is. Simple multi-cellular organisms have robust agential behavior, and trying to build them without relying on human language as a prior would help us understand the fundamental mechanisms essential for intelligence that acts robustly in the real-world.</p>
<p>In the short-term application side, I found that solving NLP tasks have second order effects in improving productivity. But I wanted to work on robots that automate things which have first-order effects. I think this is the classic Asimovian robotics vision which partly inspired me to work on AI.</p>
<p>It’s interesting that long-term research direction in some of the top AI people is currently towards building active agents.</p>
<p>The Turing award winner and one of the leading figures in Deep learning, Prof.&nbsp;Yann LeCun’s <a href="https://ai.facebook.com/blog/yann-lecun-advances-in-ai-research/">current plan</a> on AI is also focused on building active agents.</p>
<p>The co-author of the LSTM paper and other seminal works in AI, Prof.&nbsp;Jürgen Schmidhuber thinks that the next wave of AI applications is going to be in robotics and it might have far larger impact on the world economy than current passive predictive models.</p>
<div class="ratio ratio-16x9"><iframe src="https://www.youtube.com/embed/3FIo6evmweo" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>




<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-prasanna-etal-2020-bert" class="csl-entry">
Prasanna, Sai, Anna Rogers, and Anna Rumshisky. 2020. <span>“<span>W</span>hen <span>BERT</span> <span>P</span>lays the <span>L</span>ottery, <span>A</span>ll <span>T</span>ickets <span>A</span>re <span>W</span>inning.”</span> In <em>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, 3208–29. Online: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/2020.emnlp-main.259">https://doi.org/10.18653/v1/2020.emnlp-main.259</a>.
</div>
</div></section></div> ]]></description>
  <category>NLP</category>
  <guid>saiprasanna.in/posts/2022-10-when-bert-plays-the-lottery/index.html</guid>
  <pubDate>Sat, 08 Jan 2022 00:00:00 GMT</pubDate>
  <media:content url="saiprasanna.in/posts/2022-10-when-bert-plays-the-lottery/bert-lottery.webp" medium="image" type="image/webp"/>
</item>
<item>
  <title>Efficient Dynamic Batching of Large Datasets with Infinibatch</title>
  <link>saiprasanna.in/posts/efficient-dynamic-batching-of-large-datasets-with-infinibatch/index.html</link>
  <description><![CDATA[ 




<p>We will explore how to efficiently batch large datasets with varied sequence length for training using <a href="https://github.com/microsoft/infinibatch/">infinibatch</a>. The focus will be on solving multiple challenges associated with this and making it work with <code>dataloader</code> abstraction in pytorch library. Though our focus is on pytorch, Infinibatch is a pure python library agnostic of the deep learning library.</p>
<p>This post was inspired by this thread on twitter. &gt; twitter: https://twitter.com/marian_nmt/status/1292850875715604480</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>We will use <a href="https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/">wikitext-103</a> dataset as an example. It’s a dataset with sentences from wikipedia. It has 103,227,021 word level tokens in it’s training split. It is used only for illustration, the techniques discussed here are can work for far larger dataset sizes.</p>
</div>
</div>
<div class="cell" data-outputid="b2293107-5487-4715-8c63-0863531cb557" data-execution_count="1">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="op" style="color: #5E5E5E;">!</span>wget https:<span class="op" style="color: #5E5E5E;">//</span>s3.amazonaws.com<span class="op" style="color: #5E5E5E;">/</span>research.metamind.io<span class="op" style="color: #5E5E5E;">/</span>wikitext<span class="op" style="color: #5E5E5E;">/</span>wikitext<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">103</span><span class="op" style="color: #5E5E5E;">-</span>raw<span class="op" style="color: #5E5E5E;">-</span>v1.<span class="bu" style="color: null;">zip</span></span>
<span id="cb1-2"><span class="op" style="color: #5E5E5E;">!</span>unzip wikitext<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">103</span><span class="op" style="color: #5E5E5E;">-</span>raw<span class="op" style="color: #5E5E5E;">-</span>v1.<span class="bu" style="color: null;">zip</span></span></code></pre></div>
</details>
</div>
<section id="challenges-in-efficiently-processing-large-datasets" class="level1">
<h1>Challenges in efficiently processing large datasets</h1>
<section id="loading-and-shuffling-large-datasets" class="level2">
<h2 class="anchored" data-anchor-id="loading-and-shuffling-large-datasets">1. Loading and shuffling large datasets</h2>
<p>For large datasets, loading the entire data into memory might not be possible. If we were to sample fully random batches we need to do random access on huge dataset. Depending on the disk latency this might be unfeasible.</p>
<p>To solve this we can do the following.</p>
<ol type="1">
<li>Shard the data into chunks larger than single instances so that it reduces the disk access.</li>
<li>Shuffle the chunks and load few of them and shuffle the data loaded from the chunks.</li>
</ol>
<p>If we shard the pieces into too big chunks we might end up loosing statistical power in our training updates as we are essentially reducing the randomness of our samples used for training. But we can’t shard them too small either as that wouldn’t solve our disk access problem.</p>
<p>We need a flexible approach would make it easy to control how much data is to be loaded into memory for shuffling. To address this challenge in isolation, you can refer dataset sharding logic in NVIDIA’s <a href="https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/data/indexed_dataset.py">MEGATRON language model training code</a>. But infinibatch solves it in a more generalized manner along with our other challenges.</p>
</section>
<section id="dynamic-batching" class="level2">
<h2 class="anchored" data-anchor-id="dynamic-batching">2. Dynamic Batching</h2>
<p>NLP datasets generally have samples which are of varied lengths. When we batch the data for training on devices like GPU, we are forced to make them into n-dimensional tensors with fixed dimension. The most common type of input for NLP models is of the shape <strong>Mini-batch size x Sequence length</strong>. The sequence length is either a fixed value or is the length of longest sequence in that batch. The shorter sequences in the minii-batch are generally padded with a <em>padding token</em>. These padding tokens are wasteful in terms of computation as they don’t do anything useful.</p>
<p>Some tutorials and examples you would find for pre-processing data would pad batches to a pre-determined sequence length independent of the elements in each batch. This is fully wasteful as many batches would have all the members less than the pre-determined length.</p>
<p>A better option would be to pad the elements of each batch to the sequence length which is maximum in that batch. This <strong>dynamic padding</strong> can improve efficiency but it doesn’t solve the entire problem. Let’s see why with an example.</p>
<section id="tokenization-and-length-distribution" class="level3">
<h3 class="anchored" data-anchor-id="tokenization-and-length-distribution">Tokenization and length distribution</h3>
<p>Let’s implement a typical dynamic padding workflow with pytorch dataloader and a subword level tokenizer. We use BERT-base-cased tokenizer from huggingface’s transformers library. This tokenizes words to subwords. The BERT model was pre-trained with maximum subword length of 512. We can theoretically use sequence lengths larger than that but for our purposes we will leave it as such at 512.</p>
<p>We will use torch’s <a href="https://pytorch.org/docs/stable/data.html">dataset and dataloader</a> abstraction for this. It will as both an illustration of real world usage and is convinent as it helps avoid having to entire tokenized dataset in memory. We still have to load the sentences into memory once. This is not a problem for small datasets, but for very large corpuses it’s a big problem as mentioned before.</p>
<div class="cell" data-outputid="20390f6a-10b2-4073-bdad-213f1d35045a" data-execution_count="2">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="op" style="color: #5E5E5E;">!</span>pip install git<span class="op" style="color: #5E5E5E;">+</span>https:<span class="op" style="color: #5E5E5E;">//</span>github.com<span class="op" style="color: #5E5E5E;">/</span>microsoft<span class="op" style="color: #5E5E5E;">/</span>infinibatch.git transformers torch</span>
<span id="cb2-2"><span class="im" style="color: #00769E;">from</span> typing <span class="im" style="color: #00769E;">import</span> Dict, List</span>
<span id="cb2-3"><span class="im" style="color: #00769E;">import</span> torch</span>
<span id="cb2-4"><span class="im" style="color: #00769E;">from</span> torch.utils.data <span class="im" style="color: #00769E;">import</span> Dataset, DataLoader</span>
<span id="cb2-5"><span class="im" style="color: #00769E;">import</span> tqdm</span>
<span id="cb2-6"><span class="im" style="color: #00769E;">from</span> transformers <span class="im" style="color: #00769E;">import</span> AutoTokenizer, PreTrainedTokenizerFast</span>
<span id="cb2-7"></span>
<span id="cb2-8"><span class="im" style="color: #00769E;">import</span> numpy <span class="im" style="color: #00769E;">as</span> np</span>
<span id="cb2-9"><span class="im" style="color: #00769E;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;">as</span> plt</span>
<span id="cb2-10"><span class="im" style="color: #00769E;">import</span> seaborn <span class="im" style="color: #00769E;">as</span> sns</span>
<span id="cb2-11"><span class="im" style="color: #00769E;">import</span> torch</span>
<span id="cb2-12"></span>
<span id="cb2-13"><span class="op" style="color: #5E5E5E;">%</span>matplotlib inline</span></code></pre></div>
</details>
</div>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="kw" style="color: #003B4F;">class</span> Wiki103(Dataset):</span>
<span id="cb3-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, tokenizer: PreTrainedTokenizerFast, data_path<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"wikitext-103-raw/wiki.train.raw"</span>, max_length: <span class="bu" style="color: null;">int</span> <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">512</span>):</span>
<span id="cb3-3">        <span class="va" style="color: #111111;">self</span>.tokenizer <span class="op" style="color: #5E5E5E;">=</span> tokenizer</span>
<span id="cb3-4">        <span class="cf" style="color: #003B4F;">with</span> <span class="bu" style="color: null;">open</span>(data_path) <span class="im" style="color: #00769E;">as</span> dp:</span>
<span id="cb3-5">            <span class="co" style="color: #5E5E5E;"># We are </span></span>
<span id="cb3-6">            <span class="va" style="color: #111111;">self</span>.sentences <span class="op" style="color: #5E5E5E;">=</span> [sentence.strip() <span class="cf" style="color: #003B4F;">for</span> sentence <span class="kw" style="color: #003B4F;">in</span> dp <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">len</span>(sentence.strip()) <span class="op" style="color: #5E5E5E;">&gt;</span> <span class="dv" style="color: #AD0000;">2</span>]</span>
<span id="cb3-7">        <span class="va" style="color: #111111;">self</span>.max_length <span class="op" style="color: #5E5E5E;">=</span> max_length</span>
<span id="cb3-8"></span>
<span id="cb3-9">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__len__</span>(<span class="va" style="color: #111111;">self</span>):</span>
<span id="cb3-10">        <span class="cf" style="color: #003B4F;">return</span> <span class="bu" style="color: null;">len</span>(<span class="va" style="color: #111111;">self</span>.sentences)</span>
<span id="cb3-11">    </span>
<span id="cb3-12">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__getitem__</span>(<span class="va" style="color: #111111;">self</span>, i) <span class="op" style="color: #5E5E5E;">-&gt;</span> Dict[<span class="bu" style="color: null;">str</span>, torch.Tensor]:</span>
<span id="cb3-13">        <span class="cf" style="color: #003B4F;">return</span> <span class="va" style="color: #111111;">self</span>.tokenizer(<span class="va" style="color: #111111;">self</span>.sentences[i], max_length<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">self</span>.max_length, truncation<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span></code></pre></div>
</div>
<div class="cell" data-outputid="99722997-5edb-49aa-9206-c68979125c34" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1">tokenizer <span class="op" style="color: #5E5E5E;">=</span> AutoTokenizer.from_pretrained(<span class="st" style="color: #20794D;">"bert-base-cased"</span>, use_fast<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span>
<span id="cb4-2">wiki_dataset <span class="op" style="color: #5E5E5E;">=</span> Wiki103(tokenizer<span class="op" style="color: #5E5E5E;">=</span>tokenizer)</span>
<span id="cb4-3">sequence_lengths <span class="op" style="color: #5E5E5E;">=</span> []</span>
<span id="cb4-4"><span class="cf" style="color: #003B4F;">for</span> example <span class="kw" style="color: #003B4F;">in</span> tqdm.tqdm(<span class="bu" style="color: null;">iter</span>(wiki_dataset)):</span>
<span id="cb4-5">    sequence_lengths.append(<span class="bu" style="color: null;">len</span>(example[<span class="st" style="color: #20794D;">"input_ids"</span>]))</span></code></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"cd77a1ae224f4a14a8a72edb6c1003e7","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code></code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"c970f42678fe494f829848541ef975df","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code></code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>1164464it [04:25, 4380.35it/s]</code></pre>
</div>
</div>
<p>By plotting the truncated <code>Subword sequence length vs Frequency</code> we see a distribution with a large variance.</p>
<div class="cell" data-outputid="db2d5efd-77b8-44ed-d9ca-515a77f27ae2" data-execution_count="5">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><span class="cf" style="color: #003B4F;">with</span> plt.style.context(<span class="st" style="color: #20794D;">'fivethirtyeight'</span>):</span>
<span id="cb8-2">    plt.figure(figsize<span class="op" style="color: #5E5E5E;">=</span>[<span class="dv" style="color: #AD0000;">7</span>,<span class="dv" style="color: #AD0000;">5</span>])</span>
<span id="cb8-3">    n, bins, patches <span class="op" style="color: #5E5E5E;">=</span> plt.hist(x<span class="op" style="color: #5E5E5E;">=</span>sequence_lengths, bins<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">50</span>, color<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'#0504aa'</span>,alpha<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.7</span>, rwidth<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.85</span>)</span>
<span id="cb8-4">    plt.grid(axis<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'y'</span>, alpha<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.75</span>)</span>
<span id="cb8-5">    plt.xlabel(<span class="st" style="color: #20794D;">'Subword Sequence Length'</span>,fontsize<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">15</span>)</span>
<span id="cb8-6">    plt.ylabel(<span class="st" style="color: #20794D;">'Frequency'</span>,fontsize<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">15</span>)</span>
<span id="cb8-7">    plt.xticks(fontsize<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">15</span>)</span>
<span id="cb8-8">    plt.yticks(fontsize<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">15</span>)</span>
<span id="cb8-9">    plt.ylabel(<span class="st" style="color: #20794D;">'Frequency'</span>,fontsize<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">15</span>)</span>
<span id="cb8-10">    plt.title(<span class="st" style="color: #20794D;">'Sequence Length Distribution'</span>,fontsize<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">15</span>)</span>
<span id="cb8-11">    plt.show()</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="saiprasanna.in/posts/efficient-dynamic-batching-of-large-datasets-with-infinibatch/index_files/figure-html/cell-6-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="dynamic-padding" class="level3">
<h3 class="anchored" data-anchor-id="dynamic-padding">Dynamic Padding</h3>
<p>From the above graph we can intuit that if we draw random samples from the data to form a mini-batch, we would have few examples which are significantly longer than the rest. This would mean that we would add a lot of padding tokens. This holds even if we clean the very short length instances as noise.</p>
<p>Let’s implement dynamic padding and measure how much. We can use torch’s <code>DataLoader</code> abstraction to do efficient batching with multi-processing. Since our tokenized outputs are of different lengths we have to implement a collate function to pad them dynamically together. We can pass the <code>tokenizer.pad</code> function implemented in huggingface’s tokenizer as the collate function.</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><span class="kw" style="color: #003B4F;">def</span> collate_fn(examples: List[Dict[<span class="bu" style="color: null;">str</span>, torch.Tensor]]) <span class="op" style="color: #5E5E5E;">-&gt;</span> Dict[<span class="bu" style="color: null;">str</span>, torch.Tensor]:</span>
<span id="cb9-2">    <span class="co" style="color: #5E5E5E;"># Since huggingface has already implemented this, this function is just to illustrate what a collator does.</span></span>
<span id="cb9-3">    <span class="cf" style="color: #003B4F;">return</span> tokenizer.pad(examples, return_tensors<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'pt'</span>)</span>
<span id="cb9-4"></span>
<span id="cb9-5">dataloader <span class="op" style="color: #5E5E5E;">=</span> DataLoader(dataset<span class="op" style="color: #5E5E5E;">=</span>wiki_dataset, batch_size<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">32</span>, collate_fn<span class="op" style="color: #5E5E5E;">=</span>collate_fn)</span></code></pre></div>
</div>
<p>Let’s assume that we can use maximum a batch size of 32 for max sequence length of 512 for our model in our training hardware without out-of-memory errors. The tokens per batch would be <code>512 * 32 = 16384</code>. We can now compute how much of it is padding tokens and what is the distribution of the batch’s sequence length(which depends on the maximum element in the batch).</p>
<div class="cell" data-outputid="f25e7bea-9d40-4ad0-d8f0-b2e197018005" data-execution_count="7">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1">total_tokens <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">0</span></span>
<span id="cb10-2">padding_tokens <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">0</span></span>
<span id="cb10-3">batch_lengths <span class="op" style="color: #5E5E5E;">=</span> []</span>
<span id="cb10-4"><span class="cf" style="color: #003B4F;">for</span> batch <span class="kw" style="color: #003B4F;">in</span> tqdm.tqdm(<span class="bu" style="color: null;">iter</span>(dataloader)):</span>
<span id="cb10-5">    batched_input_ids <span class="op" style="color: #5E5E5E;">=</span> batch[<span class="st" style="color: #20794D;">"input_ids"</span>]</span>
<span id="cb10-6">    batch_lengths.append(batched_input_ids.shape[<span class="dv" style="color: #AD0000;">1</span>])</span>
<span id="cb10-7">    total_tokens <span class="op" style="color: #5E5E5E;">+=</span> batched_input_ids.numel()</span>
<span id="cb10-8">    padding_tokens <span class="op" style="color: #5E5E5E;">+=</span> batched_input_ids[batched_input_ids <span class="op" style="color: #5E5E5E;">==</span> tokenizer.pad_token_id].numel()</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>100%|██████████| 36390/36390 [06:04&lt;00:00, 99.96it/s] </code></pre>
</div>
</div>
<div class="cell" data-outputid="0b1acde0-2567-4f4b-cfc9-d30d8d9fa183" data-execution_count="8">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f"Total Batches    : </span><span class="sc" style="color: #5E5E5E;">{</span><span class="bu" style="color: null;">len</span>(<span class="bu" style="color: null;">iter</span>(dataloader))<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">"</span>)</span>
<span id="cb12-2"><span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f"Padding Tokens   : </span><span class="sc" style="color: #5E5E5E;">{</span>padding_tokens<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">"</span>)</span>
<span id="cb12-3"><span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f"Input Tokens     : </span><span class="sc" style="color: #5E5E5E;">{</span>total_tokens <span class="op" style="color: #5E5E5E;">-</span> padding_tokens<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">"</span>)</span>
<span id="cb12-4"><span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f"Total Tokens     : </span><span class="sc" style="color: #5E5E5E;">{</span>total_tokens<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">"</span>)</span>
<span id="cb12-5"><span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f"Padding Tokens % : </span><span class="sc" style="color: #5E5E5E;">{</span>(padding_tokens<span class="op" style="color: #5E5E5E;">*</span><span class="dv" style="color: #AD0000;">100</span>)<span class="op" style="color: #5E5E5E;">/</span>total_tokens<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Total Batches    : 36390
Padding Tokens   : 244072396
Input Tokens     : 119699332
Total Tokens     : 363771728
Padding Tokens % : 67.09493267712108</code></pre>
</div>
</div>
<p>Surprise, surprise, <strong>67% of our net tokens are padding tokens</strong>. This would imply that of all the computations that we do, only 33% of is done for useful work. This starkly highlights the problem with static batch lengths even when accounting for dynamic padding.</p>
<p>Let’s also plot the distribution of batch lengths.</p>
<div class="cell" data-outputid="dd2980fb-b374-4454-9212-ff07ef9ef4b5" data-execution_count="9">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><span class="cf" style="color: #003B4F;">with</span> plt.style.context(<span class="st" style="color: #20794D;">'fivethirtyeight'</span>):</span>
<span id="cb14-2">    plt.figure(figsize<span class="op" style="color: #5E5E5E;">=</span>[<span class="dv" style="color: #AD0000;">7</span>,<span class="dv" style="color: #AD0000;">5</span>])</span>
<span id="cb14-3">    n, bins, patches <span class="op" style="color: #5E5E5E;">=</span> plt.hist(x<span class="op" style="color: #5E5E5E;">=</span>batch_lengths, bins<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">50</span>, color<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'#0504aa'</span>,alpha<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.7</span>, rwidth<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.85</span>)</span>
<span id="cb14-4">    plt.grid(axis<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'y'</span>, alpha<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.75</span>)</span>
<span id="cb14-5">    plt.xlabel(<span class="st" style="color: #20794D;">'Batch Sequence Length'</span>,fontsize<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">15</span>)</span>
<span id="cb14-6">    plt.ylabel(<span class="st" style="color: #20794D;">'Frequency'</span>,fontsize<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">15</span>)</span>
<span id="cb14-7">    plt.xticks(fontsize<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">15</span>)</span>
<span id="cb14-8">    plt.yticks(fontsize<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">15</span>)</span>
<span id="cb14-9">    plt.ylabel(<span class="st" style="color: #20794D;">'Frequency'</span>,fontsize<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">15</span>)</span>
<span id="cb14-10">    plt.title(<span class="st" style="color: #20794D;">'Static Batch - Dynamic Padding Length Distribution'</span>,fontsize<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">15</span>)</span>
<span id="cb14-11">    plt.show()</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="saiprasanna.in/posts/efficient-dynamic-batching-of-large-datasets-with-infinibatch/index_files/figure-html/cell-10-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>As batches are randomly sampled, we see a normal distribution as we can should expect by the Central Limit Theorem. The frequency in the final bin is deviant because we have a significant number of sentences which we had truncated, hence batches with them will have the maximum sequence length.</p>
</section>
<section id="general-approach-to-dynamic-batching" class="level3">
<h3 class="anchored" data-anchor-id="general-approach-to-dynamic-batching">General approach to dynamic batching</h3>
<p>Instead of drawing samples in random, had we sorted our dataset by length, then we can form batches by packing similar length sequences together into a batch till we reach the maximum number of tokens that we can fit. The maximum number of tokens that can be packed can be derived approximately from our previous memory limit <em>static_batch x max_sequence_length</em>. This allows us to pack more instances in one batch without much padding because the sequences would be of similar lengths after sorting.</p>
<p>We can’t sort the entire dataset because machine learning training is based on the assumption that our instances are drawn independently from an identical distribution (IID). If we were to sort the entire dataset this breaks the assumption as our samples are no longer drawn independently from each other. If sentence length were a confounding factor then the model might fit on this spurious correlation.</p>
<p>We have a trade-off here between statistical power derived from randomization of our samples and lesser error in gradient updates derived from larger batch sizes if we batch dynamically.</p>
<p>Generally, we can have a positive trade off by sampling a window of instances and sorting withing the window and forming batches.</p>
<p>The <code>Dataset</code> we implemented above is a <a href="https://pytorch.org/docs/stable/data.html#map-style-datasets">map-style</a> dataset. It implements length and random access to each individual data sample with index (<code>__getitem__</code>). The sampling into batches is taken care of a sampler passed to <code>DataLoader</code>.</p>
<p>I don’t think there is a clean way to implement a map-style dataset and a collate function such that we get batches with dynamic batch sizes but same number of tokens per batch. This comes from the basic mismatch of number of dynamic batches which you can form keeps changing based on the larger window you sample.</p>
<p>So it turns out that we have to do all the shuffling, windowing, sorting and batching inside a <a href="https://pytorch.org/docs/stable/data.html#iterable-style-datasets">iterable-style</a> <code>IterableDataset</code> dataset abstraction. These features are implemented by infinibatch.</p>
</section>
</section>
<section id="checkpointing" class="level2">
<h2 class="anchored" data-anchor-id="checkpointing">3. Checkpointing</h2>
<p>In large datasets, it’s typical not to wait for an entire epoch to checkpoint your model to recover from failures. So to be able to recover and continue training in a deterministic manner, such that it converges to same state if the failure hadn’t occured, we have to checkpoint the random state that controls the order in which our samples are generated.</p>
</section>
</section>
<section id="infinibatch-to-the-rescue" class="level1">
<h1>Infinibatch to the rescue</h1>
<blockquote class="blockquote">
<p>Infinibatch is a library of checkpointable iterators for randomized data loading of massive data sets in deep neural network training.</p>
</blockquote>
<p>It is aimed at simplify the processing of large datasets. It is a collection of pure python classes that implement <code>__iter__</code> interface. They can be composed inside one another easily and the final composed iterator can be checkpointed as a single entity.You can checkout it’s basic tutorial <a href="https://github.com/microsoft/infinibatch">here</a>. We will use it to address the listed challenges piece by piece and then finally make it work inside <code>IterableDataset</code> and <code>DataLoader</code> abstractions. We will also see the tricks needed to make it work distributed data parallel training.</p>
<section id="loading-and-shuffling-large-datasets-1" class="level2">
<h2 class="anchored" data-anchor-id="loading-and-shuffling-large-datasets-1">1. Loading and shuffling large datasets</h2>
<p>Following the infinibatch tutorial, we divide our dataset into multiple gzip chunks of 10000 sentences each.</p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><span class="op" style="color: #5E5E5E;">!</span>mkdir <span class="op" style="color: #5E5E5E;">-</span>p wikitext<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">103</span><span class="op" style="color: #5E5E5E;">-</span>chunks</span>
<span id="cb15-2"><span class="op" style="color: #5E5E5E;">!</span>split  <span class="op" style="color: #5E5E5E;">-</span>a <span class="dv" style="color: #AD0000;">4</span> <span class="op" style="color: #5E5E5E;">--</span>lines <span class="dv" style="color: #AD0000;">10000</span>  <span class="op" style="color: #5E5E5E;">--</span>numeric<span class="op" style="color: #5E5E5E;">-</span>suffixes <span class="op" style="color: #5E5E5E;">--</span><span class="bu" style="color: null;">filter</span> <span class="st" style="color: #20794D;">'gzip &gt; wikitext-103-chunks/$FILE.txt.gz'</span> wikitext<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">103</span><span class="op" style="color: #5E5E5E;">-</span>raw<span class="op" style="color: #5E5E5E;">/</span>wiki.train.raw  train.</span></code></pre></div>
</div>
<p>We can now create an iterator using infinibatch with a function that can deserialize a shard. Infinibatch takes care of loading multiple in a shuffled order. We can control the amount of deserialized individual examples from the shards be buffered using <code>buffer_size</code> parameter. The library returns a python iterable. We can call <code>next(iterable)</code> or iterate with a <code>for</code> to get the examples.</p>
<p>Note: Passing <code>train=True</code> creates an infinite iterator that cycles after a full run on the dataset. The <code>chunked_dataset_iterator</code> method returns a composition of iterators, you can refer the source code <a href="https://github.com/microsoft/infinibatch/blob/master/infinibatch/iterators.py">here</a></p>
<div class="cell" data-outputid="98e9fbf6-b426-4da2-eef2-4c7feff5cc55" data-execution_count="11">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><span class="im" style="color: #00769E;">import</span> gzip, glob</span>
<span id="cb16-2"><span class="im" style="color: #00769E;">from</span> functools <span class="im" style="color: #00769E;">import</span> partial</span>
<span id="cb16-3"></span>
<span id="cb16-4"><span class="im" style="color: #00769E;">from</span> infinibatch <span class="im" style="color: #00769E;">import</span> datasets, iterators</span>
<span id="cb16-5"></span>
<span id="cb16-6"><span class="kw" style="color: #003B4F;">def</span> read_chunk(path):</span>
<span id="cb16-7">    <span class="cf" style="color: #003B4F;">with</span> <span class="bu" style="color: null;">open</span>(path, <span class="st" style="color: #20794D;">"rb"</span>) <span class="im" style="color: #00769E;">as</span> fp:</span>
<span id="cb16-8">        lines <span class="op" style="color: #5E5E5E;">=</span> gzip.decompress(fp.read()).decode(encoding<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'utf-8'</span>).splitlines()</span>
<span id="cb16-9">        lines <span class="op" style="color: #5E5E5E;">=</span> [sentence.strip() <span class="cf" style="color: #003B4F;">for</span> sentence <span class="kw" style="color: #003B4F;">in</span> lines <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">len</span>(sentence.strip()) <span class="op" style="color: #5E5E5E;">&gt;</span> <span class="dv" style="color: #AD0000;">2</span>]</span>
<span id="cb16-10">    <span class="cf" style="color: #003B4F;">return</span> <span class="bu" style="color: null;">iter</span>(lines)</span>
<span id="cb16-11"></span>
<span id="cb16-12">sentence_it <span class="op" style="color: #5E5E5E;">=</span> datasets.chunked_dataset_iterator(</span>
<span id="cb16-13">    chunk_refs <span class="op" style="color: #5E5E5E;">=</span> glob.glob(<span class="st" style="color: #20794D;">'wikitext-103-chunks/train.*.txt.gz'</span>),</span>
<span id="cb16-14">    read_chunk_fn <span class="op" style="color: #5E5E5E;">=</span> read_chunk,</span>
<span id="cb16-15">    buffer_size <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">100000</span>, seed <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">1337</span>, train<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>, shuffle<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span>
<span id="cb16-16"></span>
<span id="cb16-17"><span class="bu" style="color: null;">print</span>(<span class="bu" style="color: null;">next</span>(sentence_it))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>In 1993 , David Mirkin hired Scully to write for The Simpsons , as a replacement for the departing Conan O 'Brien , after reading some of his sample scripts . He began as a writer and producer for the show during its fifth season and wrote the episodes " Lisa 's Rival " , " Two Dozen and One Greyhounds " and " Lisa on Ice " which aired in season six . " Lisa 's Rival " was his first episode ; he wrote the script , but the original concept had been conceived by O 'Brien . Similarly , he wrote the script for " Two Dozen and One Greyhounds " , which was based on an idea by Al Jean and Mike Reiss . " Lisa on Ice " was inspired by Scully 's love of ice hockey and featured many experiences from his childhood , as was " Marge Be Not Proud " ( which he wrote for season seven ) which was based " one of the most traumatic moments " of his life , when he was caught shoplifting aged twelve . He jokingly told Variety that " It 's great to be paid for reliving the horrors of your life . " He also wrote " Team Homer " and " Lisa 's Date with Density " . Scully noted : " I wrote a lot of Lisa 's shows . I have five daughters , so I like Lisa a lot . I like Homer , too . Homer comes very naturally to me : I don 't know if that 's a good or a bad thing . A lot of my favorite episodes are the ones when Homer and Lisa are in conflict with each other ... They 're very human , I think that 's their appeal . "</code></pre>
</div>
</div>
<section id="tensorize-our-dataset-with-a-map-iterator" class="level3">
<h3 class="anchored" data-anchor-id="tensorize-our-dataset-with-a-map-iterator">Tensorize our dataset with a map iterator</h3>
<p>We can now compose our tokenizer upon our sentence iterator. Infinibatch has two ways of doing this, 1. <code>MapIterator</code> 2. <code>ParallelMapIterator</code></p>
<p>If you use pytorch and need multiprocessing to do costly transformations over your data on the fly, use the <code>ParallelMap</code> and set the <code>num_processes</code> with what you would have with <code>num_workers</code>. And set <code>num_workers=0</code> in your dataloader.</p>
<div class="cell" data-outputid="a3fc83c0-b76c-4304-d04f-e976d5160e53" data-execution_count="12">
<div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1">tokenize_fn <span class="op" style="color: #5E5E5E;">=</span> partial(tokenizer, max_length<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">512</span>, truncation<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span>
<span id="cb18-2"></span>
<span id="cb18-3">features_it <span class="op" style="color: #5E5E5E;">=</span> iterators.ParallelMapIterator(</span>
<span id="cb18-4">    source_iterator<span class="op" style="color: #5E5E5E;">=</span>sentence_it,</span>
<span id="cb18-5">    num_processes<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">4</span>,</span>
<span id="cb18-6">    num_items_per_process<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1000</span>,</span>
<span id="cb18-7">    transform<span class="op" style="color: #5E5E5E;">=</span>tokenize_fn</span>
<span id="cb18-8">)</span>
<span id="cb18-9"><span class="bu" style="color: null;">next</span>(features_it)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="12">
<pre><code>{'input_ids': [101, 1130, 1949, 117, 1681, 20522, 4314, 4327, 20452, 16125, 1106, 3593, 1111, 1109, 20726, 117, 1112, 170, 5627, 1111, 1103, 18646, 17727, 152, 112, 9620, 117, 1170, 3455, 1199, 1104, 1117, 6876, 15690, 119, 1124, 1310, 1112, 170, 2432, 1105, 2451, 1111, 1103, 1437, 1219, 1157, 3049, 1265, 1105, 1724, 1103, 3426, 107, 6516, 112, 188, 155, 15895, 107, 117, 107, 1960, 2091, 10947, 1105, 1448, 6285, 16930, 1116, 107, 1105, 107, 6516, 1113, 6172, 107, 1134, 4086, 1107, 1265, 1565, 119, 107, 6516, 112, 188, 155, 15895, 107, 1108, 1117, 1148, 2004, 132, 1119, 1724, 1103, 5444, 117, 1133, 1103, 1560, 3400, 1125, 1151, 10187, 1118, 152, 112, 9620, 119, 10321, 117, 1119, 1724, 1103, 5444, 1111, 107, 1960, 2091, 10947, 1105, 1448, 6285, 16930, 1116, 107, 117, 1134, 1108, 1359, 1113, 1126, 1911, 1118, 2586, 2893, 1105, 2639, 11336, 14788, 119, 107, 6516, 1113, 6172, 107, 1108, 3768, 1118, 20452, 16125, 112, 188, 1567, 1104, 2854, 4700, 1105, 2081, 1242, 5758, 1121, 1117, 5153, 117, 1112, 1108, 107, 9751, 2176, 4108, 1753, 5096, 4867, 107, 113, 1134, 1119, 1724, 1111, 1265, 1978, 114, 1134, 1108, 1359, 107, 1141, 1104, 1103, 1211, 23057, 4899, 107, 1104, 1117, 1297, 117, 1165, 1119, 1108, 2347, 4130, 18867, 4079, 4030, 119, 1124, 18114, 1193, 1500, 15526, 1115, 107, 1135, 112, 188, 1632, 1106, 1129, 3004, 1111, 1231, 2646, 3970, 1103, 5367, 1116, 1104, 1240, 1297, 119, 107, 1124, 1145, 1724, 107, 2649, 12353, 107, 1105, 107, 6516, 112, 188, 14265, 1114, 14760, 13730, 107, 119, 20452, 16125, 2382, 131, 107, 146, 1724, 170, 1974, 1104, 6516, 112, 188, 2196, 119, 146, 1138, 1421, 5421, 117, 1177, 146, 1176, 6516, 170, 1974, 119, 146, 1176, 12353, 117, 1315, 119, 12353, 2502, 1304, 8534, 1106, 1143, 131, 146, 1274, 112, 189, 1221, 1191, 1115, 112, 188, 170, 1363, 1137, 170, 2213, 1645, 119, 138, 1974, 1104, 1139, 5095, 3426, 1132, 1103, 3200, 1165, 12353, 1105, 6516, 1132, 1107, 4139, 1114, 1296, 1168, 119, 119, 119, 1220, 112, 1231, 1304, 1769, 117, 146, 1341, 1115, 112, 188, 1147, 5767, 119, 107, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}</code></pre>
</div>
</div>
</section>
</section>
<section id="dynamic-batching-1" class="level2">
<h2 class="anchored" data-anchor-id="dynamic-batching-1">2. Dynamic Batching</h2>
<p>Now comes the magic of dynamic batching with <code>BucketedReadaheadBatchIterator</code>. Let’s fix the maximum tokens per batch to <code>32 * 512 = 16384</code>. This iterator allows you to compute dynamic batch size by iteratively applying a user given function over the current longest example (with length computed by user function) in a sorted <code>read_ahead</code> window. This window is sorted and batches are formed by using the user provided <code>batch_size</code> function iteratively.</p>
<section id="example" class="level3">
<h3 class="anchored" data-anchor-id="example">Example</h3>
<p>Say we want 50 tokens per batch. If we set a read ahead window of 6. Assume we fetch six items [a, b, c, d, e, f] in the first read ahead window.</p>
<table class="table">
<thead>
<tr class="header">
<th>Sequence id</th>
<th>Length</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>a</td>
<td>50</td>
</tr>
<tr class="even">
<td>b</td>
<td>30</td>
</tr>
<tr class="odd">
<td>c</td>
<td>20</td>
</tr>
<tr class="even">
<td>d</td>
<td>20</td>
</tr>
<tr class="odd">
<td>e</td>
<td>30</td>
</tr>
<tr class="even">
<td>f</td>
<td>20</td>
</tr>
</tbody>
</table>
<p>First we sort this window with lengths in decreasing order. The sort order is stable. This preserves the shuffling of equal sized elements from previous iterator. So for our example it would be [a, b, e, c, d, f]</p>
<p>Now we can Compute the dynamic batch sizes by applying the function <code>batch_size</code> iteratively till the window is exhausted. Assume our function is <code>lambda longest_instance: 60 // len(longest_instance)</code>. Then applying it once we get first longest item <code>a</code>, current batch size will be <code>60 //50 = 1</code>. The next longest item remaining can be used to calculate the size of the next batch and so on. So we will end up with [a], [b, e], [c, d, f]. Each of them will have 60 tokens.</p>
<p>You can take a look at the code that does this computation <a href="https://github.com/microsoft/infinibatch/blob/master/infinibatch/iterators.py#L1080">here</a>.</p>
<div class="cell" data-outputid="5223316b-2086-4efe-97d3-0d83f87cf96e" data-execution_count="13">
<div class="sourceCode cell-code" id="cb20" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1">tokens_per_batch <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">32</span> <span class="op" style="color: #5E5E5E;">*</span> <span class="dv" style="color: #AD0000;">512</span></span>
<span id="cb20-2">batches_it <span class="op" style="color: #5E5E5E;">=</span> iterators.BucketedReadaheadBatchIterator(</span>
<span id="cb20-3">    source_iterator<span class="op" style="color: #5E5E5E;">=</span>features_it,</span>
<span id="cb20-4">    <span class="co" style="color: #5E5E5E;"># read_ahead is the number of items to be read from previous iterator,</span></span>
<span id="cb20-5">    <span class="co" style="color: #5E5E5E;"># these are sorted and over which dynamic batches are formed.</span></span>
<span id="cb20-6">    read_ahead<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">10000</span>, </span>
<span id="cb20-7">    <span class="co" style="color: #5E5E5E;"># key determines the length used to sort and choose the longest remaining record.</span></span>
<span id="cb20-8">    key<span class="op" style="color: #5E5E5E;">=</span><span class="kw" style="color: #003B4F;">lambda</span> example: <span class="bu" style="color: null;">len</span>(example[<span class="st" style="color: #20794D;">'input_ids'</span>]), </span>
<span id="cb20-9">     <span class="co" style="color: #5E5E5E;"># Determines the dynamic batch size</span></span>
<span id="cb20-10">    batch_size<span class="op" style="color: #5E5E5E;">=</span><span class="kw" style="color: #003B4F;">lambda</span> longest_example: tokens_per_batch <span class="op" style="color: #5E5E5E;">//</span> <span class="bu" style="color: null;">len</span>(longest_example[<span class="st" style="color: #20794D;">'input_ids'</span>]),</span>
<span id="cb20-11">    seed<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">0</span> </span>
<span id="cb20-12">)</span>
<span id="cb20-13">dynamic_batch_wo_padding <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">next</span>(batches_it)</span>
<span id="cb20-14"><span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f"Dynamic batch size: </span><span class="sc" style="color: #5E5E5E;">{</span><span class="bu" style="color: null;">len</span>(dynamic_batch_wo_padding)<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">"</span>)</span>
<span id="cb20-15">dynamic_batch_wo_padding <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">next</span>(batches_it)</span>
<span id="cb20-16"><span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f"Dynamic batch size: </span><span class="sc" style="color: #5E5E5E;">{</span><span class="bu" style="color: null;">len</span>(dynamic_batch_wo_padding)<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">"</span>)</span>
<span id="cb20-17"><span class="bu" style="color: null;">print</span>(dynamic_batch_wo_padding[:<span class="dv" style="color: #AD0000;">2</span>])</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Dynamic batch size: 124
Dynamic batch size: 94
[{'input_ids': [101, 1109, 3500, 3039, 13025, 1126, 1903, 1104, 1492, 188, 1204, 121, 137, 119, 137, 8347, 5682, 113, 5787, 137, 119, 137, 125, 4023, 114, 117, 1166, 126, 137, 119, 137, 126, 6549, 113, 123, 137, 119, 137, 126, 4023, 114, 1679, 24773, 1167, 1190, 1147, 7741, 119, 3900, 112, 188, 3039, 4049, 1198, 170, 1423, 24773, 1114, 12936, 6398, 2541, 1107, 1103, 3499, 1526, 2084, 1105, 6625, 2188, 20394, 5773, 1633, 117, 1229, 3500, 1486, 2055, 11142, 117, 1681, 156, 10098, 2897, 1105, 1884, 1775, 4978, 11661, 1862, 119, 3396, 24773, 1116, 117, 1160, 1121, 1296, 2755, 117, 1127, 4410, 1112, 7474, 6635, 1107, 1103, 1886, 131, 3500, 112, 188, 4367, 155, 5792, 1108, 1925, 1229, 141, 119, 6511, 1108, 1237, 117, 1105, 3900, 112, 188, 139, 119, 3160, 1108, 1375, 2170, 1229, 1287, 25730, 1931, 17932, 1121, 1203, 2512, 119, 3500, 112, 188, 3499, 1526, 2084, 11311, 5157, 14618, 1125, 2856, 1471, 1121, 1103, 3039, 1160, 2277, 2988, 1106, 1103, 1886, 1112, 170, 1871, 1104, 170, 2960, 1104, 1532, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 2907, 1107, 1478, 117, 1119, 1125, 170, 4374, 1648, 1107, 1103, 28117, 21643, 1273, 1109, 156, 13622, 22273, 7443, 119, 1230, 1397, 1273, 1648, 1108, 1107, 1823, 20452, 1324, 10781, 1204, 2391, 112, 188, 11826, 6945, 1643, 4371, 113, 1478, 114, 119, 1130, 1103, 1273, 117, 17784, 1733, 19572, 1307, 1126, 1586, 23963, 1233, 117, 1150, 1110, 2802, 1106, 1712, 3542, 1104, 8125, 7782, 7895, 112, 188, 1959, 119, 6945, 1643, 4371, 1108, 12468, 1120, 170, 1957, 8685, 1120, 1103, 13631, 2683, 3506, 1570, 2352, 2263, 1107, 1478, 119, 2711, 1103, 3216, 3761, 117, 1103, 1273, 1108, 170, 2798, 2244, 117, 6957, 109, 22803, 1550, 4529, 117, 1543, 1122, 1117, 2439, 137, 118, 137, 19842, 1273, 1106, 1103, 1322, 1104, 1369, 119, 17784, 1733, 19572, 112, 188, 1397, 2672, 1108, 147, 1813, 3925, 113, 1478, 114, 117, 3714, 4387, 144, 7777, 7836, 2328, 1348, 119, 1109, 2523, 1110, 1359, 1113, 158, 119, 156, 119, 4620, 4140, 156, 12821, 3101, 6944, 112, 188, 1581, 5634, 1414, 14871, 1104, 1103, 1269, 1271, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}]</code></pre>
</div>
</div>
<p>Now we can collate our examples and see how much this scheme has saved us. Since a training iterator is infinite, we will recreate our iterators with a non-infinite iterator.</p>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb22" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1">sentence_it_finite <span class="op" style="color: #5E5E5E;">=</span> datasets.chunked_dataset_iterator(</span>
<span id="cb22-2">    chunk_refs <span class="op" style="color: #5E5E5E;">=</span> glob.glob(<span class="st" style="color: #20794D;">'wikitext-103-chunks/train.*.txt.gz'</span>),</span>
<span id="cb22-3">    read_chunk_fn <span class="op" style="color: #5E5E5E;">=</span> read_chunk,</span>
<span id="cb22-4">    buffer_size <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">100000</span>, </span>
<span id="cb22-5">    seed <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">1337</span>, </span>
<span id="cb22-6">    train<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>, </span>
<span id="cb22-7">    shuffle<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span></span>
<span id="cb22-8">)</span>
<span id="cb22-9">features_it_finite <span class="op" style="color: #5E5E5E;">=</span> iterators.ParallelMapIterator(</span>
<span id="cb22-10">    source_iterator<span class="op" style="color: #5E5E5E;">=</span>sentence_it_finite,</span>
<span id="cb22-11">    num_processes<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">4</span>,</span>
<span id="cb22-12">    num_items_per_process<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1000</span>,</span>
<span id="cb22-13">    transform<span class="op" style="color: #5E5E5E;">=</span>tokenize_fn</span>
<span id="cb22-14">)</span>
<span id="cb22-15">batches_it_finite <span class="op" style="color: #5E5E5E;">=</span> iterators.BucketedReadaheadBatchIterator(</span>
<span id="cb22-16">    source_iterator<span class="op" style="color: #5E5E5E;">=</span>features_it_finite,</span>
<span id="cb22-17">    read_ahead<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">10000</span>, <span class="co" style="color: #5E5E5E;"># Determines the window for the bucket which</span></span>
<span id="cb22-18">    <span class="co" style="color: #5E5E5E;"># will be sorted and  converted to batches.</span></span>
<span id="cb22-19">    key<span class="op" style="color: #5E5E5E;">=</span><span class="kw" style="color: #003B4F;">lambda</span> example: <span class="bu" style="color: null;">len</span>(example[<span class="st" style="color: #20794D;">'input_ids'</span>]), <span class="co" style="color: #5E5E5E;"># Determines the length used</span></span>
<span id="cb22-20">    <span class="co" style="color: #5E5E5E;"># to sort and choose the longest remaining record.</span></span>
<span id="cb22-21">    batch_size<span class="op" style="color: #5E5E5E;">=</span><span class="kw" style="color: #003B4F;">lambda</span> longest: tokens_per_batch <span class="op" style="color: #5E5E5E;">//</span> <span class="bu" style="color: null;">len</span>(longest[<span class="st" style="color: #20794D;">'input_ids'</span>]),</span>
<span id="cb22-22">    <span class="co" style="color: #5E5E5E;"># Determines the dynamic batch size</span></span>
<span id="cb22-23">    seed<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">0</span> </span>
<span id="cb22-24">)</span>
<span id="cb22-25">collate_fn <span class="op" style="color: #5E5E5E;">=</span> partial(tokenizer.pad, return_tensors<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'pt'</span>)</span>
<span id="cb22-26">tensors_it_finite <span class="op" style="color: #5E5E5E;">=</span> iterators.MapIterator(</span>
<span id="cb22-27">    batches_it_finite,</span>
<span id="cb22-28">    transform<span class="op" style="color: #5E5E5E;">=</span>collate_fn</span>
<span id="cb22-29">)</span></code></pre></div>
</div>
<div class="cell" data-outputid="2296d9db-6643-4d3d-f8ae-afd17ded5935" data-execution_count="15">
<div class="sourceCode cell-code" id="cb23" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1">total_batches_dynamic <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">0</span> </span>
<span id="cb23-2">total_tokens_dynamic <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">0</span></span>
<span id="cb23-3">padding_tokens_dynamic <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">0</span></span>
<span id="cb23-4">batch_lengths_dynamic <span class="op" style="color: #5E5E5E;">=</span> []</span>
<span id="cb23-5"><span class="cf" style="color: #003B4F;">for</span> batch <span class="kw" style="color: #003B4F;">in</span> tqdm.tqdm(tensors_it_finite):</span>
<span id="cb23-6">    total_batches_dynamic <span class="op" style="color: #5E5E5E;">+=</span> <span class="dv" style="color: #AD0000;">1</span></span>
<span id="cb23-7">    batched_input_ids <span class="op" style="color: #5E5E5E;">=</span> batch[<span class="st" style="color: #20794D;">"input_ids"</span>]</span>
<span id="cb23-8">    batch_lengths_dynamic.append(batched_input_ids.shape[<span class="dv" style="color: #AD0000;">1</span>])</span>
<span id="cb23-9">    total_tokens_dynamic <span class="op" style="color: #5E5E5E;">+=</span> batched_input_ids.numel()</span>
<span id="cb23-10">    padding_tokens_dynamic <span class="op" style="color: #5E5E5E;">+=</span> batched_input_ids[batched_input_ids <span class="op" style="color: #5E5E5E;">==</span> tokenizer.pad_token_id].numel()</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>7650it [08:11, 15.57it/s]</code></pre>
</div>
</div>
<div class="cell" data-outputid="27ddbb47-2dc9-4aa9-8a35-163d9ad5fe8a" data-execution_count="16">
<div class="sourceCode cell-code" id="cb25" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f"Total Batches    : </span><span class="sc" style="color: #5E5E5E;">{</span>total_batches_dynamic<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">"</span>) <span class="co" style="color: #5E5E5E;"># Seeing the tqdm stats.</span></span>
<span id="cb25-2"><span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f"Padding Tokens   : </span><span class="sc" style="color: #5E5E5E;">{</span>padding_tokens_dynamic<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">"</span>)</span>
<span id="cb25-3"><span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f"Input Tokens     : </span><span class="sc" style="color: #5E5E5E;">{</span>total_tokens_dynamic <span class="op" style="color: #5E5E5E;">-</span> padding_tokens_dynamic<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">"</span>)</span>
<span id="cb25-4"><span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f"Total Tokens     : </span><span class="sc" style="color: #5E5E5E;">{</span>total_tokens_dynamic<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">"</span>)</span>
<span id="cb25-5"><span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f"Padding Tokens % : </span><span class="sc" style="color: #5E5E5E;">{</span>(padding_tokens_dynamic<span class="op" style="color: #5E5E5E;">*</span><span class="dv" style="color: #AD0000;">100</span>)<span class="op" style="color: #5E5E5E;">/</span>total_tokens_dynamic<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Total Batches    : 7650
Padding Tokens   : 3838939
Input Tokens     : 119699332
Total Tokens     : 123538271
Padding Tokens % : 3.1074896620497463</code></pre>
</div>
</div>
<p>We have <strong>reduced</strong> the % of <strong>padding tokens</strong> per epoch from <strong>67% to just around 3%</strong>.The total batches needed to process it in the same max tokens per batch limitation hence got reduced nearly five times from 36390 to 7642.</p>
<p>The processing time is just one minute extra. I guess that might be due to IO, but you could try benchmarking that with more rigour.</p>
<p>Now, plotting the length distribution for dynamic batches.</p>
<div class="cell" data-outputid="1e6ebe34-7066-4719-94a0-de85dc36ee99" data-execution_count="17">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb27" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><span class="cf" style="color: #003B4F;">with</span> plt.style.context(<span class="st" style="color: #20794D;">'fivethirtyeight'</span>):</span>
<span id="cb27-2">    plt.figure(figsize<span class="op" style="color: #5E5E5E;">=</span>[<span class="dv" style="color: #AD0000;">7</span>,<span class="dv" style="color: #AD0000;">5</span>])</span>
<span id="cb27-3">    n, bins, patches <span class="op" style="color: #5E5E5E;">=</span> plt.hist(x<span class="op" style="color: #5E5E5E;">=</span>batch_lengths_dynamic, bins<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">50</span>, color<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'#0504aa'</span>,alpha<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.7</span>, rwidth<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.85</span>)</span>
<span id="cb27-4">    plt.grid(axis<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'y'</span>, alpha<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.75</span>)</span>
<span id="cb27-5">    plt.xlabel(<span class="st" style="color: #20794D;">'Batch Sequence Length'</span>,fontsize<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">15</span>)</span>
<span id="cb27-6">    plt.ylabel(<span class="st" style="color: #20794D;">'Frequency'</span>,fontsize<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">15</span>)</span>
<span id="cb27-7">    plt.xticks(fontsize<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">15</span>)</span>
<span id="cb27-8">    plt.yticks(fontsize<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">15</span>)</span>
<span id="cb27-9">    plt.ylabel(<span class="st" style="color: #20794D;">'Frequency'</span>,fontsize<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">15</span>)</span>
<span id="cb27-10">    plt.title(<span class="st" style="color: #20794D;">'Dynamic Batch - Dynamic Padding Length Distribution'</span>,fontsize<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">15</span>)</span>
<span id="cb27-11">    plt.show()</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="saiprasanna.in/posts/efficient-dynamic-batching-of-large-datasets-with-infinibatch/index_files/figure-html/cell-18-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>We now see that the expected per batch sequence length has reduced from 300 to 200.</p>
</section>
</section>
<section id="static-batch-sizes-with-sorted-window" class="level2">
<h2 class="anchored" data-anchor-id="static-batch-sizes-with-sorted-window">Static Batch Sizes with Sorted Window</h2>
<p>In practise using variable batch sizes could be a problem depending on your task. As tokens in an instance are already correlated, having few instances (though longer) in one batch might give a more noisy gradient update than many shorter instances in a batch. This is my <strong>speculation</strong> as to why I wasn’t able to make purely dynamic batch sizes work well with a token classification fine-tuning on BERT.</p>
<p>But all is not lost here, we can still use bucketing to have uniform length batches with fewer padding tokens. Let’s checkout the padding tokens % when we fix the batch size as 32 but sort a window of 10 batches (320 instances) and then form batches.</p>
<div class="cell" data-outputid="05fb3403-6f54-4df1-b64a-17bbbb6728a6" data-execution_count="18">
<div class="sourceCode cell-code" id="cb28" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1">sentence_it_finite <span class="op" style="color: #5E5E5E;">=</span> datasets.chunked_dataset_iterator(</span>
<span id="cb28-2">    chunk_refs <span class="op" style="color: #5E5E5E;">=</span> glob.glob(<span class="st" style="color: #20794D;">'wikitext-103-chunks/train.*.txt.gz'</span>),</span>
<span id="cb28-3">    read_chunk_fn <span class="op" style="color: #5E5E5E;">=</span> read_chunk,</span>
<span id="cb28-4">    buffer_size <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">100000</span>, </span>
<span id="cb28-5">    seed <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">1337</span>, </span>
<span id="cb28-6">    train<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>, </span>
<span id="cb28-7">    shuffle<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span></span>
<span id="cb28-8">)</span>
<span id="cb28-9">features_it_finite <span class="op" style="color: #5E5E5E;">=</span> iterators.ParallelMapIterator(</span>
<span id="cb28-10">    source_iterator<span class="op" style="color: #5E5E5E;">=</span>sentence_it_finite,</span>
<span id="cb28-11">    num_processes<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">4</span>,</span>
<span id="cb28-12">    num_items_per_process<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1000</span>,</span>
<span id="cb28-13">    transform<span class="op" style="color: #5E5E5E;">=</span>tokenize_fn</span>
<span id="cb28-14">)</span>
<span id="cb28-15">batches_it_finite <span class="op" style="color: #5E5E5E;">=</span> iterators.BucketedReadaheadBatchIterator(</span>
<span id="cb28-16">    source_iterator<span class="op" style="color: #5E5E5E;">=</span>features_it_finite,</span>
<span id="cb28-17">    key<span class="op" style="color: #5E5E5E;">=</span><span class="kw" style="color: #003B4F;">lambda</span> example: <span class="bu" style="color: null;">len</span>(example[<span class="st" style="color: #20794D;">'input_ids'</span>]), <span class="co" style="color: #5E5E5E;"># Determines the length used</span></span>
<span id="cb28-18">    read_ahead<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">320</span>, <span class="co" style="color: #5E5E5E;"># Setting this ten times the static batch size.</span></span>
<span id="cb28-19">    batch_size<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">32</span>,</span>
<span id="cb28-20">    seed<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">0</span> </span>
<span id="cb28-21">)</span>
<span id="cb28-22">collate_fn <span class="op" style="color: #5E5E5E;">=</span> partial(tokenizer.pad, return_tensors<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'pt'</span>)</span>
<span id="cb28-23">tensors_it_finite <span class="op" style="color: #5E5E5E;">=</span> iterators.MapIterator(</span>
<span id="cb28-24">    batches_it_finite,</span>
<span id="cb28-25">    transform<span class="op" style="color: #5E5E5E;">=</span>collate_fn</span>
<span id="cb28-26">)</span>
<span id="cb28-27">total_batches_bucket_sorted <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">0</span> </span>
<span id="cb28-28">total_tokens_bucket_sorted<span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">0</span></span>
<span id="cb28-29">padding_tokens_bucket_sorted <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">0</span></span>
<span id="cb28-30">batch_lengths_bucket_sorted <span class="op" style="color: #5E5E5E;">=</span> []</span>
<span id="cb28-31"><span class="cf" style="color: #003B4F;">for</span> batch <span class="kw" style="color: #003B4F;">in</span> tqdm.tqdm(tensors_it_finite):</span>
<span id="cb28-32">    total_batches_bucket_sorted <span class="op" style="color: #5E5E5E;">+=</span> <span class="dv" style="color: #AD0000;">1</span></span>
<span id="cb28-33">    batched_input_ids <span class="op" style="color: #5E5E5E;">=</span> batch[<span class="st" style="color: #20794D;">"input_ids"</span>]</span>
<span id="cb28-34">    batch_lengths_bucket_sorted.append(batched_input_ids.shape[<span class="dv" style="color: #AD0000;">1</span>])</span>
<span id="cb28-35">    total_tokens_bucket_sorted <span class="op" style="color: #5E5E5E;">+=</span> batched_input_ids.numel()</span>
<span id="cb28-36">    padding_tokens_bucket_sorted <span class="op" style="color: #5E5E5E;">+=</span> batched_input_ids[batched_input_ids <span class="op" style="color: #5E5E5E;">==</span> tokenizer.pad_token_id].numel()</span>
<span id="cb28-37"><span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f"</span><span class="ch" style="color: #20794D;">\n</span><span class="ss" style="color: #20794D;">Total Batches    : </span><span class="sc" style="color: #5E5E5E;">{</span>total_batches_bucket_sorted<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">"</span>) <span class="co" style="color: #5E5E5E;"># Seeing the tqdm stats.</span></span>
<span id="cb28-38"><span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f"Padding Tokens   : </span><span class="sc" style="color: #5E5E5E;">{</span>padding_tokens_bucket_sorted<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">"</span>)</span>
<span id="cb28-39"><span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f"Input Tokens     : </span><span class="sc" style="color: #5E5E5E;">{</span>total_tokens_bucket_sorted <span class="op" style="color: #5E5E5E;">-</span> padding_tokens_bucket_sorted<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">"</span>)</span>
<span id="cb28-40"><span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f"Total Tokens     : </span><span class="sc" style="color: #5E5E5E;">{</span>total_tokens_bucket_sorted<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">"</span>)</span>
<span id="cb28-41"><span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f"Padding Tokens % : </span><span class="sc" style="color: #5E5E5E;">{</span>(padding_tokens_bucket_sorted<span class="op" style="color: #5E5E5E;">*</span><span class="dv" style="color: #AD0000;">100</span>)<span class="op" style="color: #5E5E5E;">/</span>total_tokens_bucket_sorted<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>36390it [08:26, 71.81it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Total Batches    : 36390
Padding Tokens   : 31806252
Input Tokens     : 119699332
Total Tokens     : 151505584
Padding Tokens % : 20.993451964120347</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
</div>
<p>So we see that the % of total padding tokens has decreased from 67% to 20%. And we can see from the below the distribution of sequence length of batches is close to the distribution of individual sequence lengths. This is different from the case where we made static batches without sorting.</p>
<div class="cell" data-outputid="a75613f8-5e54-4d43-bbea-4a0424339d21" data-execution_count="19">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb32" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><span class="cf" style="color: #003B4F;">with</span> plt.style.context(<span class="st" style="color: #20794D;">'fivethirtyeight'</span>):</span>
<span id="cb32-2">    plt.figure(figsize<span class="op" style="color: #5E5E5E;">=</span>[<span class="dv" style="color: #AD0000;">7</span>,<span class="dv" style="color: #AD0000;">5</span>])</span>
<span id="cb32-3">    n, bins, patches <span class="op" style="color: #5E5E5E;">=</span> plt.hist(x<span class="op" style="color: #5E5E5E;">=</span>batch_lengths_bucket_sorted, bins<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">50</span>, color<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'#0504aa'</span>,alpha<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.7</span>, rwidth<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.85</span>)</span>
<span id="cb32-4">    plt.grid(axis<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'y'</span>, alpha<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.75</span>)</span>
<span id="cb32-5">    plt.xlabel(<span class="st" style="color: #20794D;">'Batch Sequence Length'</span>,fontsize<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">15</span>)</span>
<span id="cb32-6">    plt.ylabel(<span class="st" style="color: #20794D;">'Frequency'</span>,fontsize<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">15</span>)</span>
<span id="cb32-7">    plt.xticks(fontsize<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">15</span>)</span>
<span id="cb32-8">    plt.yticks(fontsize<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">15</span>)</span>
<span id="cb32-9">    plt.ylabel(<span class="st" style="color: #20794D;">'Frequency'</span>,fontsize<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">15</span>)</span>
<span id="cb32-10">    plt.title(<span class="st" style="color: #20794D;">'Window Sorted Static Batching + Dynamic Padding Length Distribution'</span>,fontsize<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">15</span>)</span>
<span id="cb32-11">    plt.show()</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="saiprasanna.in/posts/efficient-dynamic-batching-of-large-datasets-with-infinibatch/index_files/figure-html/cell-20-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="checkpointing-1" class="level2">
<h2 class="anchored" data-anchor-id="checkpointing-1">3. Checkpointing</h2>
<p>One cool feature of <code>infinibatch</code> is that you can checkpoint a particular state in which the composed iterators is at and restore (rewind?) it back to that state. This is very cool considering it works recursively on the composed iterators and even on infinite iterator. Let’s recreate our iterators and check this out.</p>
<div class="cell" data-execution_count="20">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb33" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1">sentence_it <span class="op" style="color: #5E5E5E;">=</span> datasets.chunked_dataset_iterator(</span>
<span id="cb33-2">    chunk_refs <span class="op" style="color: #5E5E5E;">=</span> glob.glob(<span class="st" style="color: #20794D;">'wikitext-103-chunks/train.*.txt.gz'</span>),</span>
<span id="cb33-3">    read_chunk_fn <span class="op" style="color: #5E5E5E;">=</span> read_chunk,</span>
<span id="cb33-4">    buffer_size <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">100000</span>, </span>
<span id="cb33-5">    seed <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">1337</span>, </span>
<span id="cb33-6">    train<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>, </span>
<span id="cb33-7">    shuffle<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span></span>
<span id="cb33-8">)</span>
<span id="cb33-9">features_it <span class="op" style="color: #5E5E5E;">=</span> iterators.ParallelMapIterator(</span>
<span id="cb33-10">    source_iterator<span class="op" style="color: #5E5E5E;">=</span>sentence_it,</span>
<span id="cb33-11">    num_processes<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">4</span>,</span>
<span id="cb33-12">    num_items_per_process<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1000</span>,</span>
<span id="cb33-13">    transform<span class="op" style="color: #5E5E5E;">=</span>tokenize_fn</span>
<span id="cb33-14">)</span>
<span id="cb33-15">batches_it <span class="op" style="color: #5E5E5E;">=</span> iterators.BucketedReadaheadBatchIterator(</span>
<span id="cb33-16">    source_iterator<span class="op" style="color: #5E5E5E;">=</span>features_it,</span>
<span id="cb33-17">    read_ahead<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">10000</span>, <span class="co" style="color: #5E5E5E;"># Determines the window for the bucket which</span></span>
<span id="cb33-18">    <span class="co" style="color: #5E5E5E;"># will be sorted and  converted to batches.</span></span>
<span id="cb33-19">    key<span class="op" style="color: #5E5E5E;">=</span><span class="kw" style="color: #003B4F;">lambda</span> example: <span class="bu" style="color: null;">len</span>(example[<span class="st" style="color: #20794D;">'input_ids'</span>]), <span class="co" style="color: #5E5E5E;"># Determines the length used</span></span>
<span id="cb33-20">    <span class="co" style="color: #5E5E5E;"># to sort and choose the longest remaining record.</span></span>
<span id="cb33-21">    batch_size<span class="op" style="color: #5E5E5E;">=</span><span class="kw" style="color: #003B4F;">lambda</span> longest: tokens_per_batch <span class="op" style="color: #5E5E5E;">//</span> <span class="bu" style="color: null;">len</span>(longest[<span class="st" style="color: #20794D;">'input_ids'</span>]),</span>
<span id="cb33-22">    <span class="co" style="color: #5E5E5E;"># Determines the dynamic batch size</span></span>
<span id="cb33-23">    seed<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">0</span> </span>
<span id="cb33-24">)</span>
<span id="cb33-25">collate_fn <span class="op" style="color: #5E5E5E;">=</span> partial(tokenizer.pad, return_tensors<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'pt'</span>)</span>
<span id="cb33-26">tensors_it <span class="op" style="color: #5E5E5E;">=</span> iterators.MapIterator(</span>
<span id="cb33-27">    batches_it,</span>
<span id="cb33-28">    transform<span class="op" style="color: #5E5E5E;">=</span>collate_fn</span>
<span id="cb33-29">)</span></code></pre></div>
</details>
</div>
<div class="cell" data-outputid="157ac2b8-95c2-4021-af89-b9be82530835" data-execution_count="21">
<div class="sourceCode cell-code" id="cb34" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1">initial_state <span class="op" style="color: #5E5E5E;">=</span> tensors_it.getstate() </span>
<span id="cb34-2"><span class="bu" style="color: null;">print</span>(<span class="st" style="color: #20794D;">"Initial State of composed iterators"</span>, initial_state)</span>
<span id="cb34-3"><span class="co" style="color: #5E5E5E;"># Draw 5 batches</span></span>
<span id="cb34-4">batches <span class="op" style="color: #5E5E5E;">=</span> [<span class="bu" style="color: null;">next</span>(tensors_it) <span class="cf" style="color: #003B4F;">for</span> _ <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="dv" style="color: #AD0000;">5</span>)]</span>
<span id="cb34-5"><span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f"Current State after sampling 5 batches: </span><span class="sc" style="color: #5E5E5E;">{</span>tensors_it<span class="sc" style="color: #5E5E5E;">.</span>getstate()<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">"</span>)</span>
<span id="cb34-6"></span>
<span id="cb34-7"><span class="co" style="color: #5E5E5E;"># Reset the Iterator</span></span>
<span id="cb34-8">tensors_it.setstate(initial_state)</span>
<span id="cb34-9"><span class="co" style="color: #5E5E5E;"># Redraw 5 batches</span></span>
<span id="cb34-10">redraw_batches <span class="op" style="color: #5E5E5E;">=</span> [<span class="bu" style="color: null;">next</span>(tensors_it) <span class="cf" style="color: #003B4F;">for</span> _ <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="dv" style="color: #AD0000;">5</span>)]</span>
<span id="cb34-11"><span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f"State after resampling 5 batches: </span><span class="sc" style="color: #5E5E5E;">{</span>tensors_it<span class="sc" style="color: #5E5E5E;">.</span>getstate()<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">"</span>)</span>
<span id="cb34-12"></span>
<span id="cb34-13"></span>
<span id="cb34-14"><span class="co" style="color: #5E5E5E;"># Check equal</span></span>
<span id="cb34-15">all_equal <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">True</span></span>
<span id="cb34-16"><span class="cf" style="color: #003B4F;">for</span> b1, b2 <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">zip</span>(batches, redraw_batches):</span>
<span id="cb34-17">    <span class="cf" style="color: #003B4F;">for</span> k <span class="kw" style="color: #003B4F;">in</span> b1:</span>
<span id="cb34-18">        <span class="cf" style="color: #003B4F;">if</span> torch.<span class="bu" style="color: null;">all</span>(b1[k].eq(b2[k])):</span>
<span id="cb34-19">            <span class="cf" style="color: #003B4F;">continue</span></span>
<span id="cb34-20">        all_equal <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">False</span></span>
<span id="cb34-21">        <span class="cf" style="color: #003B4F;">break</span></span>
<span id="cb34-22">    <span class="cf" style="color: #003B4F;">if</span> <span class="kw" style="color: #003B4F;">not</span> all_equal:</span>
<span id="cb34-23">        <span class="cf" style="color: #003B4F;">break</span></span>
<span id="cb34-24"><span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f"All items drawn after resetting are equal: </span><span class="sc" style="color: #5E5E5E;">{</span>all_equal<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Initial State of composed iterators {'source_state': None, 'random_state': None, 'num_served': 0}
Current State after sampling 5 batches: {'source_state': {'source_state': None, 'flattened_items_yielded': 0}, 'random_state': (3, (2147483648, 766982754, 497961170, 3952298588, 2331775348, 1811986599, 3100132149, 3188119873, 3937547222, 215718963, 3315684082, 2978012849, 2428261856, 1298227695, 1704729580, 54668373, 3285201915, 3285178464, 1552935063, 988471319, 3135387943, 1691402966, 2757551880, 416056905, 907387413, 1072924981, 33903495, 2168419592, 2429050353, 831159753, 430343641, 3315943586, 1761671042, 864453023, 334804929, 1627478028, 2596811275, 3468733638, 3994375553, 1457139722, 3139722021, 1334790738, 2656639915, 3535811098, 1464315470, 2397423927, 885719490, 1140895889, 3284299483, 2854516462, 2734973817, 147484763, 792049954, 114360641, 3345458839, 1159898878, 1410498733, 2242989638, 453922141, 1344019764, 413870456, 3089405849, 1494382840, 470157779, 4266372830, 2831181573, 1361928602, 1589253513, 1381373062, 753045124, 987032420, 781978839, 2953638767, 3258570111, 3006718191, 1675218601, 1854232715, 3655829819, 1731242722, 2192104666, 1736665161, 740150002, 1195833394, 1610203160, 159492766, 4041488705, 3128952632, 2867295744, 3272632449, 886824304, 1791482600, 221114776, 3867175393, 4020804062, 1077871826, 1298953503, 996366221, 4149754679, 2483052703, 2615558283, 274318093, 1716359450, 4099129961, 1026774175, 288240973, 1459347562, 2365566296, 3690105224, 3065780221, 2050634722, 2652606621, 3185241207, 3026457375, 3456165734, 1880121515, 3398461093, 1795638629, 2379692076, 608668379, 1261955525, 84456522, 1913485156, 106878280, 757183891, 2913957588, 160418091, 2025664758, 141497907, 1657818026, 3053760160, 672193054, 4157546743, 223046484, 1623470498, 1201972930, 675008814, 684162366, 1738776330, 3025656654, 159760723, 1908867305, 3933381342, 2545706671, 467196949, 1427819885, 842150314, 4032903454, 2140851898, 3269883445, 975813755, 4177392955, 1556690684, 2535611513, 462962732, 67591358, 1729610528, 2025206740, 3153739740, 3255032049, 4186226368, 1070144624, 3107867195, 1621006038, 63742485, 835629717, 3189842019, 3950227584, 3184714559, 841836938, 1685394870, 657939920, 766156242, 1412314179, 1048281639, 4037161120, 2044490307, 1923947830, 3900790422, 907554295, 276417304, 860658646, 3574201134, 3508771399, 2110232300, 1636296241, 1405006077, 1093408401, 3243057343, 1519791182, 1994660136, 3829840937, 2644974199, 957955566, 3487641161, 1646922510, 1907939989, 3836029453, 3429168778, 201307778, 72550089, 2464394982, 1695794191, 3344785682, 996786130, 3589457196, 1241754792, 1291082245, 4224603667, 1194379475, 2693491244, 881186965, 2705535111, 445306946, 440274268, 1980827733, 2482488861, 3205215943, 2119332222, 2928713046, 1418736938, 652581136, 2474070665, 2208621536, 4171251876, 2303664214, 443762656, 2981912989, 2199228311, 2652261633, 3166738494, 3443009210, 3498764432, 424010848, 4065487566, 2262993542, 1756076712, 1477098233, 2742171915, 306185806, 3610666541, 923091830, 1034267993, 2336668648, 1880719718, 676878038, 3788797208, 3763351494, 3985428106, 1101865631, 1130501258, 3672967388, 3432003530, 4124438011, 1660392285, 4025484827, 2108074566, 3815409682, 42955331, 3248965569, 1643835718, 1246665668, 1071162194, 3814069229, 115491158, 985096811, 3311029186, 2990827378, 3101633320, 1648574497, 1470117052, 174145027, 2019894819, 2035501481, 459104123, 3507464599, 2093352659, 3369174406, 618767835, 4009895756, 935587447, 3956987426, 33753995, 307782427, 2473424805, 1440371818, 2382619594, 2138695812, 3164510238, 1318650933, 2910086616, 3886677510, 566832801, 3718063320, 1559818704, 183047272, 1142362855, 26306548, 645536402, 3875596208, 2272778168, 3512733409, 1897046338, 38248886, 2570759766, 1806313150, 860304898, 2433450338, 4124013408, 1216634590, 1275388896, 1169566669, 652504502, 761221427, 1448403764, 3129135949, 2513214949, 1269533687, 2413509541, 1226750363, 2450740925, 4094137910, 945759293, 3636927736, 3178020081, 2509964157, 3878869300, 1848504895, 2018369720, 1579755740, 1023627943, 924838836, 2653160914, 1812804174, 1521323076, 4012390528, 1338763317, 2608655937, 16022784, 1672945066, 2177189646, 2944458483, 2213810972, 1369873847, 1224017670, 130901785, 3595066712, 2259115284, 3316038259, 455873927, 2917250465, 3599550610, 1502173758, 684943436, 3079863840, 3144992244, 942855823, 1771140188, 2118780653, 3411494225, 2711180217, 4239611184, 1371891067, 3398566397, 3105518599, 1310665701, 3345178451, 2959821156, 242241789, 2148966880, 3192740583, 404401893, 3605380577, 1446464038, 3920522056, 2577523013, 1079274576, 286634372, 1752710796, 2351075979, 981312309, 3410516352, 3468455736, 1938779182, 1592494371, 1533303080, 88045436, 438252489, 1220512168, 3487004938, 3724852871, 1073434882, 3728218947, 2977555283, 4105408406, 3553772656, 1462006821, 3917158017, 119003006, 3470530198, 3439192457, 2829375771, 3555715155, 32324691, 588735808, 1459221702, 803072782, 2699519868, 1530797005, 79738580, 671990400, 4289511388, 3207115447, 2584684068, 832698998, 760958416, 1217440464, 2517898131, 2418819938, 3629956222, 3445024962, 206619378, 365007395, 522114139, 1707954431, 540423623, 1786750801, 369253262, 4239016754, 147889201, 1637777773, 236798285, 2806120188, 586972608, 2201782716, 1323327827, 819485723, 406078680, 3407345698, 1537169369, 1821691865, 527271655, 3751827102, 1465426495, 3321682429, 2179672664, 401355478, 1068871880, 24609462, 1403522408, 2311580015, 1532058170, 3877815340, 1768430711, 1619755157, 2832904331, 475102697, 354987331, 3295386430, 2816873951, 1039415736, 363972779, 1499307670, 2895506264, 3746345349, 2678027234, 3251899088, 955392878, 2329157295, 1343358773, 309573887, 2410178377, 2843173466, 361132917, 1755816798, 1319204283, 609284796, 1998842567, 1892325921, 223190385, 1483015769, 2876023365, 3876009312, 3199738344, 491524099, 160383137, 1219178873, 3870310498, 1114580266, 4279604166, 855339774, 1983818547, 2297848784, 4118592947, 4084409863, 2225095054, 4215601993, 946447434, 4205503762, 146088676, 778046685, 1876936928, 3157333726, 2173097090, 3215738813, 4135448234, 1219619643, 1936128689, 2897130162, 3336043946, 3779039524, 4200886837, 1359380925, 3402593091, 3140713935, 50855190, 3122065768, 1501584468, 2512255124, 687125154, 2666013386, 837819715, 3057258172, 3653455791, 2868624990, 322131992, 42534870, 4036564806, 798099710, 3533853670, 190914037, 3726947981, 2601169403, 602059656, 1365668439, 1918780004, 394790500, 277566007, 3891847777, 3365421094, 3139612253, 1380519090, 1183088424, 4203794803, 3049949521, 4214159484, 3446206962, 1875544460, 3207220027, 3288287026, 913535288, 178159620, 1410694581, 4190575040, 880731713, 1427805121, 404869072, 3413191414, 2865934056, 2899472677, 4239222733, 688404529, 3923323887, 933651074, 1199453686, 642723732, 2850614853, 3104368451, 3054041024, 3129913503, 2805843726, 1829781129, 3479062313, 650272704, 4224852052, 4085038685, 2616580676, 1793860711, 585126334, 2995262791, 520446536, 3855655015, 1571815563, 2240778227, 2051010344, 1694977983, 788402852, 1988089041, 2035558649, 1800063056, 1234412692, 2490862867, 417320514, 2415019489, 3374117797, 136034611, 898704236, 1247106941, 3923519397, 3563607190, 2454738671, 3522360389, 2672645476, 146828884, 3985140042, 4233949333, 1184742586, 860278824, 2815489967, 983483427, 3190081845, 3288865305, 3575181235, 1292151129, 4007823805, 4049420597, 3499391972, 1611182906, 1721268432, 2944249577, 2487212557, 789127738, 4027610014, 1057334138, 2902720905, 624), None), 'num_served': 5}
State after resampling 5 batches: {'source_state': {'source_state': None, 'flattened_items_yielded': 0}, 'random_state': (3, (2147483648, 766982754, 497961170, 3952298588, 2331775348, 1811986599, 3100132149, 3188119873, 3937547222, 215718963, 3315684082, 2978012849, 2428261856, 1298227695, 1704729580, 54668373, 3285201915, 3285178464, 1552935063, 988471319, 3135387943, 1691402966, 2757551880, 416056905, 907387413, 1072924981, 33903495, 2168419592, 2429050353, 831159753, 430343641, 3315943586, 1761671042, 864453023, 334804929, 1627478028, 2596811275, 3468733638, 3994375553, 1457139722, 3139722021, 1334790738, 2656639915, 3535811098, 1464315470, 2397423927, 885719490, 1140895889, 3284299483, 2854516462, 2734973817, 147484763, 792049954, 114360641, 3345458839, 1159898878, 1410498733, 2242989638, 453922141, 1344019764, 413870456, 3089405849, 1494382840, 470157779, 4266372830, 2831181573, 1361928602, 1589253513, 1381373062, 753045124, 987032420, 781978839, 2953638767, 3258570111, 3006718191, 1675218601, 1854232715, 3655829819, 1731242722, 2192104666, 1736665161, 740150002, 1195833394, 1610203160, 159492766, 4041488705, 3128952632, 2867295744, 3272632449, 886824304, 1791482600, 221114776, 3867175393, 4020804062, 1077871826, 1298953503, 996366221, 4149754679, 2483052703, 2615558283, 274318093, 1716359450, 4099129961, 1026774175, 288240973, 1459347562, 2365566296, 3690105224, 3065780221, 2050634722, 2652606621, 3185241207, 3026457375, 3456165734, 1880121515, 3398461093, 1795638629, 2379692076, 608668379, 1261955525, 84456522, 1913485156, 106878280, 757183891, 2913957588, 160418091, 2025664758, 141497907, 1657818026, 3053760160, 672193054, 4157546743, 223046484, 1623470498, 1201972930, 675008814, 684162366, 1738776330, 3025656654, 159760723, 1908867305, 3933381342, 2545706671, 467196949, 1427819885, 842150314, 4032903454, 2140851898, 3269883445, 975813755, 4177392955, 1556690684, 2535611513, 462962732, 67591358, 1729610528, 2025206740, 3153739740, 3255032049, 4186226368, 1070144624, 3107867195, 1621006038, 63742485, 835629717, 3189842019, 3950227584, 3184714559, 841836938, 1685394870, 657939920, 766156242, 1412314179, 1048281639, 4037161120, 2044490307, 1923947830, 3900790422, 907554295, 276417304, 860658646, 3574201134, 3508771399, 2110232300, 1636296241, 1405006077, 1093408401, 3243057343, 1519791182, 1994660136, 3829840937, 2644974199, 957955566, 3487641161, 1646922510, 1907939989, 3836029453, 3429168778, 201307778, 72550089, 2464394982, 1695794191, 3344785682, 996786130, 3589457196, 1241754792, 1291082245, 4224603667, 1194379475, 2693491244, 881186965, 2705535111, 445306946, 440274268, 1980827733, 2482488861, 3205215943, 2119332222, 2928713046, 1418736938, 652581136, 2474070665, 2208621536, 4171251876, 2303664214, 443762656, 2981912989, 2199228311, 2652261633, 3166738494, 3443009210, 3498764432, 424010848, 4065487566, 2262993542, 1756076712, 1477098233, 2742171915, 306185806, 3610666541, 923091830, 1034267993, 2336668648, 1880719718, 676878038, 3788797208, 3763351494, 3985428106, 1101865631, 1130501258, 3672967388, 3432003530, 4124438011, 1660392285, 4025484827, 2108074566, 3815409682, 42955331, 3248965569, 1643835718, 1246665668, 1071162194, 3814069229, 115491158, 985096811, 3311029186, 2990827378, 3101633320, 1648574497, 1470117052, 174145027, 2019894819, 2035501481, 459104123, 3507464599, 2093352659, 3369174406, 618767835, 4009895756, 935587447, 3956987426, 33753995, 307782427, 2473424805, 1440371818, 2382619594, 2138695812, 3164510238, 1318650933, 2910086616, 3886677510, 566832801, 3718063320, 1559818704, 183047272, 1142362855, 26306548, 645536402, 3875596208, 2272778168, 3512733409, 1897046338, 38248886, 2570759766, 1806313150, 860304898, 2433450338, 4124013408, 1216634590, 1275388896, 1169566669, 652504502, 761221427, 1448403764, 3129135949, 2513214949, 1269533687, 2413509541, 1226750363, 2450740925, 4094137910, 945759293, 3636927736, 3178020081, 2509964157, 3878869300, 1848504895, 2018369720, 1579755740, 1023627943, 924838836, 2653160914, 1812804174, 1521323076, 4012390528, 1338763317, 2608655937, 16022784, 1672945066, 2177189646, 2944458483, 2213810972, 1369873847, 1224017670, 130901785, 3595066712, 2259115284, 3316038259, 455873927, 2917250465, 3599550610, 1502173758, 684943436, 3079863840, 3144992244, 942855823, 1771140188, 2118780653, 3411494225, 2711180217, 4239611184, 1371891067, 3398566397, 3105518599, 1310665701, 3345178451, 2959821156, 242241789, 2148966880, 3192740583, 404401893, 3605380577, 1446464038, 3920522056, 2577523013, 1079274576, 286634372, 1752710796, 2351075979, 981312309, 3410516352, 3468455736, 1938779182, 1592494371, 1533303080, 88045436, 438252489, 1220512168, 3487004938, 3724852871, 1073434882, 3728218947, 2977555283, 4105408406, 3553772656, 1462006821, 3917158017, 119003006, 3470530198, 3439192457, 2829375771, 3555715155, 32324691, 588735808, 1459221702, 803072782, 2699519868, 1530797005, 79738580, 671990400, 4289511388, 3207115447, 2584684068, 832698998, 760958416, 1217440464, 2517898131, 2418819938, 3629956222, 3445024962, 206619378, 365007395, 522114139, 1707954431, 540423623, 1786750801, 369253262, 4239016754, 147889201, 1637777773, 236798285, 2806120188, 586972608, 2201782716, 1323327827, 819485723, 406078680, 3407345698, 1537169369, 1821691865, 527271655, 3751827102, 1465426495, 3321682429, 2179672664, 401355478, 1068871880, 24609462, 1403522408, 2311580015, 1532058170, 3877815340, 1768430711, 1619755157, 2832904331, 475102697, 354987331, 3295386430, 2816873951, 1039415736, 363972779, 1499307670, 2895506264, 3746345349, 2678027234, 3251899088, 955392878, 2329157295, 1343358773, 309573887, 2410178377, 2843173466, 361132917, 1755816798, 1319204283, 609284796, 1998842567, 1892325921, 223190385, 1483015769, 2876023365, 3876009312, 3199738344, 491524099, 160383137, 1219178873, 3870310498, 1114580266, 4279604166, 855339774, 1983818547, 2297848784, 4118592947, 4084409863, 2225095054, 4215601993, 946447434, 4205503762, 146088676, 778046685, 1876936928, 3157333726, 2173097090, 3215738813, 4135448234, 1219619643, 1936128689, 2897130162, 3336043946, 3779039524, 4200886837, 1359380925, 3402593091, 3140713935, 50855190, 3122065768, 1501584468, 2512255124, 687125154, 2666013386, 837819715, 3057258172, 3653455791, 2868624990, 322131992, 42534870, 4036564806, 798099710, 3533853670, 190914037, 3726947981, 2601169403, 602059656, 1365668439, 1918780004, 394790500, 277566007, 3891847777, 3365421094, 3139612253, 1380519090, 1183088424, 4203794803, 3049949521, 4214159484, 3446206962, 1875544460, 3207220027, 3288287026, 913535288, 178159620, 1410694581, 4190575040, 880731713, 1427805121, 404869072, 3413191414, 2865934056, 2899472677, 4239222733, 688404529, 3923323887, 933651074, 1199453686, 642723732, 2850614853, 3104368451, 3054041024, 3129913503, 2805843726, 1829781129, 3479062313, 650272704, 4224852052, 4085038685, 2616580676, 1793860711, 585126334, 2995262791, 520446536, 3855655015, 1571815563, 2240778227, 2051010344, 1694977983, 788402852, 1988089041, 2035558649, 1800063056, 1234412692, 2490862867, 417320514, 2415019489, 3374117797, 136034611, 898704236, 1247106941, 3923519397, 3563607190, 2454738671, 3522360389, 2672645476, 146828884, 3985140042, 4233949333, 1184742586, 860278824, 2815489967, 983483427, 3190081845, 3288865305, 3575181235, 1292151129, 4007823805, 4049420597, 3499391972, 1611182906, 1721268432, 2944249577, 2487212557, 789127738, 4027610014, 1057334138, 2902720905, 624), None), 'num_served': 5}
All items drawn after resetting are equal: True</code></pre>
</div>
</div>
<p>Since the <code>state</code> of the iterator is just a dictionary, you can serialize it along with your model weights and restore them to continue training from exact point where you have checkpointed it.</p>
</section>
</section>
<section id="making-infinibatch-work-with-pytorch-dataloaders" class="level1">
<h1>Making Infinibatch work with Pytorch Dataloaders</h1>
<p>Infinibatch by its very nature can be used only with <code>IterableDataset</code>. The training iterator with shuffling is infinite, so you must limit the training batches to some <code>n</code> steps if you want to maintain the notion of “epochs” to start validation. Or you can eschew whole notion of epochs by validating every <code>nth</code> step or both.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The multi processing workers of <code>DataLoader</code> should be set to zero, with <code>num_workers=0</code>. Rather use <code>ParallelMapIterator</code> to parallelize your pre-processing.</p>
</div>
</div>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>While using <code>IterableDataset</code> in the typical multi-gpu <code>DistributedDataParallel</code> (ddp) setup, you should pass <code>instance_rank</code> and <code>num_instances</code> to have different slices of data distributed to different training devices.</p>
</div>
</div>
<div class="callout-warning callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p>When using finite iterators with <code>ddp</code> for validation set, if you split the data using <code>instance_rank</code> option, the validation can get stuck.This can happen either when your dataset is not divisible by number of <code>ddp</code> processes or doing dynamic batching caused an uneven number of batches produced for each instance. So it’s better to do the validation in one GPU setting <code>instance_rank=0</code>. This is a quick hack, if you find a better option please let me know in the comments.</p>
</div>
</div>
<div class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb36" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><span class="im" style="color: #00769E;">from</span> torch.utils.data <span class="im" style="color: #00769E;">import</span> IterableDataset</span>
<span id="cb36-2"><span class="im" style="color: #00769E;">import</span> torch.distributed <span class="im" style="color: #00769E;">as</span> dist</span>
<span id="cb36-3"></span>
<span id="cb36-4"><span class="kw" style="color: #003B4F;">class</span> IterableCheckpointedDataset(IterableDataset):</span>
<span id="cb36-5">    <span class="co" style="color: #5E5E5E;">"""</span></span>
<span id="cb36-6"><span class="co" style="color: #5E5E5E;">    Wraps a CheckpointableIterator into a PyTorch IterableDataset, which is </span></span>
<span id="cb36-7"><span class="co" style="color: #5E5E5E;">    recognized by its type by PyTorch's DataLoader class.</span></span>
<span id="cb36-8"><span class="co" style="color: #5E5E5E;">    """</span></span>
<span id="cb36-9"></span>
<span id="cb36-10">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, source: iterators.CheckpointableIterator, </span>
<span id="cb36-11">                 should_reset: <span class="bu" style="color: null;">bool</span>):</span>
<span id="cb36-12">        <span class="bu" style="color: null;">super</span>().<span class="fu" style="color: #4758AB;">__init__</span>()</span>
<span id="cb36-13">        <span class="va" style="color: #111111;">self</span>._source <span class="op" style="color: #5E5E5E;">=</span> source</span>
<span id="cb36-14">        <span class="va" style="color: #111111;">self</span>._source_state <span class="op" style="color: #5E5E5E;">=</span> source.getstate()</span>
<span id="cb36-15">        <span class="va" style="color: #111111;">self</span>._should_reset <span class="op" style="color: #5E5E5E;">=</span> should_reset</span>
<span id="cb36-16"></span>
<span id="cb36-17">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__iter__</span>(<span class="va" style="color: #111111;">self</span>):  <span class="co" style="color: #5E5E5E;"># this is called in the forked clone</span></span>
<span id="cb36-18">        worker_info <span class="op" style="color: #5E5E5E;">=</span> torch.utils.data.get_worker_info()</span>
<span id="cb36-19">        <span class="cf" style="color: #003B4F;">assert</span> (</span>
<span id="cb36-20">            worker_info <span class="kw" style="color: #003B4F;">is</span> <span class="va" style="color: #111111;">None</span> <span class="kw" style="color: #003B4F;">or</span> worker_info.num_workers <span class="op" style="color: #5E5E5E;">==</span> <span class="dv" style="color: #AD0000;">1</span></span>
<span id="cb36-21">        )  <span class="co" style="color: #5E5E5E;"># not supported since we can't get at the checkpoint for each worker</span></span>
<span id="cb36-22"></span>
<span id="cb36-23">        <span class="cf" style="color: #003B4F;">if</span> <span class="va" style="color: #111111;">self</span>._should_reset:</span>
<span id="cb36-24">            <span class="co" style="color: #5E5E5E;"># For training, since it's infinite iterator, if we train for </span></span>
<span id="cb36-25">            <span class="co" style="color: #5E5E5E;"># `n` batches with total instances less than dataset size</span></span>
<span id="cb36-26">            <span class="co" style="color: #5E5E5E;"># it's better not to reset the iterator by itself will cycle back</span></span>
<span id="cb36-27">            <span class="co" style="color: #5E5E5E;"># with a new shuffle order when all the instances are iterated once.</span></span>
<span id="cb36-28">            <span class="va" style="color: #111111;">self</span>._source.setstate(<span class="va" style="color: #111111;">self</span>._source_state)</span>
<span id="cb36-29">        <span class="cf" style="color: #003B4F;">return</span> <span class="va" style="color: #111111;">self</span>._source</span>
<span id="cb36-30"></span>
<span id="cb36-31"></span>
<span id="cb36-32"></span>
<span id="cb36-33"><span class="kw" style="color: #003B4F;">def</span> create_wiki_dataloader(chunks_glob: <span class="bu" style="color: null;">str</span>,</span>
<span id="cb36-34">                           tokenizer: PreTrainedTokenizerFast,</span>
<span id="cb36-35">                           is_train: <span class="bu" style="color: null;">bool</span>,</span>
<span id="cb36-36">                           max_seq_len: <span class="bu" style="color: null;">int</span> <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">512</span>,</span>
<span id="cb36-37">                           tokens_per_batch: <span class="bu" style="color: null;">int</span> <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">2</span> <span class="op" style="color: #5E5E5E;">**</span> <span class="dv" style="color: #AD0000;">16</span>, </span>
<span id="cb36-38">                           num_workers: <span class="bu" style="color: null;">int</span> <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">4</span>,</span>
<span id="cb36-39">                           buffer_size: <span class="bu" style="color: null;">int</span> <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">100000</span>, </span>
<span id="cb36-40">                           seed: <span class="bu" style="color: null;">int</span> <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">1337</span>) <span class="op" style="color: #5E5E5E;">-&gt;</span> DataLoader:</span>
<span id="cb36-41"></span>
<span id="cb36-42">    num_instances <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">1</span></span>
<span id="cb36-43">    instance_rank <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">0</span></span>
<span id="cb36-44">    <span class="cf" style="color: #003B4F;">if</span> dist.is_available() <span class="kw" style="color: #003B4F;">and</span> is_train:</span>
<span id="cb36-45">        <span class="co" style="color: #5E5E5E;"># Only in training mode we want the data to be split.</span></span>
<span id="cb36-46">        <span class="co" style="color: #5E5E5E;"># This is a hack to make dynamic batched iterators work while using ddp.</span></span>
<span id="cb36-47">        <span class="co" style="color: #5E5E5E;"># If we were to split the data and number of batches turned out uneven, then</span></span>
<span id="cb36-48">        <span class="co" style="color: #5E5E5E;"># Iterator might exhaust early in one GPU leaving it stuck there forever.</span></span>
<span id="cb36-49">        <span class="co" style="color: #5E5E5E;"># So we rather choose to do the same validation in all the GPUs.</span></span>
<span id="cb36-50">        <span class="cf" style="color: #003B4F;">try</span>:</span>
<span id="cb36-51">            num_instances <span class="op" style="color: #5E5E5E;">=</span> dist.get_world_size()</span>
<span id="cb36-52">            instance_rank <span class="op" style="color: #5E5E5E;">=</span> dist.get_rank()</span>
<span id="cb36-53">        <span class="cf" style="color: #003B4F;">except</span> <span class="pp" style="color: #AD0000;">AssertionError</span>:</span>
<span id="cb36-54">            <span class="cf" style="color: #003B4F;">pass</span></span>
<span id="cb36-55"></span>
<span id="cb36-56">    sentence_it <span class="op" style="color: #5E5E5E;">=</span> datasets.chunked_dataset_iterator(</span>
<span id="cb36-57">        chunk_refs <span class="op" style="color: #5E5E5E;">=</span> glob.glob(chunks_glob),</span>
<span id="cb36-58">        read_chunk_fn <span class="op" style="color: #5E5E5E;">=</span> read_chunk,</span>
<span id="cb36-59">        buffer_size <span class="op" style="color: #5E5E5E;">=</span> buffer_size, </span>
<span id="cb36-60">        seed <span class="op" style="color: #5E5E5E;">=</span> seed,</span>
<span id="cb36-61">        num_instances<span class="op" style="color: #5E5E5E;">=</span>num_instances,</span>
<span id="cb36-62">        instance_rank<span class="op" style="color: #5E5E5E;">=</span>instance_rank,</span>
<span id="cb36-63">        train<span class="op" style="color: #5E5E5E;">=</span>is_train,</span>
<span id="cb36-64">        shuffle<span class="op" style="color: #5E5E5E;">=</span>is_train <span class="co" style="color: #5E5E5E;"># Shuffle Only on Train</span></span>
<span id="cb36-65">    )</span>
<span id="cb36-66">    tokenize_fn <span class="op" style="color: #5E5E5E;">=</span> partial(tokenizer, max_length<span class="op" style="color: #5E5E5E;">=</span>max_seq_len, truncation<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span>
<span id="cb36-67">    features_it <span class="op" style="color: #5E5E5E;">=</span> iterators.ParallelMapIterator(</span>
<span id="cb36-68">        source_iterator<span class="op" style="color: #5E5E5E;">=</span>sentence_it,</span>
<span id="cb36-69">        num_processes<span class="op" style="color: #5E5E5E;">=</span>num_workers,</span>
<span id="cb36-70">        num_items_per_process<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1000</span>,</span>
<span id="cb36-71">        transform<span class="op" style="color: #5E5E5E;">=</span>tokenize_fn</span>
<span id="cb36-72">    )</span>
<span id="cb36-73">    batches_it <span class="op" style="color: #5E5E5E;">=</span> iterators.BucketedReadaheadBatchIterator(</span>
<span id="cb36-74">        source_iterator<span class="op" style="color: #5E5E5E;">=</span>features_it,</span>
<span id="cb36-75">        read_ahead<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">10000</span>, </span>
<span id="cb36-76">        key<span class="op" style="color: #5E5E5E;">=</span><span class="kw" style="color: #003B4F;">lambda</span> example: <span class="bu" style="color: null;">len</span>(example[<span class="st" style="color: #20794D;">'input_ids'</span>]),</span>
<span id="cb36-77">        batch_size<span class="op" style="color: #5E5E5E;">=</span><span class="kw" style="color: #003B4F;">lambda</span> longest: tokens_per_batch <span class="op" style="color: #5E5E5E;">//</span> <span class="bu" style="color: null;">len</span>(longest[<span class="st" style="color: #20794D;">'input_ids'</span>]),</span>
<span id="cb36-78">        seed<span class="op" style="color: #5E5E5E;">=</span>seed</span>
<span id="cb36-79">    )</span>
<span id="cb36-80">    collate_fn <span class="op" style="color: #5E5E5E;">=</span> partial(tokenizer.pad, return_tensors<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'pt'</span>)</span>
<span id="cb36-81">    tensors_it <span class="op" style="color: #5E5E5E;">=</span> iterators.MapIterator(</span>
<span id="cb36-82">        batches_it,</span>
<span id="cb36-83">        transform<span class="op" style="color: #5E5E5E;">=</span>collate_fn</span>
<span id="cb36-84">    )</span>
<span id="cb36-85">    dataset <span class="op" style="color: #5E5E5E;">=</span> IterableCheckpointedDataset(</span>
<span id="cb36-86">        source<span class="op" style="color: #5E5E5E;">=</span>tensors_it,</span>
<span id="cb36-87">        should_reset<span class="op" style="color: #5E5E5E;">=</span><span class="kw" style="color: #003B4F;">not</span> is_train <span class="co" style="color: #5E5E5E;">#Reset only for validation</span></span>
<span id="cb36-88">    )</span>
<span id="cb36-89">    <span class="cf" style="color: #003B4F;">return</span> DataLoader(dataset, </span>
<span id="cb36-90">                      <span class="co" style="color: #5E5E5E;"># Very important to set this to 0.</span></span>
<span id="cb36-91">                      num_workers<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb36-92">                      <span class="co" style="color: #5E5E5E;"># Important as we have already batched. </span></span>
<span id="cb36-93">                      batch_size<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>,</span>
<span id="cb36-94">                      <span class="co" style="color: #5E5E5E;"># Since batch has only one member which has all the </span></span>
<span id="cb36-95">                      <span class="co" style="color: #5E5E5E;">#tensors already collated, we just return it.</span></span>
<span id="cb36-96">                      collate_fn<span class="op" style="color: #5E5E5E;">=</span><span class="kw" style="color: #003B4F;">lambda</span> x: x[<span class="dv" style="color: #AD0000;">0</span>] </span>
<span id="cb36-97">                      )</span>
<span id="cb36-98"></span>
<span id="cb36-99">tokenizer <span class="op" style="color: #5E5E5E;">=</span> AutoTokenizer.from_pretrained(<span class="st" style="color: #20794D;">"bert-base-cased"</span>, use_fast<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span>
<span id="cb36-100">train_loader <span class="op" style="color: #5E5E5E;">=</span> create_wiki_dataloader(<span class="st" style="color: #20794D;">'wikitext-103-chunks/train.*.txt.gz'</span>,</span>
<span id="cb36-101">                                      tokenizer<span class="op" style="color: #5E5E5E;">=</span>tokenizer,</span>
<span id="cb36-102">                                      is_train<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span>
<span id="cb36-103"></span>
<span id="cb36-104">val_loader <span class="op" style="color: #5E5E5E;">=</span>  create_wiki_dataloader(<span class="st" style="color: #20794D;">'wikitext-103-chunks/train.*.txt.gz'</span>,</span>
<span id="cb36-105">                                      tokenizer<span class="op" style="color: #5E5E5E;">=</span>tokenizer,</span>
<span id="cb36-106">                                      is_train<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>)</span>
<span id="cb36-107"></span>
<span id="cb36-108"><span class="co" style="color: #5E5E5E;">#print(next(iter(train_loader)))</span></span>
<span id="cb36-109"><span class="co" style="color: #5E5E5E;">#print(next(iter(val_loader)))</span></span></code></pre></div>
</div>


</section>

 ]]></description>
  <guid>saiprasanna.in/posts/efficient-dynamic-batching-of-large-datasets-with-infinibatch/index.html</guid>
  <pubDate>Sun, 13 Sep 2020 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Roam Research - Software for building a Second Brain</title>
  <link>saiprasanna.in/posts/roam-research-software-for-building-a-second-brain/index.html</link>
  <description><![CDATA[ 




<section id="introduction" class="level1">
<h1>Introduction</h1>
<p><img src="saiprasanna.in/posts/roam-research-software-for-building-a-second-brain/https:/roamresearch.com/assets/images/Roam-Group-min.png" class="img-fluid"> <a href="roamresearch.com">Roam Research</a> is a revolutionary note-taking/knowledge management software. It is designed with the idea that data structure for a second brain should be associative (graph) rather than a rigid hierarchy. It is meant for everyone who needs to manage their knowledge effectively. Their founder aims for the product to become more or less excel for knowledge management. Professor Balaji Srinivas introduced it to me. I have been using this for about a month and fell in love with its features. Canadian philosopher who predicted the web tells that “Medium is the message”. The properties of a medium in which communication occurs can impact society a lot. I believe that roam is one such tool that shifts the medium of note-taking in a groundbreaking manner. The features of Roam allow you to arrange for the serendipity of ideas, unexpected connections with your past, present and future selves. It is built on simple building blocks that come together (emergent property) to make the whole greater than the sum of its parts. Each building block of Roam might look simple if looked separately, but together they become very powerful. Note: It is still in beta with a pricing yet to be announced. But if you try it I think you will share this sentiment. <img src="saiprasanna.in/posts/roam-research-software-for-building-a-second-brain/https:/scaledynamix.com/wp-content/uploads/2018/08/takemymoney.jpg" class="img-fluid"> Here is the <a href="https://roamresearch.com/#/v8/help/page/Vu1MmjinS">Roam white paper</a> written by founders on a Public roam database about why they think Roam is revolutionary.</p>
</section>
<section id="pain-points-of-most-knowledge-management-software" class="level1">
<h1>Pain points of most knowledge-management software</h1>
<p>I have used popular note-taking software like evernote, One-note, Zoho notebook etc. There was always huge friction with these tools. Getting them to work for Note taking, journaling, and project management was such a pain. The only tool that came close was org-mode in Emacs. I used it for the past one year. But even it had a lot of friction due to the reasons I will expand upon below. And emacs is most definitely not for popular use, it is only for the chosen few who are blessed enough to reject the cult of mouse.</p>
<section id="failure-of-files-inside-folders-way-of-organization." class="level2">
<h2 class="anchored" data-anchor-id="failure-of-files-inside-folders-way-of-organization.">Failure of Files inside Folders way of organization.</h2>
<p>Most note-taking software or even physical note-taking follow a file-folder system or a single hierarchy of bullets inside bullets (Outlining tools) for organizing notes. I will refer to this as “files inside folder” model going forward now. This system creates nested hierarchy of categories, sub-categories and so on. Inside which your notes are put.</p>
<section id="why-this-approach-fails" class="level3">
<h3 class="anchored" data-anchor-id="why-this-approach-fails"><strong>Why this approach fails?</strong></h3>
<p>You might have started taking notes/journaling etc for a few days and abandon it after some time. The following are the main reasons I think this happens.</p>
<p><strong>Friction caused by a static hierarchy of the folder system</strong></p>
<p>Every time you want to write a note you have to decide where to put it in the hierarchy. You have to ask yourselves which notebook/folder/file should I write this, for it to be useful?</p>
<p><strong>Poor Return in investment for good note-taking</strong></p>
<p>You painstakingly take notes or journal your exercise regime or note down something. But it never surfaces automatically when you write something related. Most notes are passive and useless unless you look for them. Most notes are hidden uselessly in the hierarchy where you put it in.</p>
</section>
<section id="practical-scenario-where-folder-model-fails" class="level3">
<h3 class="anchored" data-anchor-id="practical-scenario-where-folder-model-fails">Practical Scenario where folder model fails</h3>
<p>Say you had a discussion with your friend about note-taking while sipping a coffee in a cafe. You talk about some personal stuff which say you want to put in your journal. S/he brings about about a new book which you find interesting and want to read. Also, you really like the coffee in the cafe and want to note it down in your list of the favourite coffee shops.</p>
<p><strong>Now how do you take notes in this case?</strong> In a rigid hierarchy, for this note to be useful you have to file it under multiple places “To read books”, “Daily Journal”, “Meets”, “Friend’s name” But that is highly impractical because of the effort/redundancy involved. You would have to copy paste same information in multiple places or manually create links in multiple pages. And if those pages don’t exist you would have to create them.</p>
</section>
</section>
<section id="simple-tags-dont-solve-this-problem" class="level2">
<h2 class="anchored" data-anchor-id="simple-tags-dont-solve-this-problem">Simple Tags don’t solve this problem</h2>
<p>Most note-taking software provide tagging to solve this problem. But this creates friction of adding tags to everything you write. And more importantly the tags are flat. i.e.&nbsp;They have no hierarchy between them. Tags solve searching for notes, but not disovery. This makes it problematic if you want to discover notes when you stumble through a specific context automatically.</p>
</section>
<section id="we-think-in-a-associative-graphs-manner-not-as-a-rigid-hierarchy-trees" class="level2">
<h2 class="anchored" data-anchor-id="we-think-in-a-associative-graphs-manner-not-as-a-rigid-hierarchy-trees">We think in a associative (Graphs) manner not as a rigid hierarchy (Trees)</h2>
<p>Every thought/idea in our brain has a bunch of associations. Associations like people who introduced us to the idea, what we want to do with it, associations with books we read about it, tasks we completed based on it, tasks we want to do, date in which we did it etc. So this forms a “graph” (in computer science) where every idea is a node and is linked to many other ideas. When you think of an idea naturally you get reminded of the stuff it is associated with. But the problem is we forget stuff, which is the reason why we are doing note-taking in the first place. <strong>What if a note-taking software allows you to mirror how the brain works?</strong></p>
</section>
</section>
<section id="building-blocks-of-roam-that-differentiate-it" class="level1">
<h1>Building blocks of Roam that differentiate it</h1>
<p>These are the fundamental set of features that make Roam what it is. Since showing is better than telling, I am including 1-minute videos of how each fundamental building blocks of Roam work.</p>
<section id="basic-layout" class="level2">
<h2 class="anchored" data-anchor-id="basic-layout">Basic Layout</h2>
<p><iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/3SwQ4usbCX4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe> Roam’s page layout is like any other “outlining” tool. ie It has bullets which can be nested within each other infinitely and the bullets can be collapsed. There is no folder system. All notes are be seen from “All Pages” view in left side-bar, but it is rarely needed because of way roam’s navigation is organized. Roam’s home page is the Daily notes page where you can see pages titled by dates. This is the basic dumping ground for all your quick note-taking and journaling, daily tasks, habit tracking for the day.</p>
</section>
<section id="friction-free-link-creation" class="level2">
<h2 class="anchored" data-anchor-id="friction-free-link-creation">Friction Free Link Creation</h2>
<p><iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/lHkMq3aqDtw" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe> Basic way roam allows linking between pages is typing the note title inside two square brackets “[[]]” which includes autocomplete/search to all notes page titles. One Important thing here is if the note page doesn’t exist a new page gets create. This would seem weird when coming from other applications. But it serves a big purpose you will see next.</p>
</section>
<section id="bi-directional-linking" class="level2">
<h2 class="anchored" data-anchor-id="bi-directional-linking">Bi-directional Linking</h2>
<p><iframe width="854" height="480" src="https://www.youtube.com/embed/v9s3pusI1JQ" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe> This is the key feature of Roam. When a page is linked to, when you visit the page you can see all the places where it has been referred.</p>
<p>Say you write “I was reading this cool article on [[Deep Learning]]”, The Deep Learning page will have a back link to all the places it has been mentioned. So every page becomes akin to a tag, but associations between them form a dynamic/organic hierarchy. &gt; “Every page is a tag, and every tag is a page” - <a href="https://www.nateliason.com/blog/roam">Nat Elison’s blog on Roam</a></p>
</section>
<section id="un-linked-references" class="level2">
<h2 class="anchored" data-anchor-id="un-linked-references">Un-linked References</h2>
<p><iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/nROryUttSr0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe> If you have already mentioned a topic in lots of notes, but didn’t create a page for it and link them. Roam has got your back with its super cool un-linked references. When you create a new page, you can easily bulk link every other page that has mentions of the current page.</p>
</section>
<section id="ability-to-refer-or-embed-any-blockbullet-anywhere" class="level2">
<h2 class="anchored" data-anchor-id="ability-to-refer-or-embed-any-blockbullet-anywhere">Ability to refer or embed any block/bullet anywhere</h2>
<p><iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/ZFbrdv-70ME" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe> You can link to or embed any block written anywhere in roam notes. Roam prompts a search/autocomplete to any block you have written in all the pages. How awesome is that? To create a block reference two open Parentheses <code>((</code> Or type <code>/Block Reference.</code> You can also embed the entire block using Block embed. Type <code>/Block Embed</code> When embedding or linking to a block you can see the places it has been used by clicking on the number which appears at top right side of a block.</p>
</section>
<section id="all-back-links-and-block-embeds-are-editable" class="level2">
<h2 class="anchored" data-anchor-id="all-back-links-and-block-embeds-are-editable">All back-links and block embeds are editable</h2>
<p><iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/qg9uS6LlCf0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe> The coolest part of roam is all blocks (bullets) displayed by back-links and block embeds are editable. You can edit embedded notes and back-links with no duplication. So you easily remix (refactor?) notes by creating a new note with just embeds from multiple other notes in other places.</p>
</section>
<section id="navigating-with-full-text-search" class="level2">
<h2 class="anchored" data-anchor-id="navigating-with-full-text-search">Navigating with Full-text Search</h2>
<p><iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/Al69VbgKVw0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe> Roam provides full text search of all the blocks and titles. You can create a brand new page which is not linked to any other page directly from search.</p>
</section>
<section id="graph-view" class="level2">
<h2 class="anchored" data-anchor-id="graph-view">Graph View</h2>
<p><iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/OXqN4u7lKac" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe> Since every note is basically a node in graph, roam easily allows a bird’s eye of your entire graph in the “Graph Overview” page. A more helpful feature is ability to view what nodes current page has connections to and navigate visually.</p>
</section>
<section id="side-bar" class="level2">
<h2 class="anchored" data-anchor-id="side-bar">Side Bar</h2>
<p><iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/7dASSNABtIo" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe> Roam sidebar allows you to open multiple notes at a time. This is really useful when you want to aggregate knowledge across notes.</p>
</section>
<section id="filters-on-bi-directional-links" class="level2">
<h2 class="anchored" data-anchor-id="filters-on-bi-directional-links">Filters on Bi-directional Links</h2>
<p><iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/BnwWdTnXlxU" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe> When you have too many bi-directional links in a page, you can filter them to include or exclude other links.</p>
</section>
</section>
<section id="building-a-second-brain-with-roam" class="level1">
<h1>Building a second brain with Roam</h1>
<p>In this section, I will describe how Roam can be used for multiple use-cases at once thereby building your second brain.</p>
<section id="note-taking" class="level2">
<h2 class="anchored" data-anchor-id="note-taking">Note taking</h2>
<p>If you read a lot and want to retain the knowledge. Reading stuff and writing it in your own words will be a good way to test gaps in understanding. This <a href="https://www.nateliason.com/blog/smart-notes">article</a> a effective way to take smart notes using roam. I found <a href="https://fortelabs.co/blog/how-to-take-smart-notes">this summary</a> of a book called “[[How to Take Smart Notes - Book]] really helpful.</p>
</section>
<section id="journal" class="level2">
<h2 class="anchored" data-anchor-id="journal">Journal</h2>
<p>Writing a journal is a bread and (peanut) butter of roam. Daily notes encourages you to write daily at anytime. It feels really encouraging to journal in roam as unlike other apps because of the back-links. Anything recorded will automatically get associated with all the topics.</p>
</section>
<section id="task-management" class="level2">
<h2 class="anchored" data-anchor-id="task-management">Task management</h2>
<p><iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/asQ4RSjjCu4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe> Roam supports basic todos, you can use links to link tasks to the date it has to be done using date-picker. The key advantage here is your project management tasks can easily be linked to the meeting notes, research notes, and journal etc. Getting things done (GTD) is a popular method to manage tasks. It is very easy to implement that in roam with the aid of back-links. You can read how to adopt GTD in roam <a href="https://oliverschmid.space/posts/gtd-in-roam/">here</a>.</p>
</section>
<section id="bookmarks" class="level2">
<h2 class="anchored" data-anchor-id="bookmarks">Bookmarks</h2>
<p><iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/7b2AVCZOMnw" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe> You can easily put links into roam with creating tags/links. Tags are same as “[[]]” links, just that the font is greyed out.</p>
</section>
<section id="personal-crm" class="level2">
<h2 class="anchored" data-anchor-id="personal-crm">Personal CRM</h2>
<p>Personal crm is for maintaining a list of people, their contact, birthdays, how you met them or anything else you want to maintain about them. In roam you can easily create pages for people, and refer it in your daily notes. So when you go to the person’s page, you can see all the places h/she has been mentioned.</p>
</section>
<section id="content-creation" class="level2">
<h2 class="anchored" data-anchor-id="content-creation">Content Creation</h2>
<p>For writing new content, you can easily remix stuff which you wrote across different pages in a new page. So you will never have the feeling of starting at a blank page when you have done your research and taken notes on it.</p>
</section>
</section>
<section id="other-useful-features" class="level1">
<h1>Other useful features</h1>
<section id="query" class="level2">
<h2 class="anchored" data-anchor-id="query">Query</h2>
<p><iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/AlmhG6nTl9M" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe> This is an advanced feature, where you can query on your graph to show blocks that satisfy boolean conditions. Like show me all the blocks with todos with high priority etc.</p>
</section>
<section id="shortcuts" class="level2">
<h2 class="anchored" data-anchor-id="shortcuts">Shortcuts</h2>
<p><img src="saiprasanna.in/posts/roam-research-software-for-building-a-second-brain/https:/nimbus-screenshots.s3.amazonaws.com/s/823247391734eda9aec6dd9353f33a2b.png" class="img-fluid"> This is useful to keep the most important projects</p>
</section>
<section id="tables-diagrams-kanban-boards" class="level2">
<h2 class="anchored" data-anchor-id="tables-diagrams-kanban-boards">Tables, Diagrams, Kanban boards</h2>
<p>Tables can be created using <code>/Table</code> command followed by nested bullets. <img src="saiprasanna.in/posts/roam-research-software-for-building-a-second-brain/https:/nimbus-screenshots.s3.amazonaws.com/s/e3e5189d83b0ab1d85e5debe2d0d7207.png" class="img-fluid"></p>
</section>
<section id="embedding-media" class="level2">
<h2 class="anchored" data-anchor-id="embedding-media">Embedding Media</h2>
<p>Embed tweet by just pasting a twitter link. Type backslash followed by image markdown to insert a markdown for image embed. <code>/Image Markdown</code> Alternatively images can be uploaded by pasting directly or with backslash command <code>/Upload a image</code> backslash command for it to be uploaded and the markdown inserted automatically. For youtube videos, use <code>/Embed Youtube Video</code> command. Interesting point here is all the embeds are markdown or markdown like plain text syntax. No proprietary garbage.</p>
</section>
<section id="publishing-your-roam-notes" class="level2">
<h2 class="anchored" data-anchor-id="publishing-your-roam-notes">Publishing your Roam Notes</h2>
<p>You can share a note by its URL to public as read-only or even allow public edits.</p>
</section>
<section id="sharing-your-second-brain-to-form-a-hive-mind" class="level2">
<h2 class="anchored" data-anchor-id="sharing-your-second-brain-to-form-a-hive-mind">Sharing your second Brain to form a Hive mind</h2>
<p>Currently you can share the entire roam database and collabrate with peers. More fine-grained controls of sharing parts of the graph are in the works.</p>
</section>
<section id="works-offline-progressive-web-app" class="level2">
<h2 class="anchored" data-anchor-id="works-offline-progressive-web-app">Works offline (Progressive Web App)</h2>
<p>The mobile apps are coming shortly, but the web app is designed so well that it can function offline.</p>
</section>
<section id="exports-to-plain-text" class="level2">
<h2 class="anchored" data-anchor-id="exports-to-plain-text">Exports to plain text</h2>
<p>This is important for anyone who cares about not getting locked out of your data. Since the product is in development better to take regular backups if you plan to use it.</p>
</section>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<p>The concept of bi-directional editable media in Roam can be transposed to many other products with lots of unstructured text trapped in a rigid hierarchy. Roam is a well-designed innovative product that leapfrogs over existing products. This I think is because of unorthodox thinking of its founders (<a href="https://twitter.com/Conaw">Conor White-Sulivan</a> and Joshua Brown) to build useful tools for augmenting our brain. Turing award winner and Computer Science legend Alan Kay once said: “The best way to predict the future is to invent it.”. He also thought that there is a lot more that can be done to make computers truly augmenting the intellect. I think these founders are trying to fulfil that dream. Roam seems like an application that can give compounding returns of value for the knowledge put in it. This is totally refreshing in an age where apps suck your attention. Finally, I am more into engineering than content writing for productivity software. This software was so good that I couldn’t help writing this article to share it with others.</p>


</section>

 ]]></description>
  <category>Productivity</category>
  <category>Note Taking</category>
  <guid>saiprasanna.in/posts/roam-research-software-for-building-a-second-brain/index.html</guid>
  <pubDate>Wed, 25 Mar 2020 00:00:00 GMT</pubDate>
  <media:content url="https://roamresearch.com/assets/images/Roam-Group-min.png" medium="image" type="image/png"/>
</item>
<item>
  <title>Neural Module Networks</title>
  <link>saiprasanna.in/posts/neural-module-networks/index.html</link>
  <description><![CDATA[ 




<p>Research Paper - <a href="https://arxiv.org/abs/1511.02799">https://arxiv.org/abs/1511.02799</a></p>
<p>Authors - Jacob Andreas, Marcus Rohrbach, Trevor Darrell, Dan Klein ## Key Idea Parse questions of visual QA into a description of compositions of functions. These functions are neural networks called Neural Modules. Execute the neural networks and reweigh the resulting label using question representation. <img src="saiprasanna.in/posts/neural-module-networks/https:/firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fartificial_sai%2FImqekKqTqJ?alt=media&amp;token=f46ba375-0d38-4c81-b40f-f691c4099a74.png" class="img-fluid" alt="Architecture diagram"> ## Task - Visual Question Answering Given a question like “What color is the coffee mug?” and an image we want to predict the answer. <img src="saiprasanna.in/posts/neural-module-networks/https:/visualqa.org/static/img/vqa_examples.jpg" class="img-fluid" alt="4 visual qa examples. One example: Two images having a man and women. One with man wearing glasses and another image with woman wearing glasses.Question, “Who is wearing glasses?” and respective answer below the image"> ## Prior approaches</p>
<p><strong>End to End neural networks</strong></p>
<p>Use a CNN to vectorize the image and RNN to vectorize the question and use a feed forward network to classify the answer. This is a black box trying to answer in one shot.</p>
<p><strong>Semantic Parsing approach</strong></p>
<p>Parse the question into logical expressions, image into logical representation of the world and use logic based reasoning to solve the problem. This is more compositional. ## Motivation Combine the representational capacity of neural nets and compositionality of symbolic approach.</p>
<blockquote class="blockquote">
<p>“Rather than thinking of question answering as a problem of learning a single function to map from questions and contexts to answers, it’s perhaps useful to think of it as a highly-multitask learning setting, where each problem instance is associated with a novel task, and the identity of that task is expressed only noisily in language.”</p>
</blockquote>
<p>Simple example - “Is this a truck?” - Needs single task to be performed, namely truck or not classification.</p>
<p>Compositional example - “What is the object to the left of the tea pot?” - Needs one to find the teapot, detect object to its left, then classify the object. ## Architecture ### Neural Modules Identify set of modules that can be composed to solve all/most tasks. Modules can be thought of as a function parametrized by a neural network, with a type signature. Data Types - Image, Unnormalized attention map, labels <img src="saiprasanna.in/posts/neural-module-networks/https:/firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fartificial_sai%2FCukmmNqjRE?alt=media&amp;token=f63a5c71-e85f-4936-a3db-d6c2e210da46.png" class="img-fluid" alt="Attention Module"> <img src="saiprasanna.in/posts/neural-module-networks/https:/firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fartificial_sai%2Fn0rzgVl42h?alt=media&amp;token=74d06d94-d7a2-4911-9f10-dac8a01a7970.png" class="img-fluid" alt="Classification Module"> <img src="saiprasanna.in/posts/neural-module-networks/https:/firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fartificial_sai%2FnOFbj1IDtR?alt=media&amp;token=044075ba-b992-4618-905e-e4a5e69fe4a4.png" class="img-fluid" alt="Reattention module"> <img src="saiprasanna.in/posts/neural-module-networks/https:/firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fartificial_sai%2FGhc8_NHxUn?alt=media&amp;token=b06a6670-84f7-4c65-89da-9475008fe5c3.png" class="img-fluid" alt="Combination module"> <img src="saiprasanna.in/posts/neural-module-networks/https:/firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fartificial_sai%2FQt95yqX3Cf?alt=media&amp;token=b2628c2d-ca01-48fd-b596-c4e01e491823.png" class="img-fluid" alt="Measurement Module"> ### Strings -&gt; Modules <strong>Parsing</strong> Use few rules on dependency parse of the question to convert it into a structured query. e.g.&nbsp;“Is there a circle next to a square?” -&gt; is(circle, next-to(square)) <strong>Layout</strong> “All leaves become attend modules, all internal nodes become re-attend or combine modules dependent on their arity, and root nodes become measure modules for yes/no questions and classify modules for all other question types.” The queries could come from anywhere not just natural language question. As long as they can be converted to a layout in the end. ### Answering An RNN is used to process the question and predict a label directly without looking into the image. This is combined with the final label from the root node of the Neural Modules using geometric mean to get the final result. This is done for 2 reasons <strong>Syntactic Regularity/Prior</strong> When converting to structured query, certain syntactic elements are lost. For e.g.&nbsp;What is in the sky? and What are in the sky? both result in what(fly). But answer varies from kite to kites. <strong>Semantic Regularity/Prior</strong> Some answers are unreasonable just by inspecting the question. For example, What colour is the bear? eliminates all non-colour answers. ## Benchmarks They try this in vqa dataset - https://visualqa.org/ a huge dataset with natural images and questions with answers. <img src="saiprasanna.in/posts/neural-module-networks/https:/d3i71xaburhd42.cloudfront.net/21c99706bb26e9012bfb4d8d48009a3d45af59b2/7-Table3-1.png" class="img-fluid" alt="Benchmarks table for VQA"> Since VQA doesn’t have many deep compositional questions, they use shapes a synthetically generated dataset. <img src="saiprasanna.in/posts/neural-module-networks/https:/d3i71xaburhd42.cloudfront.net/21c99706bb26e9012bfb4d8d48009a3d45af59b2/7-Table2-1.png" class="img-fluid" alt="Synthetic Shapes dataset"> ## Examples What colour is his tie? <img src="saiprasanna.in/posts/neural-module-networks/https:/d3i71xaburhd42.cloudfront.net/21c99706bb26e9012bfb4d8d48009a3d45af59b2/5-Figure2-1.png" class="img-fluid" alt="Statue of a man with yellow tie, question parsed to modules 1. find tie 2. describe colour"> <img src="saiprasanna.in/posts/neural-module-networks/https:/firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fartificial_sai%2FacB53oMkSP?alt=media&amp;token=a75a6545-1c5f-44f6-9ed5-3f17d26061b0.png" class="img-fluid" alt="Correct and incorrect predictions"></p>



 ]]></description>
  <category>Semantic Parsing</category>
  <category>Visual Q/A</category>
  <category>Symbolic AI</category>
  <category>CV</category>
  <category>NLP</category>
  <category>Research Paper Summary</category>
  <guid>saiprasanna.in/posts/neural-module-networks/index.html</guid>
  <pubDate>Mon, 02 Mar 2020 00:00:00 GMT</pubDate>
  <media:content url="https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fartificial_sai%2FImqekKqTqJ?alt=media&amp;token=f46ba375-0d38-4c81-b40f-f691c4099a74" medium="image"/>
</item>
<item>
  <title>ACL 2019 Conference Summary</title>
  <link>saiprasanna.in/posts/ACL-2019-Conference-Summary/index.html</link>
  <description><![CDATA[ 




<p>My colleague Ananda and I attended <a href="https://www.aclweb.org/portal/">ACL 2019 conference</a> at the enchanting city of Florence. All the accepted papers can be accessed <a href="https://www.aclweb.org/anthology/events/acl-2019/">here</a>. Here’s the summary of interesting trends and also specific research work that caught my eye at the conference. A note of thanks to my employer at Zoho for sponsoring us to attend.</p>
<p><em>I wrote this summary an many months ago and forgot posting it. Better late than never I guess.</em></p>
<section id="grammatical-error-correction" class="level2">
<h2 class="anchored" data-anchor-id="grammatical-error-correction">Grammatical Error Correction</h2>
<ul>
<li>Among the ACL workshops, Building Educational Applications (BEA) Workshop had a <a href="https://www.cl.cam.ac.uk/research/nl/bea2019st/">Grammar Error Correction competition</a>.</li>
</ul>
<p>The system description papers for this competition were presented as posters in the conference.</p>
<ul>
<li>Three tracks were present in the competition. <strong>Restricted track</strong> - Only organizer provided human labelled parallel (error and corrected sentence pairs) data can be used. (No restriction on synthetic data) <strong>Unrestricted track</strong> - Any data including private data can be used. <strong>Low Resource track</strong> - No human labelled data can be used.</li>
<li>Interestingly, the winning team (Edinburgh + Microsoft)’s <a href="https://www.aclweb.org/anthology/W19-4427/">submission</a> for Track 1 also beat Track 2 without using additional restricted data.</li>
<li>Synthetic data generated by corrupting good grammatical sentences from news, books and wikipedia are the techniques used overall by top performing teams.</li>
</ul>
</section>
<section id="multi-lingual-models" class="level2">
<h2 class="anchored" data-anchor-id="multi-lingual-models">Multi-Lingual Models</h2>
<p>MultiLingual models is a hot area of research now. Earlier results where using single model to perform tasks on multiple languages has shown promising results.</p>
<ul>
<li>Lots of papers on multi-lingual shared models were presented.</li>
<li>Paper - <a href="https://www.aclweb.org/anthology/papers/P/P19/P19-1301/">Choosing Transfer Languages for Cross-Lingual Learning</a></li>
</ul>
</section>
<section id="rise-of-automated-metrics" class="level2">
<h2 class="anchored" data-anchor-id="rise-of-automated-metrics">Rise of Automated Metrics</h2>
<p>Until recently, we compare model outputs with human written sentences for translation, summarization etc. This can artificially penalize models that generate sentences with equivalent meaning but not same words. There are couple of papers that train models to score quality of the output. Then use these model scores as reward for reinforcement learning. (FYI reinforcement learning is only used for fine tuning, none of the seq2seq models can be trained from scratch using it)</p>
<ul>
<li>Paper - <a href="https://www.aclweb.org/anthology/papers/P/P19/P19-1043/">This Email Could Save Your Life: Introducing the Task of Email Subject Line Generation</a> This paper uses automated score instead of typical NGram match (ROUGE) score for summarization task.</li>
<li>Paper - <a href="https://www.aclweb.org/anthology/papers/P/P19/P19-1427/">Beyond BLEU:Training Neural Machine Translation with Semantic Similarity</a></li>
<li>Paper - <a href="https://www.aclweb.org/anthology/papers/P/P19/P19-1264/">Sentence Mover’s Similarity: Automatic Evaluation for Multi-Sentence Texts</a></li>
</ul>
</section>
<section id="statistical-evaluation" class="level2">
<h2 class="anchored" data-anchor-id="statistical-evaluation">Statistical Evaluation</h2>
<p>If we have two architectures and couple of datasets, how to say empirically one is better than the other? Few questions are how to compare two models on the same dataset, across multiple datasets, across various hyperparameter configurations. Problems in applying frequentist tests on the metrics such as accuracy, f1-score etc are that assumptions such as Independent and Identically distributed (IID) cannot be made for deep learning datasets. So we cannot assume that the score the model gets in one dataset is “independent” of the score on another dataset. Statistical tests that don’t assume underlying distribution are needed. Recent statistical methods/tests to do so are being developed and some were presented at the conference.</p>
<ul>
<li>Paper - <a href="https://www.aclweb.org/anthology/papers/P/P19/P19-1266/%20">Deep Dominance - How to Properly Compare Deep Neural Models</a></li>
<li>Paper - <a href="https://www.aclweb.org/anthology/papers/P/P19/P19-1405/">Bayes Test of Precision, Recall, and F1 Measure for Comparison of Two Natural Language Processing Models</a></li>
</ul>
</section>
<section id="bayesian-methods" class="level2">
<h2 class="anchored" data-anchor-id="bayesian-methods">Bayesian Methods</h2>
<ul>
<li>Attended a very detailed tutorial on it. The presenter has summarized the evolution of research in this area and the current papers. Here’s link to the detailed <a href="https://drive.google.com/file/d/1SgNVpspG-m0O_k-_qAbxg-3HSZg3FOec/view?usp=sharing">slides</a> for fellow Bayesians.</li>
</ul>
</section>
<section id="analyzing-neural-nets-and-interpretability" class="level2">
<h2 class="anchored" data-anchor-id="analyzing-neural-nets-and-interpretability">Analyzing Neural Nets and Interpretability</h2>
<p>There is an entire sub-fields of research into analyzing and interpreting neural networks.</p>
<section id="bertology" class="level3">
<h3 class="anchored" data-anchor-id="bertology">BERTology</h3>
<p>“BERT-ology” papers that explore what linguistic structures do pre-trained models like BERT learn.</p>
<ul>
<li>Paper - <a href="https://www.aclweb.org/anthology/papers/W/W19/W19-4828/">What Does BERT Look at? An Analysis of BERT’s Attention</a></li>
</ul>
</section>
<section id="blackboxnlp-workshop" class="level3">
<h3 class="anchored" data-anchor-id="blackboxnlp-workshop"><a href="https://www.aclweb.org/anthology/volumes/W19-48/">BlackBoxNLP Workshop</a></h3>
<p>An entire workshop devoted for analyzing what Neural Networks learn.</p>
<ul>
<li>Paper - <a href="https://www.aclweb.org/anthology/papers/W/W19/W19-4814/">On the Realization of Compositionality in Neural Networks</a> Interesting paper studying what is required for neural models to compose two very trivial functions.</li>
<li>Paper - <a href="https://www.aclweb.org/anthology/papers/W/W19/W19-4826/">GEval: Tool for Debugging NLP Datasets and Models</a></li>
</ul>
</section>
<section id="formal-languages-workshop" class="level3">
<h3 class="anchored" data-anchor-id="formal-languages-workshop"><a href="https://www.aclweb.org/anthology/volumes/W19-39/%20">Formal Languages Workshop</a></h3>
<p>An entire small workshop devoted to finding what Formal Languages (Finite state Automata, etc) neural networks can learn. e.g.&nbsp;Can we reduce a RNN to Weighted Finite State Machine (which is far more interpretable, amenable to theory etc). Although this area sounds exciting to me, I was unable to attend it as I was in an another workshop. Slides from talk of Noah Smith’s talk on <a href="https://homes.cs.washington.edu/~nasmith/slides/rrnn-dlfl-2019-08-02.pdf">Rational Recurrences</a> at this workshop.</p>
</section>
<section id="neuroscience-and-nlp" class="level3">
<h3 class="anchored" data-anchor-id="neuroscience-and-nlp">Neuroscience and NLP</h3>
<p>Neuroscience labs have started to use deep learning. An interesting conjunction of research in NLP and neuroscience research in correlating ANN representations with brain signals was presented.</p>
<ul>
<li>Paper - <a href="https://www.aclweb.org/anthology/papers/P/P19/P19-1507/">Relating Simple Sentence Representations in Deep Neural Networks and the Brain</a> The researchers try to find relationship between deep learning language representations and brain signals. Paper of interest is where they predict neural brain patterns using pre-trained ANN models like BERT.</li>
</ul>
</section>
<section id="language-emergence-in-multi-agent-systems" class="level3">
<h3 class="anchored" data-anchor-id="language-emergence-in-multi-agent-systems">Language Emergence in Multi-Agent systems</h3>
<p>In this frontier, people try train models to solve some task by communicating symbols. Researchers analyze the properties of language used by the agents to solve the task and how it compares with properties of human language.</p>
<ul>
<li>Paper - <a href="https://www.aclweb.org/anthology/papers/P/P19/P19-1509/">Word-order Biases in Deep-agent Emergent Communication</a></li>
</ul>
</section>
</section>
<section id="conversational-ai" class="level2">
<h2 class="anchored" data-anchor-id="conversational-ai">Conversational AI</h2>
<ul>
<li>Neural Models for selecting conversation from past history, detecting intent and slot fitting are all increasingly being deployed by companies.</li>
<li>PolyAI (a startup at Singapore shipping conversational AI) shared three <a href="https://twitter.com/poly_ai/status/1154027323810861057/photo/1">interesting papers</a>. Their slides are also <a href="https://www.matthen.com/assets/pdf/Neural%2520Models%2520of%2520Response%2520Selection%2520for%2520Bootstrapping%2520Dialogue%2520Systems.pdf">interesting</a>.</li>
<li>On a related note, Baidu has is doing impressive research and engineering on meeting transcription. They have a stack that does speech to text, translating the text as its spoken (a problem that needed separate research as the text would be incomplete), detecting english phrases being spoken (code switching) and then NLP over the transcribed text.</li>
</ul>
</section>
<section id="translation" class="level2">
<h2 class="anchored" data-anchor-id="translation">Translation</h2>
<ul>
<li>Lots of new work on adapting translation models for low-resource languages.</li>
<li>Unsupervised translation, Multi-lingual translation models are few areas of research.</li>
<li>Unbabel a YC funded startup doing translation systems shared lots of interesting and important results. <a href="https://www.aclweb.org/anthology/W18-2103">Slides from their talk</a>. This company employs a hybrid system where human translators do “post-edits” on machine translations. And some of their system work in real-time.</li>
</ul>
</section>
<section id="contextual-search-using-neural-representations-at-scale" class="level2">
<h2 class="anchored" data-anchor-id="contextual-search-using-neural-representations-at-scale">Contextual Search using Neural Representations at scale</h2>
<p>This <a href="https://arxiv.org/abs/1906.05807">paper</a> has demonstrated a system which does dense vector search on entire wikipedia for open domain QA.</p>
<p>Scaling search on neural vectors to do question answering on entire wikipedia on CPU - <a href="https://github.com/uwnlp/denspi" class="uri">https://github.com/uwnlp/denspi</a></p>
<p>Demo - <a href="http://allgood.cs.washington.edu:15001/" class="uri">http://allgood.cs.washington.edu:15001/</a></p>


</section>

 ]]></description>
  <category>Computer Science</category>
  <category>Machine Learning</category>
  <category>Conference</category>
  <category>ACL</category>
  <guid>saiprasanna.in/posts/ACL-2019-Conference-Summary/index.html</guid>
  <pubDate>Mon, 10 Feb 2020 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Semantic Legion</title>
  <link>saiprasanna.in/posts/semantic-legion-1/index.html</link>
  <description><![CDATA[ 




<p>I am guilty of spamming people in the degree one of my network with too many links in topics that fancy the Legion of varied interests that haunt me. Following the suggestion of Ananda Seelan, I am consolidating my link blasts into a considated blog post format, thus begins the “Semantic Legion”. This exercise might help organize the “Legion” in my head and maybe lead to more focused blog posts.</p>
<blockquote class="blockquote">
<p>“My name is Legion, for we are many.”</p>
</blockquote>
<section id="machine-learning" class="level2">
<h2 class="anchored" data-anchor-id="machine-learning">Machine Learning</h2>
<section id="the-lottery-ticket-hypothesis-finding-sparse-trainable-neural-networks" class="level3">
<h3 class="anchored" data-anchor-id="the-lottery-ticket-hypothesis-finding-sparse-trainable-neural-networks"><a href="https://arxiv.org/abs/1803.03635">The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks</a></h3>
<p>The lottery ticket hypothesis suggests that big Deep Neural Nets train better than smaller nets because they get lucky. Essentially like someone who has purchased more number of lottery tickets.</p>
<ol type="1">
<li>Prune a large neural network by zeroing the bottom x% of weights by magnitude. This can be done one shot or iteratively while training.</li>
<li>Reset the obtained subnetwork weights to the exact weights you randomly intialized before training the large neural network.</li>
<li>The pruned subnetwork converges to similar test error rate as the full network or even better in the same number of epochs.</li>
<li>The authors notice that if you were try some other initialization for the subnetwork or even sample from similar distribution it doesn’t work. Hence they hypothesise that the larger network essentially got lucky.</li>
</ol>
<p>Though the subnetwork is smaller, computations will need sparse matrix multiplication optimizations to be faster.</p>
</section>
<section id="understanding-the-generalization-of-lottery-tickets-in-neural-networks" class="level3">
<h3 class="anchored" data-anchor-id="understanding-the-generalization-of-lottery-tickets-in-neural-networks"><a href="https://ai.facebook.com/blog/understanding-the-generalization-of-lottery-tickets-in-neural-networks">Understanding the generalization of ‘lottery tickets’ in neural networks</a></h3>
<p>Facebook extends the study and checks it for various architectures, tasks and optimizer setting. The lottery ticket phenomena seems to occur in most places. The lottery ticket subnetworks generalize across datasets. This blog post is a summary of multiple papers by Facebook AI group in analyzing this phenomena.</p>
</section>
<section id="new-theory-cracks-open-the-black-box-of-deep-learning" class="level3">
<h3 class="anchored" data-anchor-id="new-theory-cracks-open-the-black-box-of-deep-learning"><a href="https://www.quantamagazine.org/new-theory-cracks-open-the-black-box-of-deep-learning-20170921/">New Theory Cracks Open the Black Box of Deep Learning</a></h3>
<p>Information bottleneck theory a hypothesis about how neural nets learn is creating some buzz. One of the claims is that the output of earlier layers have more mutual information with the inputs while final layer outputs have more mutual information with the outputs than the inputs. The information about input gets compressed in each layer.</p>
<p><img src="saiprasanna.in/posts/semantic-legion-1/https:/d2r55xnwy6nx47.cloudfront.net/uploads/2017/09/DeepLearning_5001.jpg" class="img-fluid" alt="Information Bottleneck process"> {: style=“text-align: center;”} <em><a href="https://www.quantamagazine.org/new-theory-cracks-open-the-black-box-of-deep-learning-20170921/">Lucy Reading-Ikkanda/Quanta Magazine; adapted from arXiv:1703.00810 [cs.LG]</a></em> {: style=“color:gray; font-size: 80%; text-align: center;”}</p>
</section>
<section id="evolution-of-representations-in-the-transformer" class="level3">
<h3 class="anchored" data-anchor-id="evolution-of-representations-in-the-transformer"><a href="https://lena-voita.github.io/posts/emnlp19_evolution.html">Evolution of Representations in the Transformer</a></h3>
<p>This is a great practical example of using information bottlenecks to analyze neural nets behaviour. This research (accompanied by inspirationally well written blog post) compares the evolution of representations in three different NLP encoder models. And in part explains some empirical findings such as why de-noising objective works better than casual language model objective or encoders from translation objective for transfer learning.</p>
</section>
<section id="universal-adversarial-triggers-for-attacking-and-analyzing-nlp-wallace-et-al.-emnlp-19" class="level3">
<h3 class="anchored" data-anchor-id="universal-adversarial-triggers-for-attacking-and-analyzing-nlp-wallace-et-al.-emnlp-19"><a href="http://www.ericswallace.com/triggers">Universal Adversarial Triggers for Attacking and Analyzing NLP (Wallace et al.&nbsp;EMNLP 19)</a></h3>
<p>This paper finds magic spells that make your NLP models malfunction. They find phrases that cause a specific model prediction when concatenated to 𝘢𝘯𝘺 input from a dataset. These phrases are reported to work across architectures for the same dataset.</p>
<pre><code>Triggers cause:

1. GPT-2 to spew racism
2. SQuAD models to answer "to kill american people" for 72% of questions asking "Why..."
3. Classification models to drop from 90% accuracy to 1%</code></pre>
</section>
<section id="allennlp-interpret" class="level3">
<h3 class="anchored" data-anchor-id="allennlp-interpret"><a href="https://allennlp.org/interpret">AllenNLP Interpret</a></h3>
<p>This is a great set of features for interpretability added to AllenNLP library.</p>
<blockquote class="blockquote">
<p>We present AllenNLP Interpret, a toolkit built on top of AllenNLP for interactive model interpretations. The toolkit makes it easy to apply gradient-based saliency maps and adversarial attacks to new models, as well as develop new interpretation methods. AllenNLP interpret contains three components: a suite of interpretation techniques applicable to most models, APIs for developing new interpretation methods (e.g., APIs to obtain input gradients), and reusable front-end components for visualizing the interpretation results.</p>
</blockquote>
<p>The amazing thing here is with implementing a simple interface in your model predictor allows you to apply a suite of interpretability techniques for our models.</p>
</section>
<section id="aidungeon2-is-here" class="level3">
<h3 class="anchored" data-anchor-id="aidungeon2-is-here"><a href="http://www.aidungeon.io/2019/12/aidungeon2-is-here.html">AIDungeon2 is here</a></h3>
<p>This is a real fun application of langauge model generation. <a href="https://twitter.com/nickwalton00">Nick Walton</a> has adapted GPT2 to generate user guided “Choose your own” text RPG type games. Now you can try out anything you fancy by just issuing commands like “Cast a spell to Reverse entropy”. A truly open world RPG with a AI dungeon master. The model weaves your actions to generalte plausible/surreal story continuations. <a href="https://news.ycombinator.com/item?id=21717022">Hacker News</a> discussion about it. The nature of the model make them generate surreal dream like scenarios. There are glaring consistency issues in the generated story lines. This points to a symbolic gap that is yet to be filled.</p>
<p><img src="saiprasanna.in/posts/semantic-legion-1/https:/encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcSMELPoU7Br4TBHmaDn-eCYqQMFFrFUPlELxS1pYR1i3iPBOLTO.png" class="img-fluid" alt="AIDungeon 2 generated story example."> {: style=“text-align: center;”} <em><a href="https://aiweirdness.com/post/189511103367/play-ai-dungeon-2-become-a-dragon-eat-the-moon">Source: aiweirdness.com</a></em> {: style=“color:gray; font-size: 80%; text-align: center;”}</p>
</section>
<section id="controlling-text-generation-with-plug-and-play-language-models" class="level3">
<h3 class="anchored" data-anchor-id="controlling-text-generation-with-plug-and-play-language-models"><a href="https://eng.uber.com/pplm/">Controlling Text Generation with Plug and Play Language Models</a></h3>
<p>On the topic of controlling language models, uber research has found a way to control the generation of models like GPT2 without fine-tuning.</p>
</section>
</section>
<section id="quantum-computing-linear-algebra-tools-for-learning" class="level2">
<h2 class="anchored" data-anchor-id="quantum-computing-linear-algebra-tools-for-learning">Quantum Computing, Linear Algebra, Tools for Learning</h2>
<section id="quantum-computing-for-the-very-curious" class="level3">
<h3 class="anchored" data-anchor-id="quantum-computing-for-the-very-curious"><a href="https://quantum.country/qcvc">Quantum Computing for the Very Curious</a></h3>
<p>I wanted to try out Micheal Nielsen’s (of neuralnetworksanddeeplearning.com fame) Quantum computing article. This long-form educational article attempts a unique teaching method by embedding flash cards (anki cards) and reminding readers via email to revisit the cards. I got around doing it at behest of the amzing Professor Balaji (a teacher of mine) who gave this as an exercise to test Linear Algebra understanding. Prior knowledge of the truly abstract nature of linear algebra (basis, linear transformations, linear combinations) really helped me to grok the essay.</p>
<p>The learning approach taken by this article (embedding flash cards + reminders) article shows how computing medium can be extended to augment our understanding. This scratches the surface of Alan Kay’s vision of computers being tools that extend our mind.</p>
</section>
<section id="augmenting-long-term-memory" class="level3">
<h3 class="anchored" data-anchor-id="augmenting-long-term-memory"><a href="http://augmentingcognition.com/ltm.html">Augmenting Long-term Memory</a></h3>
<p>If you’re curious about spaced repition flash card approach to learn new math theorems, machine learning concepts etc Micheal has written extensively about it in the above link.</p>
</section>
<section id="anki-flash-cards-with-spaced-repitition" class="level3">
<h3 class="anchored" data-anchor-id="anki-flash-cards-with-spaced-repitition"><a href="https://apps.ankiweb.net/">Anki Flash Cards with Spaced Repitition</a></h3>
<p>The free app Anki is example of good software aimed at expanding our capabilites rather than popular objective of draining attention. It has web, desktop and mobile versions for creating Anki (flash) cards with spaced repitition tracking. I am in the process of adopting it for my learning. Not yet successful in integrating it fully, will blog more about my experience in future.</p>
</section>
<section id="polar-app" class="level3">
<h3 class="anchored" data-anchor-id="polar-app"><a href="https://getpolarized.io/">Polar App</a></h3>
<p>Related learning tool I found is Polar. &gt; “A powerful document manager for web pages, textbooks, PDFs, and anything you want to read. Supports tagging, annotation, highlighting and keeps track of your reading progress.”</p>
<p>It doesn’t have a firefox extension yet. But it allows creating anki cards that can be synced to Anki app from web highlights. This helps in creating a learning expereince like the quantum computing blog for any document.</p>
</section>
</section>
<section id="philosophy" class="level2">
<h2 class="anchored" data-anchor-id="philosophy">Philosophy</h2>
<section id="would-aliens-understand-lambda-calculus" class="level3">
<h3 class="anchored" data-anchor-id="would-aliens-understand-lambda-calculus"><a href="http://tomasp.net/blog/2018/alien-lambda-calculus/">Would aliens understand lambda calculus?</a></h3>
<p>Platonism vs Aristotelianism is an age old debate in philosophy. Professor Balaji (a teacher of mine) had a strong notion that the current mathematics we have is strongly influenced by our spatio-visual sense. Stumbled upon the above post which makes similar claims. It claims that certain cognitive priors are necessary to converge upon ideas which some consider as universal.</p>
<p>I don’t know enough to lean on any side of the debate heavily. But my intution lies with universality/platonism of physics, mathematics and computatability. I think even if Alien’s use some other metaphors to arrive at Lambda Calculus, the underlying notion of universal computability (if correct) will be the same.</p>
</section>
<section id="new-ai-strategy-mimics-how-brains-learn-to-smell" class="level3">
<h3 class="anchored" data-anchor-id="new-ai-strategy-mimics-how-brains-learn-to-smell"><a href="https://www.quantamagazine.org/new-ai-strategy-mimics-how-brains-learn-to-smell-20180918/">New AI Strategy Mimics How Brains Learn to Smell</a></h3>
<p>I am now exploring search systems over neural net generated representation (vector spaces). This generally involves approximate methods such as Locality senstive Hashing. The method described in this post was interestingly derived from the sense of smell of fruit-flies. This lends some weight top the notion that our cognitive reliance on certain senses (vision) makes some ideas intutive, but exploring outside it can expand our horizons. (Purely my speculation to be taken with a grain of salt.)</p>
</section>
</section>
<section id="programming-languages" class="level2">
<h2 class="anchored" data-anchor-id="programming-languages">Programming Languages</h2>
<section id="type-state-pattern" class="level3">
<h3 class="anchored" data-anchor-id="type-state-pattern"><a href="http://cliffle.com/blog/rust-typestate/">Type State Pattern</a></h3>
<p>To eliminate errors make them impossible in runtime is a mantra I stand behind. Programming Patterns that are finally entering mainstream (after stewing in the academic functional world) such as Optional are moving errors to compile time. Among the patterns, type state caught my eye. Using rust’s borrow checker and other langauge features allows one to build compile time state machines. They can be as simple as allowing the compiler to disallow methods such as read on file references that are closed. Or it can be taken one step beyond to write a full blown state machines that track the current state in compile time. ie Say you have an API that needs a handshake to be performed before sending, you can ensure in compile time that the “send” method can be called only after “handshake” is called. How awesome is that.</p>
</section>
<section id="why-monads-matter" class="level3">
<h3 class="anchored" data-anchor-id="why-monads-matter"><a href="https://cdsmith.wordpress.com/2012/04/18/why-do-monads-matter/">Why Monads matter?</a></h3>
<p>This article explains what the usally hyped functional programming concept of monad solves for a imperative programmer. I have not dived deeply into any functional language yet. Seeing how even weakly adopted fucntional programming concepts such as Optionals (algebraic data types) and Optional Chaining (which is a monad) makes me question what is the cost with which the programming world is ignoring Functional paradigmn. Are the functional languages difficult to learn, or is it exposure bias towards imperative languages? Or do we need the functional abstractions to be put in better terms for people to grok them? Only time will tell.</p>


</section>
</section>

 ]]></description>
  <category>Computer Science</category>
  <category>Machine Learning</category>
  <category>Philosophy</category>
  <category>Semantic Legion</category>
  <guid>saiprasanna.in/posts/semantic-legion-1/index.html</guid>
  <pubDate>Sun, 08 Dec 2019 00:00:00 GMT</pubDate>
  <media:content url="https://d2r55xnwy6nx47.cloudfront.net/uploads/2017/09/DeepLearning_5001.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>SemEval 2019 - Semi-Supervised Domain Adaptation for Suggestion mining</title>
  <link>saiprasanna.in/posts/semeval-19-semi-supervised-domain-adaptation-for-suggestion-mining/index.html</link>
  <description><![CDATA[ 




<p>SemEval Workshop regularly has been conducting tasks in NLP to evaluate the progress in the field.</p>
<p>I and my colleague Ananda Seelan participated in this year SemEval’s Suggestion mining task (Task 9). Here is our <a href="https://arxiv.org/abs/1902.10623">submission</a> to be published in NAACL 2019 proceedings, and the code is on <a href="https://github.com/sai-prasanna/suggestion-mining-semeval19">github</a>.</p>
<p>This blog is a summary of the key techniques and ideas which influenced this work.</p>
<section id="suggestion-mining-task" class="level2">
<h2 class="anchored" data-anchor-id="suggestion-mining-task">Suggestion Mining Task</h2>
<p>The suggestion mining task in brief is a text classification task to find whether a sentence contains a suggestion.</p>
<p>Example,</p>
<ul>
<li><strong>Suggestion</strong> - It would be nice if they had vegan options.</li>
<li><strong>Non Suggestion</strong> - This restaurant has good vegan options.</li>
</ul>
<p>About 8k sentences scrapped from technical forumns were provided as training data. The task was divided into two subtasks.</p>
<ul>
<li>Subtask A - Evaluation on same domain - technical forums posts.</li>
<li>Subtask B - Evaluation on out of domain - hotel reviews.</li>
</ul>
<p>The catch for subtask B is human labelled data in hotel reviews domain is not allowed for training. Our model was placed third place in the leaderboard for Subtask B.</p>
</section>
<section id="key-techniques" class="level2">
<h2 class="anchored" data-anchor-id="key-techniques">Key Techniques</h2>
<p>We used simple convolutional neural networks for text classification. And we applied transfer learning and semi-supervised learning for the tasks.</p>
<section id="transfer-learning" class="level3">
<h3 class="anchored" data-anchor-id="transfer-learning">Transfer Learning</h3>
<p>The current trend in machine learning for NLP is to using pre-trained language models. We used google’s recently published BERT model as our representation layer. Take a look at <a href="http://jalammar.github.io/illustrated-bert/" class="uri">http://jalammar.github.io/illustrated-bert/</a> for a good description of how pre-trained models work for NLP.</p>
</section>
<section id="semi-supervised-learning" class="level3">
<h3 class="anchored" data-anchor-id="semi-supervised-learning">Semi-Supervised Learning</h3>
<p>In ACL 2018 conference Melbourne, I attended two talks which impacted the work in this paper. One was Sebastien Ruder’s talk on Strong baselines for semi-supervised learning in NLP. The conclusion of <a href="https://aclweb.org/anthology/P18-1096">Sebastian Ruder, Barbara Plank (2018)</a> was that classic machine learning techniques for semi-supervised learning such as Tri-Training prove as strong baseline in NLP with neural nets. Sebastien has a very accessible and thorough <a href="http://ruder.io/semi-supervised/">blog post</a> explaining the techniques. They have also made their code available on <a href="https://github.com/bplank/semi-supervised-baselines">github</a>.</p>
<iframe src="https://player.vimeo.com/video/285802189" width="640" height="360" frameborder="0" webkitallowfullscreen="" mozallowfullscreen="" allowfullscreen="">
</iframe>
<p>We applied a variant of tri-training. We use three models of the same architecture trained initially on data <a href="https://en.wikipedia.org/wiki/Bootstrapping_(statistics)">bootstrap sampled</a> from the tech reviews data. The three models are used to iteratively label unlabelled data from hotel reviews domain. Agreement of labels between two models is used as way to select sentences to be added to next iteration of training. Pseudo-code and detailed explanations can be found in the paper. Or you might as well look to the code, as its way simpler than a dry description of it might suggest.</p>
</section>
</section>
<section id="statistical-significance" class="level2">
<h2 class="anchored" data-anchor-id="statistical-significance">Statistical Significance</h2>
<p>The other work <a href="https://aclweb.org/anthology/P18-1128">published</a> and presented in ACL 2018 that influenced this paper is Rotem Dror’s “The Hitchhiker’s Guide to Statistical Significance in NLP”.</p>
<iframe src="https://player.vimeo.com/video/285803636" width="640" height="360" frameborder="0" webkitallowfullscreen="" mozallowfullscreen="" allowfullscreen="">
</iframe>
<p>We report confidence intervals for five random seeds for all our experiments. And we also do pair-wise significance testing via McNemar’s test to evaluate whether pair-wise model performance on the test set vary significantly.</p>
<section id="metrics" class="level3">
<h3 class="anchored" data-anchor-id="metrics">Metrics</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="saiprasanna.in/posts/semeval-19-semi-supervised-domain-adaptation-for-suggestion-mining/https:/ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/bf7927798419277eea7063f40d4329f8b8fa31ad/3-Table1-1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">img</figcaption><p></p>
</figure>
</div>
</section>
<section id="mcnemars-test" class="level3">
<h3 class="anchored" data-anchor-id="mcnemars-test">McNemar’s Test</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="saiprasanna.in/posts/semeval-19-semi-supervised-domain-adaptation-for-suggestion-mining/https:/ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/bf7927798419277eea7063f40d4329f8b8fa31ad/5-Table3-1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">img</figcaption><p></p>
</figure>
</div>


</section>
</section>

 ]]></description>
  <category>NLP</category>
  <category>Semi-Supervised Learning</category>
  <category>Research Paper Summary</category>
  <guid>saiprasanna.in/posts/semeval-19-semi-supervised-domain-adaptation-for-suggestion-mining/index.html</guid>
  <pubDate>Sun, 07 Apr 2019 00:00:00 GMT</pubDate>
  <media:content url="https://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/bf7927798419277eea7063f40d4329f8b8fa31ad/3-Table1-1.png" medium="image" type="image/png"/>
</item>
<item>
  <title>God’s own Programming Language</title>
  <link>saiprasanna.in/posts/gods-own-programming-language/index.html</link>
  <description><![CDATA[ 




<p>Found this interesting <a href="https://twobithistory.org/2018/10/14/lisp.html">blog post</a> explores why many programmers hold a high regard for an ancient programming language which you might not have heard about or use daily.</p>
<pre><code>For God wrote in Lisp code
When he filled the leaves with green.
The fractal flowers and recursive roots:
The most lovely hack I’ve seen.
And when I ponder snowflakes,
never finding two the same,
I know God likes a language
with its own four-letter name.</code></pre>
<p>{: style=“text-align: center;”}</p>
<p><em>Poem from <a href="https://twobithistory.org/2018/10/14/lisp.html">twobithistory.org</a></em> {: style=“color:gray; font-size: 80%; text-align: center;”}</p>
<p><img src="saiprasanna.in/posts/gods-own-programming-language/https:/imgs.xkcd.com/comics/lisp.jpg" class="img-fluid" alt="XKCD LISP - We think God created the world in LISP, but he merely hacked it in Perl."> <em><a href="https://xkcd.com/224/">XKCD Comics</a></em> {: style=“color:gray; font-size: 80%; text-align: center;”}</p>
<p>Also a good read are the <a href="http://www.paulgraham.com/lisp.html">posts on LISP</a> by Paul Graham of YCombinator/HackerNews fame.</p>
<section id="how-to-attain-nirvana-with-the-gods-own-language" class="level2">
<h2 class="anchored" data-anchor-id="how-to-attain-nirvana-with-the-gods-own-language">How to attain Nirvana with the God’s own language?</h2>
<p>To attain programming nirvana - Start reading the SICP Book.</p>
<p>SICP (Structure and Interpretation of Computer Programs) is a introduction to computer science book. It can change how you view even simple constructs we use for code (like loops, if, etc). A book that will teach timeless concepts in programming which you would never come across easily. I started reading it ages ago, haven’t completed it , the first few chapters themselves were sufficiently mind blowing.</p>
<p><strong><a href="https://web.mit.edu/alexmv/6.037/sicp.pdf">Original book</a></strong></p>
<p><strong><a href="https://xuanji.appspot.com/isicp/1-1-elements.html">A modern interactive version</a></strong> Now you can run the examples of SICP book God’s own language in the browser with godforsaken javascript. And laugh morosely on the irony of running LISP in a half baked language (javascript) which was inspired from it.</p>
<p><strong><a href="http://www.sicpdistilled.com/">A Distilled version with illustrations</a></strong> - Smaller, condensed version.</p>
<blockquote class="blockquote">
<p>To use an analogy, if SICP were about automobiles, it would be for the person who wants to know how cars work, how they are built, and how one might design fuel-efficient, safe, reliable vehicles for the 21st century. The people who hate SICP are the ones who just want to know how to drive their car on the highway, just like everyone else. <strong>- Peter Norvig on SICP</strong></p>
</blockquote>
<section id="how-to-write-a-lisp-interpreter-in-python" class="level3">
<h3 class="anchored" data-anchor-id="how-to-write-a-lisp-interpreter-in-python">(How to Write a (Lisp) Interpreter (in Python))</h3>
<p>Follow this <a href="http://www.norvig.com/lispy.html">guide</a> to implement your own LISP Interpreter in an hundred lines of python. You might think, “Is he crazy to ask a language beginner to implement the interpreter before learning it?” Answer is while I am partly crazy LISP is not, it has the simplest structure of all programming languages. It’s just lists duh! (LiSt Processing).</p>
</section>
</section>
<section id="lisp-for-ai" class="level2">
<h2 class="anchored" data-anchor-id="lisp-for-ai">LISP for AI</h2>
<p>You can practise LISP for Artificial Intelligence algorithms with Peter Norvig’s book on <a href="https://github.com/norvig/paip-lisp">“Paradigms of Artificial Intelligence Programming”</a>.</p>
</section>
<section id="say-you-dont-want-nirvana-but-something-more-pragmatic" class="level2">
<h2 class="anchored" data-anchor-id="say-you-dont-want-nirvana-but-something-more-pragmatic">Say you don’t want Nirvana, but something more pragmatic</h2>
<p>You can learn Clojure to use God’s Language to do some real world wizardry. Clojure is a form of LISP which is modern, functional and runs on JVM. <a href="https://braveclojure.com">Brave Clojure</a> is one of the best sources out there for Clojure. For front end development/nodejs there is clojure-script which compiles down to javascript.</p>
<blockquote class="blockquote">
<p>Learning Clojure is the best way you can improve as a programmer because it introduces you to powerful concepts implemented in a simple, cohesive, and practical language. You learn Clojure here. Therefore, Brave Clojure is your very best friend when it comes to programming.” And lo, the syllogism was born!</p>
</blockquote>
<p><img src="saiprasanna.in/posts/gods-own-programming-language/https:/imgs.xkcd.com/comics/lisp_cycles.png" class="img-fluid" alt="XKCD comic - LISP is passed down generation after generation as elegant weapons for a more Civilized age."> <em><a href="https://xkcd.com/297/">XKCD Comics</a></em> {: style=“color:gray; font-size: 80%; text-align: center;”}</p>
</section>
<section id="a-note-and-a-zen-koan" class="level2">
<h2 class="anchored" data-anchor-id="a-note-and-a-zen-koan">A Note and a Zen Koan</h2>
<p>Use any language that solves your problem and learn about others which have different paradigms conceptually like LISP, etc when you find the time. It a enjoyable exercise if you are curious about digging deeply into what makes computers tick.</p>
<p>As <a href="http://catb.org/jargon/html/koans.html">Master Foo says</a> says,</p>
<blockquote class="blockquote">
<p>Master Foo once said to a visiting programmer: “There is more Unix-nature in one line of shell script than there is in ten thousand lines of C.”</p>
<p>The programmer, who was very proud of his mastery of C, said: “How can this be? C is the language in which the very kernel of Unix is implemented!”</p>
<p>Master Foo replied: “That is so. Nevertheless, there is more Unix-nature in one line of shell script than there is in ten thousand lines of C.”</p>
<p>The programmer grew distressed. “But through the C language we experience the enlightenment of the Patriarch Ritchie! We become as one with the operating system and the machine, reaping matchless performance!”</p>
<p>Master Foo replied: “All that you say is true. But there is still more Unix-nature in one line of shell script than there is in ten thousand lines of C.”</p>
<p>The programmer scoffed at Master Foo and rose to depart. But Master Foo nodded to his student Nubi, who wrote a line of shell script on a nearby whiteboard, and said: “Master programmer, consider this pipeline. Implemented in pure C, would it not span ten thousand lines?”</p>
<p>The programmer muttered through his beard, contemplating what Nubi had written. Finally he agreed that it was so.</p>
<p>“And how many hours would you require to implement and debug that C program?” asked Nubi.</p>
<p>“Many,” admitted the visiting programmer. “But only a fool would spend the time to do that when so many more worthy tasks await him.”</p>
<p>“And who better understands the Unix-nature?” Master Foo asked. “Is it he who writes the ten thousand lines, or he who, perceiving the emptiness of the task, gains merit by not coding?”</p>
<p>Upon hearing this, the programmer was enlightened.</p>
</blockquote>
<p>For more funny hacker koans, visit <a href="http://thecodelesscode.com/contents">here</a> and <a href="http://catb.org/esr/writings/unix-koans/introduction.html">here</a>.</p>
<p>I am currently learning emacs-lisp for I have become a convert/evangelizer of the <a href="https://stallman.org/saint.html">Church</a> of <a href="https://www.gnu.org/s/emacs/">Emacs</a> on a Starship called <a href="http://spacemacs.org/">Spacemacs</a>. But that’s a post/sermon (;P) for another time.</p>


</section>

 ]]></description>
  <category>LISP</category>
  <category>Programming Languages</category>
  <guid>saiprasanna.in/posts/gods-own-programming-language/index.html</guid>
  <pubDate>Thu, 04 Apr 2019 00:00:00 GMT</pubDate>
  <media:content url="https://imgs.xkcd.com/comics/lisp.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Learning Longer-term Dependencies in RNNs with Auxiliary Losses (ACL 2018)</title>
  <link>saiprasanna.in/posts/learning-long-term-dependencies-rnn/index.html</link>
  <description><![CDATA[ 




<p><a href="https://arxiv.org/pdf/1803.00144.pdf">Paper</a> by Trieu H. Trinh, Andrew M. Dai, Minh-Thang Luong, Quoc V. Le</p>
<p><strong>TLDR;</strong> RNNs can learn very long sequences when auxiliary loss for predicting input sequence (forward and backward) from different timesteps is added.</p>
<section id="problem-being-addressed" class="level3">
<h3 class="anchored" data-anchor-id="problem-being-addressed">Problem being addressed</h3>
<p>Recurrent neural nets in theory can learn arbitrarily long sequences, but in practice suffer from problems like vanishing gradients etc. Techniques to reduce vanishing gradients problem like LSTM alone don’t work for very long sequences.</p>
<p>RNN has a better tradeoff when it comes to memory requirements compared to CNN based networks or vanilla <a href="https://arxiv.org/abs/1706.03762">Transformer</a> nets. When processing very large sequences this becomes important.</p>
</section>
<section id="proposed-method" class="level3">
<h3 class="anchored" data-anchor-id="proposed-method">Proposed Method</h3>
<p>Take the hidden state of main RNN used for a given task at sampled timesteps, use another RNN at sampled intervals, and try to predict the input sequence to certain time steps. Truncated BPTT (Back propagation through time) to few timesteps would give a new loss.</p>
</section>
<section id="evaluation-and-results" class="level3">
<h3 class="anchored" data-anchor-id="evaluation-and-results">Evaluation and results</h3>
<p>Evaluation is done on MNIST, CIFAR-10, Stanford dog dataset is given as sequence of pixels to an RNN with classification being the target. Since the pixels are flattened to sequential input, spatial location information is now across whole range of the sequence, requiring long dependencies to be formed to get good results. The authors also test it on character based classification on dbpedia. This technique achieves very significant results on long sequences compared to existing LSTMs, Transformers etc.</p>
</section>
<section id="opinions" class="level3">
<h3 class="anchored" data-anchor-id="opinions">Opinions</h3>
<p>This paper makes a significant experiment to improve a crucial behavior of RNNs on long sequences. The ablation study is well done. This auxiliary loss reminded me of the <a href="https://worldmodels.github.io/">World models</a> paper where the task of predicting future states improves current tasks output.</p>


</section>

 ]]></description>
  <category>Research Paper Summary</category>
  <category>Deep learning</category>
  <category>RNN</category>
  <guid>saiprasanna.in/posts/learning-long-term-dependencies-rnn/index.html</guid>
  <pubDate>Tue, 05 Jun 2018 08:00:00 GMT</pubDate>
</item>
<item>
  <title>Neural Open Information Extraction (ACL 2018)</title>
  <link>saiprasanna.in/posts/neural-open-information-extraction/index.html</link>
  <description><![CDATA[ 




<p><a href="https://arxiv.org/abs/1805.04270">Paper</a> by Lei Cui, Furu Wei, Ming Zhou</p>
<p><strong>TLDR;</strong> Models Open information extraction as Sequence to Sequence problem using neural nets.</p>
<section id="what-is-open-information-extraction" class="level3">
<h3 class="anchored" data-anchor-id="what-is-open-information-extraction">What is Open information extraction?</h3>
<p>Open Information Extraction aims to extract one or more (Entity 1, Relationship, Entity 2) tuples from sentences.</p>
<p><strong>Example</strong></p>
<pre><code>"Deep learning is a subfield of machine learning." 
                   |
                   v
(Deep learning, is a subfield of , machine learning) </code></pre>
<p>Existing methods use handcrafted rules written on <strong>syntatic parsers</strong> which have poor perfomance and suffer from <strong>cascading of errors</strong>.<br>
This paper applies neural networks to get better accuracy and alleviate errors.</p>
</section>
<section id="how-is-the-problem-modeled" class="level3">
<h3 class="anchored" data-anchor-id="how-is-the-problem-modeled">How is the problem modeled?</h3>
<p>Sequence 2 Sequence task, where you take a source sentence as input and output the information tuple as sequence separated by special tokens (open arg1, arg2, arg3 and close arg1, arg2 arg3). Currently the model is trained for single tuple extraction.</p>
<p>“Deep learning is a subfield of machine learning.” -&gt; “<arg1> Deep learning </arg1> <arg2> is a subfield of </arg2> <arg3> machine learning </arg3>”</p>
<p>The paper uses a LSTM based <strong>Sequence to Sequence</strong> model with <strong>attention</strong>. The source and target vocabulary is same. If unknown word target is found, the model forms the target probability vector by placing attention on of source sequence as probability of the corresponding source words occurring.</p>
</section>
<section id="what-are-the-benchmarks" class="level3">
<h3 class="anchored" data-anchor-id="what-are-the-benchmarks">What are the benchmarks?</h3>
<p>Evaluating on a large benchmark dataset, this model gets <strong>0.473 AUC</strong> (Area under curve) for Precision-Recall on <strong>top 5 predictions</strong> of this model (I think generated from beam search) has better performance , significantly higher than existing systems. Among existing systems <strong>OpenIE</strong> has the best score of <strong>0.373 AUC</strong> .</p>
<p>One observation of authors is only 11 % of the predictions of Neural model matches with Open IE (rule based) model, but the performance is higher. This could be due to neural model generalizing on some of the patterns hard to capture by rules.</p>


</section>

 ]]></description>
  <category>Research Paper Summary</category>
  <category>Deep learning</category>
  <category>NLP</category>
  <guid>saiprasanna.in/posts/neural-open-information-extraction/index.html</guid>
  <pubDate>Mon, 04 Jun 2018 08:00:00 GMT</pubDate>
</item>
<item>
  <title>Dependency Injection - What, Why and How?</title>
  <link>saiprasanna.in/posts/dependency-injection-what-why-and-how/index.html</link>
  <description><![CDATA[ 




<p>We are going to explore dependency injection with emphasis on swift iOS development. But the concept applies to most object oriented languages. We will also see some practical considerations on applying DI in iOS environment. This article is result of my deep dive into implementing DI, and learning about various practical, theoretical aspects of it.</p>
<section id="what-is-dependency-injection" class="level1">
<h1>What is Dependency Injection?</h1>
<blockquote class="blockquote">
<p>“Dependency Injection” is a 25-dollar term for a 5-cent concept”.</p>
</blockquote>
<p>is a often repeated maxim regarding DI.</p>
<p>In its essence DI means wherever possible, <strong>replace object creation inside a piece of code by providing it from outside that piece of code</strong>. Hence the term.</p>
<p>Constructor, Property, Method are where we usually do object creation, and that can be replaced from outside. So we end up with 3 types of injection.</p>
<ol type="1">
<li>Constructor Injection</li>
<li>Property Injection</li>
<li>Method Injection</li>
</ol>
<p>We will explore they 3 types in “How ..?” section.</p>
</section>
<section id="why-dependency-injection" class="level1">
<h1>Why Dependency Injection?</h1>
<p>Let’s dwelve into this with help of a scenario.</p>
<section id="scenario---koala-koder" class="level2">
<h2 class="anchored" data-anchor-id="scenario---koala-koder">Scenario - Koala Koder!</h2>
<p>Say you have an app with a user login. A user model struct/class encapsulates the logged in user data.</p>
<p>Suppose you are a Koala Koder( a programmer who is as lazy as a koala bear). You perhaps made a solution which is quick and dirty. Store the user model inside NSUserDefaults, and fetch it via properties. And as we all know how apple loves its singleton classes, we follow them making UserModel a singleton.</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode swift code-with-copy"><code class="sourceCode swift"><span id="cb1-1"><span class="kw" style="color: #003B4F;">class</span> UserModel <span class="op" style="color: #5E5E5E;">{</span></span>
<span id="cb1-2"></span>
<span id="cb1-3">    <span class="kw" style="color: #003B4F;">static</span> <span class="kw" style="color: #003B4F;">let</span> <span class="va" style="color: #111111;">sharedInstance</span> <span class="op" style="color: #5E5E5E;">=</span> UserModel<span class="op" style="color: #5E5E5E;">()</span></span>
<span id="cb1-4"></span>
<span id="cb1-5">    <span class="kw" style="color: #003B4F;">var</span> <span class="va" style="color: #111111;">name</span><span class="op" style="color: #5E5E5E;">:</span> String <span class="op" style="color: #5E5E5E;">{</span></span>
<span id="cb1-6">        <span class="kw" style="color: #003B4F;">get</span> <span class="op" style="color: #5E5E5E;">{</span></span>
<span id="cb1-7">            <span class="kw" style="color: #003B4F;">return</span> NSUserDefaults<span class="op" style="color: #5E5E5E;">.</span>standard<span class="op" style="color: #5E5E5E;">.</span>string<span class="op" style="color: #5E5E5E;">(</span>forKey<span class="op" style="color: #5E5E5E;">:</span> <span class="st" style="color: #20794D;">"userName"</span><span class="op" style="color: #5E5E5E;">)</span>  <span class="op" style="color: #5E5E5E;">??</span> <span class="st" style="color: #20794D;">""</span></span>
<span id="cb1-8">        <span class="op" style="color: #5E5E5E;">}</span></span>
<span id="cb1-9">        <span class="kw" style="color: #003B4F;">set</span> <span class="op" style="color: #5E5E5E;">{</span></span>
<span id="cb1-10">            NSUserDefaults<span class="op" style="color: #5E5E5E;">.</span>standard<span class="op" style="color: #5E5E5E;">.</span><span class="kw" style="color: #003B4F;">set</span><span class="op" style="color: #5E5E5E;">(</span>newValue<span class="op" style="color: #5E5E5E;">,</span> forKey<span class="op" style="color: #5E5E5E;">:</span><span class="st" style="color: #20794D;">"userName"</span><span class="op" style="color: #5E5E5E;">)</span></span>
<span id="cb1-11">        <span class="op" style="color: #5E5E5E;">}</span></span>
<span id="cb1-12">    <span class="op" style="color: #5E5E5E;">}</span></span>
<span id="cb1-13"></span>
<span id="cb1-14">    <span class="kw" style="color: #003B4F;">func</span> <span class="fu" style="color: #4758AB;">greet</span><span class="op" style="color: #5E5E5E;">()</span> -&gt; <span class="fu" style="color: #4758AB;">String</span> <span class="op" style="color: #5E5E5E;">{</span></span>
<span id="cb1-15">        <span class="kw" style="color: #003B4F;">return</span> <span class="st" style="color: #20794D;">"</span><span class="er" style="color: #AD0000;">\(</span><span class="st" style="color: #20794D;">name), Vannakam :)"</span></span>
<span id="cb1-16">    <span class="op" style="color: #5E5E5E;">}</span></span>
<span id="cb1-17"><span class="op" style="color: #5E5E5E;">}</span></span></code></pre></div>
<p>We write our app with this usermodel in mind, and <code>UserModel.sharedInstance</code> is everywhere in our app.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="saiprasanna.in/posts/dependency-injection-what-why-and-how/https:/ih0.redbubble.net/image.5150955.4141/flat,1000x1000,075,f.jpg" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Koala lazing off</figcaption><p></p>
</figure>
</div>
</section>
<section id="problems-problems-everywhere" class="level2">
<h2 class="anchored" data-anchor-id="problems-problems-everywhere">Problems, problems everywhere…</h2>
<section id="unit-tester-vader-strikes" class="level3">
<h3 class="anchored" data-anchor-id="unit-tester-vader-strikes">1. Unit Tester Vader strikes!</h3>
<p>A senior developer suddenly turns to the dark side, and starts ranting about <em>unit testing</em>. He/She will not let apps which are not unit tested pass the code review.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="saiprasanna.in/posts/dependency-injection-what-why-and-how/http:/s2.quickmeme.com/img/03/0347c3efdc17cc1959d089f60b8b2fc267d9093caa8e8cb483bf476b58e63e45.jpg" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Meme: Darth Vader says “I find your lack of unit testing disturbing”</figcaption><p></p>
</figure>
</div>
</section>
<section id="a-wild-new-use-case-appears" class="level3">
<h3 class="anchored" data-anchor-id="a-wild-new-use-case-appears">2. A wild New Use case appears!</h3>
<p>And if that isn’t enough a <em>new use case</em> should be supported. Our app should now support <em>multiple users</em>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="saiprasanna.in/posts/dependency-injection-what-why-and-how/https:/cdn.meme.am/instances/500x/64312241.jpg" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Meme: Back to future - Dr Brown says “New Usecase in no time? I’ve an extra flux capacitor”</figcaption><p></p>
</figure>
</div>
</section>
<section id="lets-move-to-insert-any-serialization-library-here" class="level3">
<h3 class="anchored" data-anchor-id="lets-move-to-insert-any-serialization-library-here">3. “Lets move to &lt;insert any serialization library here&gt;”</h3>
<p>Now we also reached a point where userdefaults didn’t scale and wish to migrate to new data serialization method. Now even our getters and setters inside the UserProfile is not safe.</p>
</section>
</section>
<section id="why-are-there-problems" class="level2">
<h2 class="anchored" data-anchor-id="why-are-there-problems">Why are there problems?</h2>
<p>So we find ourselves in deep trouble. Lets analyze why so.</p>
<section id="singletons-are-hard-to-test" class="level3">
<h3 class="anchored" data-anchor-id="singletons-are-hard-to-test">1. Singletons are hard to test</h3>
<p>Now if you want to unit test a viewcontroller that uses this singleton.</p>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode swift code-with-copy"><code class="sourceCode swift"><span id="cb2-1"></span>
<span id="cb2-2"><span class="kw" style="color: #003B4F;">class</span> UserProfile<span class="op" style="color: #5E5E5E;">:</span> <span class="dt" style="color: #AD0000;">UIViewController</span> <span class="op" style="color: #5E5E5E;">{</span></span>
<span id="cb2-3"></span>
<span id="cb2-4">    <span class="kw" style="color: #003B4F;">func</span> <span class="fu" style="color: #4758AB;">viewDidLoad</span><span class="op" style="color: #5E5E5E;">()</span> <span class="op" style="color: #5E5E5E;">{</span></span>
<span id="cb2-5">        greetingLabel<span class="op" style="color: #5E5E5E;">.</span>text <span class="op" style="color: #5E5E5E;">=</span> UserModel<span class="op" style="color: #5E5E5E;">.</span>sharedInstance<span class="op" style="color: #5E5E5E;">.</span>greet<span class="op" style="color: #5E5E5E;">()</span></span>
<span id="cb2-6">        <span class="co" style="color: #5E5E5E;">//...</span></span>
<span id="cb2-7">    <span class="op" style="color: #5E5E5E;">}</span></span>
<span id="cb2-8"><span class="op" style="color: #5E5E5E;">}</span></span></code></pre></div>
<p>In unit testing you create a UserProfile object, call viewDidLoad or other methods manually. Now you have to verify whether <code>UserModel.sharedInstance.greet()</code> was called.</p>
<p>Since UserModel.sharedInstance is immutable, either we can’t replace it with our mock class extending UserModel, which overrides greetUser, and sets a flag which can be checked. So our testing coverage comes down.</p>
</section>
<section id="instantiation-inside-our-code-constraints-creates-strong-coupling-creating-harmful-constraints" class="level3">
<h3 class="anchored" data-anchor-id="instantiation-inside-our-code-constraints-creates-strong-coupling-creating-harmful-constraints">2. Instantiation inside our code constraints creates strong coupling, creating harmful constraints</h3>
<p>In our scenario, by using a singleton, we tied our codebase to a single UserModel but now our app needs multiple user models. So in general, using singletons will make it hard to adapt to new use cases. <em>What you thought as a singleton suddenly is not so single anymore</em>.</p>
<p>But consider that we didn’t use singleton, but instantiated UserModel, by calling <code>UserModel()</code> wherever we needed it.</p>
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode swift code-with-copy"><code class="sourceCode swift"><span id="cb3-1"></span>
<span id="cb3-2"><span class="kw" style="color: #003B4F;">class</span> UserProfile<span class="op" style="color: #5E5E5E;">:</span> <span class="dt" style="color: #AD0000;">UIViewController</span> <span class="op" style="color: #5E5E5E;">{</span></span>
<span id="cb3-3"></span>
<span id="cb3-4">    <span class="kw" style="color: #003B4F;">let</span> <span class="va" style="color: #111111;">userModel</span> <span class="op" style="color: #5E5E5E;">=</span> UserModel<span class="op" style="color: #5E5E5E;">()</span></span>
<span id="cb3-5"></span>
<span id="cb3-6">    <span class="kw" style="color: #003B4F;">func</span> <span class="fu" style="color: #4758AB;">viewDidLoad</span><span class="op" style="color: #5E5E5E;">()</span> <span class="op" style="color: #5E5E5E;">{</span></span>
<span id="cb3-7">        userNameLabel<span class="op" style="color: #5E5E5E;">.</span>text <span class="op" style="color: #5E5E5E;">=</span> userModel</span>
<span id="cb3-8">        <span class="co" style="color: #5E5E5E;">//...</span></span>
<span id="cb3-9">    <span class="op" style="color: #5E5E5E;">}</span></span>
<span id="cb3-10"><span class="op" style="color: #5E5E5E;">}</span></span></code></pre></div>
<p>We can’t support our new usecase of multiple users, without userprofile class knowing about multi-usermodel, or some other global allowing UserModel() to return the correct usermodel, these solutions are ugly hacks, which add complexity by either giving too much knowledge to classes or using globals and forgoing object oriented encapsulation.</p>
</section>
<section id="concrete-type-usage-creates-strong-coupling" class="level3">
<h3 class="anchored" data-anchor-id="concrete-type-usage-creates-strong-coupling">3. Concrete Type usage creates strong coupling</h3>
<p>Consider the UserProfile, it uses NSUserDefaults, now suppose we move to coredata to save our data, we are again in trouble because of using singleton inside. <strong>Our UserProfile rather needed only just a way to serialize some data, it didn’t need to know about WHAT we use for serialization</strong>. This is the key insight to keep in mind when thinking about dependency injection.</p>
</section>
</section>
<section id="di-to-the-rescue" class="level2">
<h2 class="anchored" data-anchor-id="di-to-the-rescue">DI to the rescue</h2>
<p>DI helps to solve the variety of issues that we face above, with regard to Unit Testablity, Singletons and strong coupling we face above.</p>
</section>
</section>
<section id="how-to-do-dependency-injection" class="level1">
<h1>How to do Dependency Injection?</h1>
<section id="constructor-injection" class="level2">
<h2 class="anchored" data-anchor-id="constructor-injection">Constructor Injection</h2>
<p>So we will be injecting a serializer into UserModel via constructor.</p>
<p><strong>Move dependency to constructor, and if possible make it a interface/protocol type instead of concrete class/struct</strong></p>
<p>So we remove singleton access to userdefaults. And rather pass a serializer protocol which has methods we require for serialization to constructor.</p>
<div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode swift code-with-copy"><code class="sourceCode swift"><span id="cb4-1"></span>
<span id="cb4-2"><span class="kw" style="color: #003B4F;">protocol</span> Serializing <span class="op" style="color: #5E5E5E;">{</span></span>
<span id="cb4-3">    <span class="kw" style="color: #003B4F;">func</span> <span class="fu" style="color: #4758AB;">string</span><span class="op" style="color: #5E5E5E;">(</span><span class="va" style="color: #111111;">forKey</span> <span class="va" style="color: #111111;">defaultName</span><span class="op" style="color: #5E5E5E;">:</span> <span class="dt" style="color: #AD0000;">String</span><span class="op" style="color: #5E5E5E;">)</span> -&gt; <span class="fu" style="color: #4758AB;">String</span>?</span>
<span id="cb4-4">    <span class="kw" style="color: #003B4F;">func</span> <span class="kw" style="color: #003B4F;">set</span><span class="op" style="color: #5E5E5E;">(</span><span class="va" style="color: #111111;">_</span> <span class="va" style="color: #111111;">value</span><span class="op" style="color: #5E5E5E;">:</span> <span class="dt" style="color: #AD0000;">Any</span><span class="op" style="color: #5E5E5E;">?,</span> <span class="va" style="color: #111111;">forKey</span> <span class="va" style="color: #111111;">defaultName</span><span class="op" style="color: #5E5E5E;">:</span> <span class="dt" style="color: #AD0000;">String</span><span class="op" style="color: #5E5E5E;">)</span></span>
<span id="cb4-5">}</span>
<span id="cb4-6"></span>
<span id="cb4-7"><span class="kw" style="color: #003B4F;">class</span> <span class="fu" style="color: #4758AB;">UserModel</span> <span class="op" style="color: #5E5E5E;">{</span></span>
<span id="cb4-8">    <span class="kw" style="color: #003B4F;">static</span> <span class="kw" style="color: #003B4F;">let</span> <span class="va" style="color: #111111;">serializer</span><span class="op" style="color: #5E5E5E;">:</span> Serializing</span>
<span id="cb4-9"></span>
<span id="cb4-10">    <span class="kw" style="color: #003B4F;">init</span><span class="op" style="color: #5E5E5E;">(</span>_ serializer<span class="op" style="color: #5E5E5E;">:</span> Serializing<span class="op" style="color: #5E5E5E;">)</span> <span class="op" style="color: #5E5E5E;">{</span></span>
<span id="cb4-11">        <span class="kw" style="color: #003B4F;">self</span><span class="op" style="color: #5E5E5E;">.</span>serializer <span class="op" style="color: #5E5E5E;">=</span> serializer</span>
<span id="cb4-12">    <span class="op" style="color: #5E5E5E;">}</span></span>
<span id="cb4-13"></span>
<span id="cb4-14">    <span class="kw" style="color: #003B4F;">var</span> <span class="va" style="color: #111111;">name</span><span class="op" style="color: #5E5E5E;">:</span> String <span class="op" style="color: #5E5E5E;">{</span></span>
<span id="cb4-15">        <span class="kw" style="color: #003B4F;">get</span> <span class="op" style="color: #5E5E5E;">{</span></span>
<span id="cb4-16">            <span class="kw" style="color: #003B4F;">return</span> serializer<span class="op" style="color: #5E5E5E;">.</span>string<span class="op" style="color: #5E5E5E;">(</span>forKey<span class="op" style="color: #5E5E5E;">:</span> <span class="st" style="color: #20794D;">"userName"</span><span class="op" style="color: #5E5E5E;">)</span>  <span class="op" style="color: #5E5E5E;">??</span> <span class="st" style="color: #20794D;">""</span></span>
<span id="cb4-17">         <span class="op" style="color: #5E5E5E;">}</span></span>
<span id="cb4-18">        <span class="kw" style="color: #003B4F;">set</span> <span class="op" style="color: #5E5E5E;">{</span></span>
<span id="cb4-19">            serializer<span class="op" style="color: #5E5E5E;">.</span>setObject<span class="op" style="color: #5E5E5E;">(</span>newValue<span class="op" style="color: #5E5E5E;">,</span> forKey<span class="op" style="color: #5E5E5E;">:</span><span class="st" style="color: #20794D;">"userName"</span><span class="op" style="color: #5E5E5E;">)</span></span>
<span id="cb4-20">        <span class="op" style="color: #5E5E5E;">}</span></span>
<span id="cb4-21">    <span class="op" style="color: #5E5E5E;">}</span></span>
<span id="cb4-22"></span>
<span id="cb4-23"><span class="op" style="color: #5E5E5E;">}</span></span></code></pre></div>
<p>Since we are Koala Koder, we just make the Serializing protocol methods to same ones in NSUserdefaults, so we can make NSUserDefaults conform easily by</p>
<div class="sourceCode" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode swift code-with-copy"><code class="sourceCode swift"><span id="cb5-1"><span class="kw" style="color: #003B4F;">extension</span> NSUserDefaults<span class="op" style="color: #5E5E5E;">:</span> <span class="dt" style="color: #AD0000;">Serializing</span> <span class="op" style="color: #5E5E5E;">{}</span></span></code></pre></div>
<p>.</p>
<p>Now to use user defaults as our serializer, we do the following.</p>
<div class="sourceCode" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode swift code-with-copy"><code class="sourceCode swift"><span id="cb6-1">UserModel<span class="op" style="color: #5E5E5E;">(</span>serializer<span class="op" style="color: #5E5E5E;">:</span> NSUserDefaults<span class="op" style="color: #5E5E5E;">.</span>standard<span class="op" style="color: #5E5E5E;">)</span></span></code></pre></div>
<p>For our fancy multiple user use case we can also do this. (Note: did this in a hurry, it may have edge cases, just providing it as a illustration)</p>
<div class="sourceCode" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode swift code-with-copy"><code class="sourceCode swift"><span id="cb7-1"></span>
<span id="cb7-2"><span class="kw" style="color: #003B4F;">struct</span> MultiUserSerializer<span class="op" style="color: #5E5E5E;">:</span> <span class="dt" style="color: #AD0000;">Serializing</span> <span class="op" style="color: #5E5E5E;">{</span></span>
<span id="cb7-3"></span>
<span id="cb7-4">    <span class="kw" style="color: #003B4F;">let</span> <span class="va" style="color: #111111;">serializer</span><span class="op" style="color: #5E5E5E;">:</span> Serializing</span>
<span id="cb7-5">    <span class="kw" style="color: #003B4F;">init</span><span class="op" style="color: #5E5E5E;">(</span>_ serializer<span class="op" style="color: #5E5E5E;">:</span> Serializing<span class="op" style="color: #5E5E5E;">)</span> <span class="op" style="color: #5E5E5E;">{</span></span>
<span id="cb7-6">        <span class="kw" style="color: #003B4F;">self</span><span class="op" style="color: #5E5E5E;">.</span>serializer <span class="op" style="color: #5E5E5E;">=</span> serializer</span>
<span id="cb7-7">    <span class="op" style="color: #5E5E5E;">}</span></span>
<span id="cb7-8"></span>
<span id="cb7-9">    <span class="co" style="color: #5E5E5E;">// So we fetch the current currentUserId and use that to prefix stored data</span></span>
<span id="cb7-10">    <span class="kw" style="color: #003B4F;">private</span> <span class="kw" style="color: #003B4F;">func</span> <span class="fu" style="color: #4758AB;">fetchCurrentUserId</span><span class="op" style="color: #5E5E5E;">()</span> -&gt; <span class="fu" style="color: #4758AB;">String</span> <span class="op" style="color: #5E5E5E;">{</span></span>
<span id="cb7-11">        <span class="kw" style="color: #003B4F;">return</span> serializer<span class="op" style="color: #5E5E5E;">.</span>string<span class="op" style="color: #5E5E5E;">(</span>forKey<span class="op" style="color: #5E5E5E;">:</span> <span class="st" style="color: #20794D;">"CURRENT_USER_ID"</span><span class="op" style="color: #5E5E5E;">)</span> <span class="op" style="color: #5E5E5E;">??</span> <span class="st" style="color: #20794D;">"0"</span></span>
<span id="cb7-12">    <span class="op" style="color: #5E5E5E;">}</span></span>
<span id="cb7-13"></span>
<span id="cb7-14">    <span class="kw" style="color: #003B4F;">func</span> <span class="fu" style="color: #4758AB;">string</span><span class="op" style="color: #5E5E5E;">(</span><span class="va" style="color: #111111;">forKey</span> <span class="va" style="color: #111111;">defaultName</span><span class="op" style="color: #5E5E5E;">:</span> <span class="dt" style="color: #AD0000;">String</span><span class="op" style="color: #5E5E5E;">)</span> -&gt; <span class="fu" style="color: #4758AB;">String</span>? <span class="op" style="color: #5E5E5E;">{</span></span>
<span id="cb7-15">        <span class="kw" style="color: #003B4F;">return</span> serializer<span class="op" style="color: #5E5E5E;">.</span>string<span class="op" style="color: #5E5E5E;">(</span>forKey<span class="op" style="color: #5E5E5E;">:</span> fetchCurrentUserId<span class="op" style="color: #5E5E5E;">()</span> <span class="op" style="color: #5E5E5E;">+</span> <span class="st" style="color: #20794D;">":"</span> <span class="op" style="color: #5E5E5E;">+</span> defaultName<span class="op" style="color: #5E5E5E;">)</span></span>
<span id="cb7-16">    <span class="op" style="color: #5E5E5E;">}</span></span>
<span id="cb7-17"></span>
<span id="cb7-18">    <span class="kw" style="color: #003B4F;">func</span> <span class="kw" style="color: #003B4F;">set</span><span class="op" style="color: #5E5E5E;">(</span><span class="va" style="color: #111111;">_</span> <span class="va" style="color: #111111;">value</span><span class="op" style="color: #5E5E5E;">:</span> <span class="dt" style="color: #AD0000;">Any</span><span class="op" style="color: #5E5E5E;">?,</span> <span class="va" style="color: #111111;">forKey</span> <span class="va" style="color: #111111;">defaultName</span><span class="op" style="color: #5E5E5E;">:</span> <span class="dt" style="color: #AD0000;">String</span><span class="op" style="color: #5E5E5E;">)</span> <span class="op" style="color: #5E5E5E;">{</span></span>
<span id="cb7-19">        <span class="kw" style="color: #003B4F;">return</span> serializer<span class="op" style="color: #5E5E5E;">.</span><span class="kw" style="color: #003B4F;">set</span><span class="op" style="color: #5E5E5E;">(</span>value<span class="op" style="color: #5E5E5E;">,</span> forKey defaultName<span class="op" style="color: #5E5E5E;">:</span> fetchCurrentUserId<span class="op" style="color: #5E5E5E;">()</span> <span class="op" style="color: #5E5E5E;">+</span> <span class="st" style="color: #20794D;">":"</span> <span class="op" style="color: #5E5E5E;">+</span> defaultName<span class="op" style="color: #5E5E5E;">)</span></span>
<span id="cb7-20">    <span class="op" style="color: #5E5E5E;">}</span></span>
<span id="cb7-21"></span>
<span id="cb7-22"><span class="op" style="color: #5E5E5E;">}</span></span>
<span id="cb7-23"></span>
<span id="cb7-24"><span class="co" style="color: #5E5E5E;">//Instantiate using userdefaults, assume we implemented contextFetcher somewhere else</span></span>
<span id="cb7-25"></span>
<span id="cb7-26"><span class="kw" style="color: #003B4F;">let</span> <span class="va" style="color: #111111;">multiUserSerializer</span> <span class="op" style="color: #5E5E5E;">=</span> MultiUserSerializer<span class="op" style="color: #5E5E5E;">(</span>serializer<span class="op" style="color: #5E5E5E;">:</span> NSUserDefaults<span class="op" style="color: #5E5E5E;">.</span>standard<span class="op" style="color: #5E5E5E;">)</span></span>
<span id="cb7-27"></span>
<span id="cb7-28"><span class="kw" style="color: #003B4F;">let</span> <span class="va" style="color: #111111;">userModel</span> <span class="op" style="color: #5E5E5E;">=</span> UserModel<span class="op" style="color: #5E5E5E;">(</span>serializer<span class="op" style="color: #5E5E5E;">:</span> multiUserSerializer<span class="op" style="color: #5E5E5E;">)</span></span></code></pre></div>
<p>So now userModel fetches data, and sets data to the current userID without even knowing about it. We could also use something other than <code>NSUserDefaults.standard</code> to serialize the data in top level.</p>
<p>The point is by removing replacing the concrete dependency <code>NSUserDefaults.standard</code> out of <code>UserModel</code> and swapping it to <code>Serializing</code> protocol we can now satisfy the new usecase of multi user modelling easily. This is the core idea behind of <strong>loose coupling</strong>.</p>
<p>Also we can now unit test User Model by just passing a Mock implementation of <code>Serializing</code></p>
<section id="dependency-injection-works-even-with-only-concrete-types." class="level3">
<h3 class="anchored" data-anchor-id="dependency-injection-works-even-with-only-concrete-types.">Dependency injection works even with only concrete types.</h3>
<p>Sometimes you don’t have the time, or are sure that concrete type used will not have to be changed. You can still DI the concrete type without bothering with protocol creation, just for the sake of it.</p>
<section id="for-example" class="level4">
<h4 class="anchored" data-anchor-id="for-example">For example:</h4>
<div class="sourceCode" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode swift code-with-copy"><code class="sourceCode swift"><span id="cb8-1"></span>
<span id="cb8-2"><span class="kw" style="color: #003B4F;">class</span> UserProfile<span class="op" style="color: #5E5E5E;">:</span> <span class="dt" style="color: #AD0000;">UIViewController</span> <span class="op" style="color: #5E5E5E;">{</span></span>
<span id="cb8-3"></span>
<span id="cb8-4">    <span class="kw" style="color: #003B4F;">let</span> <span class="va" style="color: #111111;">userModel</span><span class="op" style="color: #5E5E5E;">:</span> UserModel</span>
<span id="cb8-5"></span>
<span id="cb8-6">    <span class="co" style="color: #5E5E5E;">// This works only if you don't use storyboards</span></span>
<span id="cb8-7">    <span class="kw" style="color: #003B4F;">init</span><span class="op" style="color: #5E5E5E;">(</span>userModel<span class="op" style="color: #5E5E5E;">:</span> UserModel<span class="op" style="color: #5E5E5E;">)</span> <span class="op" style="color: #5E5E5E;">{</span></span>
<span id="cb8-8">        <span class="kw" style="color: #003B4F;">self</span><span class="op" style="color: #5E5E5E;">.</span>userModel <span class="op" style="color: #5E5E5E;">=</span> userModel</span>
<span id="cb8-9">    <span class="op" style="color: #5E5E5E;">}</span></span>
<span id="cb8-10"></span>
<span id="cb8-11">    <span class="kw" style="color: #003B4F;">func</span> <span class="fu" style="color: #4758AB;">viewDidLoad</span><span class="op" style="color: #5E5E5E;">()</span> <span class="op" style="color: #5E5E5E;">{</span></span>
<span id="cb8-12">        userNameLabel<span class="op" style="color: #5E5E5E;">.</span>text <span class="op" style="color: #5E5E5E;">=</span> userModel<span class="op" style="color: #5E5E5E;">.</span>name</span>
<span id="cb8-13">    <span class="op" style="color: #5E5E5E;">}</span></span>
<span id="cb8-14"><span class="op" style="color: #5E5E5E;">}</span></span></code></pre></div>
<p>We move UserModel creation out of the controller, without creating any protocol for UserModel properties.</p>
<p>We still gain advantages of unit testability using ordinary mock objects, and also we are free to use our multi user serialized UserModel , hence making UserProfile support multi user model without changing any logic in user profile.</p>
<p>(But if you have to do something to notify changes in data, that is seperate topic)</p>
<div class="sourceCode" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode swift code-with-copy"><code class="sourceCode swift"><span id="cb9-1"><span class="kw" style="color: #003B4F;">let</span> <span class="va" style="color: #111111;">multiUserSerializer</span> <span class="op" style="color: #5E5E5E;">=</span> MultiUserSerializer<span class="op" style="color: #5E5E5E;">(</span>serializer<span class="op" style="color: #5E5E5E;">:</span> NSUserDefaults<span class="op" style="color: #5E5E5E;">.</span>standard<span class="op" style="color: #5E5E5E;">)</span></span>
<span id="cb9-2"><span class="kw" style="color: #003B4F;">let</span> <span class="va" style="color: #111111;">multiUserModel</span> <span class="op" style="color: #5E5E5E;">=</span> UserModel<span class="op" style="color: #5E5E5E;">(</span>serializer<span class="op" style="color: #5E5E5E;">:</span> multiUserSerializer<span class="op" style="color: #5E5E5E;">)</span></span>
<span id="cb9-3"><span class="kw" style="color: #003B4F;">let</span> <span class="va" style="color: #111111;">userProfile</span> <span class="op" style="color: #5E5E5E;">=</span> UserProfile<span class="op" style="color: #5E5E5E;">(</span>userModel<span class="op" style="color: #5E5E5E;">:</span> multiUserModel<span class="op" style="color: #5E5E5E;">)</span></span></code></pre></div>
</section>
</section>
<section id="my-constructor-is-a-monster-now" class="level3">
<h3 class="anchored" data-anchor-id="my-constructor-is-a-monster-now">My constructor is a monster now&nbsp;:(</h3>
<p>A common problem which you will come across is, your UIViewController (if you don’t use storyboards) or any class where you do DI becomes a to big.</p>
<div class="sourceCode" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode swift code-with-copy"><code class="sourceCode swift"><span id="cb10-1"><span class="kw" style="color: #003B4F;">class</span> UserProfile<span class="op" style="color: #5E5E5E;">:</span> <span class="dt" style="color: #AD0000;">UIViewController</span> <span class="op" style="color: #5E5E5E;">{</span></span>
<span id="cb10-2"></span>
<span id="cb10-3"><span class="kw" style="color: #003B4F;">init</span><span class="op" style="color: #5E5E5E;">(</span>userModel<span class="op" style="color: #5E5E5E;">:</span> UserModel<span class="op" style="color: #5E5E5E;">,</span> apiService<span class="op" style="color: #5E5E5E;">:</span> APIService<span class="op" style="color: #5E5E5E;">,</span> x<span class="op" style="color: #5E5E5E;">:</span> XService<span class="op" style="color: #5E5E5E;">,</span> y<span class="op" style="color: #5E5E5E;">:</span>YService<span class="op" style="color: #5E5E5E;">,</span> z<span class="op" style="color: #5E5E5E;">:</span> ZService<span class="op" style="color: #5E5E5E;">,</span> <span class="op" style="color: #5E5E5E;">....)</span> </span></code></pre></div>
<p>This is a good thing, it points out clearly that your class is violating <strong>Single Responsibility rule</strong> . Single responsibility rule states that a class should have singe responsibility.</p>
<p>We can solve this by moving some of the current dependencies to a new class, and pass the new class as dependency to UserProfile.</p>
<p>Guess what if we do this we properly we would be re-inventing design patterns l MVVM(Nodel View ViewModel) or MVP. Whew! DI just solved Huge ViewController problem!!.</p>
<blockquote class="blockquote">
<p>DI can easily help refactoring code to better design, in a gradual manner.</p>
</blockquote>
</section>
</section>
<section id="property-injection" class="level2">
<h2 class="anchored" data-anchor-id="property-injection">Property Injection</h2>
<p>We seemed to have solved all our above problems, why bother with more types of injections?</p>
<blockquote class="blockquote">
<p>If you don’t control the creation of a object, your best bet is property injection. But prefer constructor injection if that’s not the case.</p>
</blockquote>
<p>Sometimes you don’t create objects of classes, some messy framework does it. For example, if you use Storyboards, you can’t do stuff like <code>let userProfileViewController = UserProfile(multiUserModel)</code>. You would have to refactor to something like</p>
<div class="sourceCode" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode swift code-with-copy"><code class="sourceCode swift"><span id="cb11-1"></span>
<span id="cb11-2"><span class="kw" style="color: #003B4F;">class</span> UserProfile<span class="op" style="color: #5E5E5E;">:</span> <span class="dt" style="color: #AD0000;">UIViewController</span> <span class="op" style="color: #5E5E5E;">{</span></span>
<span id="cb11-3"></span>
<span id="cb11-4">    <span class="kw" style="color: #003B4F;">var</span> <span class="va" style="color: #111111;">userModel</span><span class="op" style="color: #5E5E5E;">:</span> UserModel<span class="op" style="color: #5E5E5E;">!</span></span>
<span id="cb11-5"></span>
<span id="cb11-6">    <span class="kw" style="color: #003B4F;">func</span> <span class="fu" style="color: #4758AB;">viewDidLoad</span><span class="op" style="color: #5E5E5E;">()</span> <span class="op" style="color: #5E5E5E;">{</span></span>
<span id="cb11-7">        userNameLabel<span class="op" style="color: #5E5E5E;">.</span>text <span class="op" style="color: #5E5E5E;">=</span> userModel<span class="op" style="color: #5E5E5E;">.</span>name</span>
<span id="cb11-8">    <span class="op" style="color: #5E5E5E;">}</span></span>
<span id="cb11-9"><span class="op" style="color: #5E5E5E;">}</span></span>
<span id="cb11-10"></span>
<span id="cb11-11"><span class="kw" style="color: #003B4F;">let</span> <span class="va" style="color: #111111;">multiUserSerializer</span> <span class="op" style="color: #5E5E5E;">=</span> MultiUserSerializer<span class="op" style="color: #5E5E5E;">(</span>serializer<span class="op" style="color: #5E5E5E;">:</span> NSUserDefaults<span class="op" style="color: #5E5E5E;">.</span>standard<span class="op" style="color: #5E5E5E;">)</span></span>
<span id="cb11-12"><span class="kw" style="color: #003B4F;">let</span> <span class="va" style="color: #111111;">multiUserModel</span> <span class="op" style="color: #5E5E5E;">=</span> UserModel<span class="op" style="color: #5E5E5E;">(</span>serializer<span class="op" style="color: #5E5E5E;">:</span> multiUserSerializer<span class="op" style="color: #5E5E5E;">)</span></span>
<span id="cb11-13"></span>
<span id="cb11-14"><span class="kw" style="color: #003B4F;">let</span> <span class="va" style="color: #111111;">storyboard</span> <span class="op" style="color: #5E5E5E;">=</span> UIStoryboard<span class="op" style="color: #5E5E5E;">(</span>name<span class="op" style="color: #5E5E5E;">:</span> <span class="st" style="color: #20794D;">"main"</span><span class="op" style="color: #5E5E5E;">,</span> bundle<span class="op" style="color: #5E5E5E;">:</span> <span class="kw" style="color: #003B4F;">nil</span><span class="op" style="color: #5E5E5E;">)</span></span>
<span id="cb11-15"><span class="kw" style="color: #003B4F;">let</span> <span class="va" style="color: #111111;">userProfile</span> <span class="op" style="color: #5E5E5E;">=</span> storyboard<span class="op" style="color: #5E5E5E;">.</span>instantiateViewController<span class="op" style="color: #5E5E5E;">(</span>withIdentifier<span class="op" style="color: #5E5E5E;">:</span> <span class="st" style="color: #20794D;">"UserProfile"</span><span class="op" style="color: #5E5E5E;">)</span> <span class="kw" style="color: #003B4F;">as</span><span class="op" style="color: #5E5E5E;">!</span> UserProfile</span>
<span id="cb11-16">userProfile<span class="op" style="color: #5E5E5E;">.</span>userModel <span class="op" style="color: #5E5E5E;">=</span> multiUserModel</span></code></pre></div>
<p>By using implicitly unwrapped Optional property, and setting it from outside, we achive the same effects of constructor injection. But is not perfect, as <code>userModel</code> property can be mutated from outside. Butthis is as good as it can get.</p>
</section>
<section id="method-injection" class="level2">
<h2 class="anchored" data-anchor-id="method-injection">Method Injection</h2>
<p>Method injection is just replacing instantiating inside method by one of its parameters.</p>
<div class="sourceCode" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode swift code-with-copy"><code class="sourceCode swift"><span id="cb12-1"><span class="co" style="color: #5E5E5E;">// Before injection</span></span>
<span id="cb12-2"></span>
<span id="cb12-3"><span class="kw" style="color: #003B4F;">func</span> <span class="fu" style="color: #4758AB;">someMethod</span><span class="op" style="color: #5E5E5E;">()</span> <span class="op" style="color: #5E5E5E;">{</span></span>
<span id="cb12-4">    <span class="kw" style="color: #003B4F;">let</span> <span class="va" style="color: #111111;">x</span> <span class="op" style="color: #5E5E5E;">=</span> X<span class="op" style="color: #5E5E5E;">()</span></span>
<span id="cb12-5">    <span class="kw" style="color: #003B4F;">let</span> <span class="va" style="color: #111111;">y</span> <span class="op" style="color: #5E5E5E;">=</span> x<span class="op" style="color: #5E5E5E;">.</span>something<span class="op" style="color: #5E5E5E;">()</span> <span class="op" style="color: #5E5E5E;">+</span> <span class="dv" style="color: #AD0000;">10</span></span>
<span id="cb12-6">    <span class="kw" style="color: #003B4F;">return</span> y</span>
<span id="cb12-7"><span class="op" style="color: #5E5E5E;">}</span></span>
<span id="cb12-8"></span>
<span id="cb12-9"><span class="co" style="color: #5E5E5E;">// After injection</span></span>
<span id="cb12-10"></span>
<span id="cb12-11"><span class="kw" style="color: #003B4F;">func</span> <span class="fu" style="color: #4758AB;">someMethod</span><span class="op" style="color: #5E5E5E;">(</span><span class="va" style="color: #111111;">_</span> <span class="va" style="color: #111111;">x</span><span class="op" style="color: #5E5E5E;">:</span> <span class="dt" style="color: #AD0000;">XService</span><span class="op" style="color: #5E5E5E;">)</span> <span class="op" style="color: #5E5E5E;">{</span></span>
<span id="cb12-12">    <span class="kw" style="color: #003B4F;">let</span> <span class="va" style="color: #111111;">y</span> <span class="op" style="color: #5E5E5E;">=</span> x<span class="op" style="color: #5E5E5E;">.</span>something<span class="op" style="color: #5E5E5E;">()</span> <span class="op" style="color: #5E5E5E;">+</span> <span class="dv" style="color: #AD0000;">10</span></span>
<span id="cb12-13">    <span class="kw" style="color: #003B4F;">return</span> y</span>
<span id="cb12-14"><span class="op" style="color: #5E5E5E;">}</span></span></code></pre></div>
</section>
<section id="runtime-injection---factory-pattern" class="level2">
<h2 class="anchored" data-anchor-id="runtime-injection---factory-pattern">Runtime Injection - Factory pattern</h2>
<p>Sometimes you want to create a object of a particular class in runtime. But you want to use only protocol type(or a super type) instead of actual implementation (or subclass). Let’s say you need a networking service, which you set based on a user action, but as you are going to need it in runtime, you may think that you can’t inject it.</p>
<p>But what you can do is inject a factory Networking object or closure, that constructs it in runtime. This factory can fill the dependency of the concrete Networking class, so your class can be unaware of this.</p>
<div class="sourceCode" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode swift code-with-copy"><code class="sourceCode swift"><span id="cb13-1"></span>
<span id="cb13-2"><span class="kw" style="color: #003B4F;">protocol</span> Networking <span class="op" style="color: #5E5E5E;">{</span></span>
<span id="cb13-3"> <span class="co" style="color: #5E5E5E;">// Some methods ..</span></span>
<span id="cb13-4"><span class="op" style="color: #5E5E5E;">}</span></span>
<span id="cb13-5"></span>
<span id="cb13-6"><span class="kw" style="color: #003B4F;">class</span> ProxyNetworking<span class="op" style="color: #5E5E5E;">:</span> <span class="dt" style="color: #AD0000;">Networking</span>  <span class="op" style="color: #5E5E5E;">{</span></span>
<span id="cb13-7">    <span class="kw" style="color: #003B4F;">init</span><span class="op" style="color: #5E5E5E;">(</span>a<span class="op" style="color: #5E5E5E;">:</span> A<span class="op" style="color: #5E5E5E;">)</span> <span class="op" style="color: #5E5E5E;">{</span></span>
<span id="cb13-8"></span>
<span id="cb13-9">    <span class="op" style="color: #5E5E5E;">}</span></span>
<span id="cb13-10"><span class="op" style="color: #5E5E5E;">}</span></span>
<span id="cb13-11"></span>
<span id="cb13-12"><span class="kw" style="color: #003B4F;">class</span> NormalNetworking<span class="op" style="color: #5E5E5E;">:</span> <span class="dt" style="color: #AD0000;">Networking</span> <span class="op" style="color: #5E5E5E;">{</span></span>
<span id="cb13-13">    <span class="kw" style="color: #003B4F;">init</span><span class="op" style="color: #5E5E5E;">(</span>a<span class="op" style="color: #5E5E5E;">:</span> A<span class="op" style="color: #5E5E5E;">)</span> <span class="op" style="color: #5E5E5E;">{</span></span>
<span id="cb13-14"></span>
<span id="cb13-15">    <span class="op" style="color: #5E5E5E;">}</span></span>
<span id="cb13-16"><span class="op" style="color: #5E5E5E;">}</span></span>
<span id="cb13-17"></span>
<span id="cb13-18"></span>
<span id="cb13-19"><span class="co" style="color: #5E5E5E;">// Injection Via Factory Class</span></span>
<span id="cb13-20"></span>
<span id="cb13-21"><span class="kw" style="color: #003B4F;">class</span> NetworkingFactory <span class="op" style="color: #5E5E5E;">{</span></span>
<span id="cb13-22"></span>
<span id="cb13-23">    <span class="kw" style="color: #003B4F;">let</span> <span class="va" style="color: #111111;">a</span><span class="op" style="color: #5E5E5E;">:</span> A</span>
<span id="cb13-24">    <span class="kw" style="color: #003B4F;">init</span><span class="op" style="color: #5E5E5E;">(</span>a<span class="op" style="color: #5E5E5E;">:</span> A<span class="op" style="color: #5E5E5E;">)</span> <span class="op" style="color: #5E5E5E;">{</span></span>
<span id="cb13-25">        <span class="kw" style="color: #003B4F;">self</span><span class="op" style="color: #5E5E5E;">.</span>a <span class="op" style="color: #5E5E5E;">=</span> A</span>
<span id="cb13-26">    <span class="op" style="color: #5E5E5E;">}</span></span>
<span id="cb13-27"></span>
<span id="cb13-28">    <span class="kw" style="color: #003B4F;">func</span> <span class="fu" style="color: #4758AB;">create</span><span class="op" style="color: #5E5E5E;">(</span><span class="va" style="color: #111111;">_</span> <span class="va" style="color: #111111;">withProxy</span><span class="op" style="color: #5E5E5E;">:</span> <span class="dt" style="color: #AD0000;">Bool</span><span class="op" style="color: #5E5E5E;">)</span> -&gt; <span class="fu" style="color: #4758AB;">Networking</span> <span class="op" style="color: #5E5E5E;">{</span></span>
<span id="cb13-29">         <span class="cf" style="color: #003B4F;">if</span> <span class="op" style="color: #5E5E5E;">(</span>withProxy<span class="op" style="color: #5E5E5E;">)</span> <span class="op" style="color: #5E5E5E;">{</span></span>
<span id="cb13-30">            <span class="kw" style="color: #003B4F;">return</span> ProxyNetworking<span class="op" style="color: #5E5E5E;">(</span>a<span class="op" style="color: #5E5E5E;">:</span> a<span class="op" style="color: #5E5E5E;">)</span></span>
<span id="cb13-31">         <span class="op" style="color: #5E5E5E;">}</span> <span class="cf" style="color: #003B4F;">else</span> <span class="op" style="color: #5E5E5E;">{</span></span>
<span id="cb13-32">            <span class="kw" style="color: #003B4F;">return</span> NormalNetworking<span class="op" style="color: #5E5E5E;">(</span>a<span class="op" style="color: #5E5E5E;">:</span> a<span class="op" style="color: #5E5E5E;">)</span></span>
<span id="cb13-33">         <span class="op" style="color: #5E5E5E;">}</span></span>
<span id="cb13-34">    <span class="op" style="color: #5E5E5E;">}</span></span>
<span id="cb13-35"><span class="op" style="color: #5E5E5E;">}</span></span>
<span id="cb13-36"></span>
<span id="cb13-37"></span>
<span id="cb13-38"><span class="kw" style="color: #003B4F;">class</span> Some <span class="op" style="color: #5E5E5E;">:</span><span class="dt" style="color: #AD0000;">UIViewController</span> <span class="op" style="color: #5E5E5E;">{</span></span>
<span id="cb13-39"></span>
<span id="cb13-40">  <span class="co" style="color: #5E5E5E;">// This has to be injected via property injection</span></span>
<span id="cb13-41">    <span class="kw" style="color: #003B4F;">var</span> <span class="va" style="color: #111111;">networkingFactory</span><span class="op" style="color: #5E5E5E;">:</span> NetworkingFactory<span class="op" style="color: #5E5E5E;">!</span> </span>
<span id="cb13-42"></span>
<span id="cb13-43">    <span class="kw" style="color: #003B4F;">var</span> <span class="va" style="color: #111111;">networking</span><span class="op" style="color: #5E5E5E;">:</span> Networking<span class="op" style="color: #5E5E5E;">?</span></span>
<span id="cb13-44"></span>
<span id="cb13-45">    <span class="at" style="color: #657422;">@IBOutlet</span> <span class="kw" style="color: #003B4F;">weak</span> <span class="kw" style="color: #003B4F;">var</span> <span class="va" style="color: #111111;">proxySwitch</span><span class="op" style="color: #5E5E5E;">:</span> UISwitch<span class="op" style="color: #5E5E5E;">!</span></span>
<span id="cb13-46"></span>
<span id="cb13-47">    <span class="at" style="color: #657422;">@IBAction</span> <span class="kw" style="color: #003B4F;">func</span> <span class="fu" style="color: #4758AB;">userTappedSubmit</span><span class="op" style="color: #5E5E5E;">(</span><span class="va" style="color: #111111;">sender</span><span class="op" style="color: #5E5E5E;">:</span> <span class="dt" style="color: #AD0000;">UIButton</span><span class="op" style="color: #5E5E5E;">)</span> <span class="op" style="color: #5E5E5E;">{</span></span>
<span id="cb13-48">        networking <span class="op" style="color: #5E5E5E;">=</span> networkingFactory<span class="op" style="color: #5E5E5E;">.</span>create<span class="op" style="color: #5E5E5E;">(</span>withProxy<span class="op" style="color: #5E5E5E;">:</span> proxySwitch<span class="op" style="color: #5E5E5E;">.</span>isOn<span class="op" style="color: #5E5E5E;">)</span></span>
<span id="cb13-49">    <span class="op" style="color: #5E5E5E;">}</span></span>
<span id="cb13-50"><span class="op" style="color: #5E5E5E;">}</span></span>
<span id="cb13-51"></span>
<span id="cb13-52"></span>
<span id="cb13-53"><span class="co" style="color: #5E5E5E;">// Injection using closure</span></span>
<span id="cb13-54"></span>
<span id="cb13-55"><span class="kw" style="color: #003B4F;">class</span> Some <span class="op" style="color: #5E5E5E;">:</span><span class="dt" style="color: #AD0000;">UIViewController</span> <span class="op" style="color: #5E5E5E;">{</span></span>
<span id="cb13-56"></span>
<span id="cb13-57">    <span class="co" style="color: #5E5E5E;">// This has to be injected via property injection</span></span>
<span id="cb13-58">    <span class="kw" style="color: #003B4F;">var</span> <span class="va" style="color: #111111;">networkingFactory</span><span class="op" style="color: #5E5E5E;">:</span> <span class="op" style="color: #5E5E5E;">((</span>Bool<span class="op" style="color: #5E5E5E;">)</span> <span class="op" style="color: #5E5E5E;">-&gt;</span> Networking<span class="op" style="color: #5E5E5E;">)!</span></span>
<span id="cb13-59"></span>
<span id="cb13-60">    <span class="kw" style="color: #003B4F;">var</span> <span class="va" style="color: #111111;">networking</span><span class="op" style="color: #5E5E5E;">:</span> Networking<span class="op" style="color: #5E5E5E;">?</span></span>
<span id="cb13-61"></span>
<span id="cb13-62">    <span class="at" style="color: #657422;">@IBOutlet</span> <span class="kw" style="color: #003B4F;">weak</span> <span class="kw" style="color: #003B4F;">var</span> <span class="va" style="color: #111111;">proxySwitch</span><span class="op" style="color: #5E5E5E;">:</span> UISwitch<span class="op" style="color: #5E5E5E;">!</span></span>
<span id="cb13-63"></span>
<span id="cb13-64">    <span class="at" style="color: #657422;">@IBAction</span> <span class="kw" style="color: #003B4F;">func</span> <span class="fu" style="color: #4758AB;">userTappedSubmit</span><span class="op" style="color: #5E5E5E;">(</span><span class="va" style="color: #111111;">sender</span><span class="op" style="color: #5E5E5E;">:</span> <span class="dt" style="color: #AD0000;">UIButton</span><span class="op" style="color: #5E5E5E;">)</span> <span class="op" style="color: #5E5E5E;">{</span></span>
<span id="cb13-65">        networking <span class="op" style="color: #5E5E5E;">=</span> networkingFactory<span class="op" style="color: #5E5E5E;">(</span>proxySwitch<span class="op" style="color: #5E5E5E;">.</span>isOn<span class="op" style="color: #5E5E5E;">)</span></span>
<span id="cb13-66">    <span class="op" style="color: #5E5E5E;">}</span></span>
<span id="cb13-67"><span class="op" style="color: #5E5E5E;">}</span></span>
<span id="cb13-68"></span></code></pre></div>
</section>
</section>
<section id="dependency-injection---containers-frameworks" class="level1">
<h1>Dependency injection - Containers &amp; Frameworks</h1>
<p>There are Dependency Injection frameworks that make the job of dependency injection easier. You may say, “whoa Sai! wait,Do we really need a dependency injection framework as a dependency? Can’t it be done”.</p>
<p>When we examine what we are doing with DI, we are building a graph with our concrete types as nodes, and their dependencies linking them. If we do this completely, all dependencies will originate from a root object.</p>
<p>Without a framework, we will be doing a lot of copy paste coding. If our app uses networking protocol type in multiple areas, we have to type out the same concrete implementation everywhere, and fill out every dependency of networking class everywhere.</p>
<p>For example:</p>
<div class="sourceCode" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode swift code-with-copy"><code class="sourceCode swift"><span id="cb14-1"> <span class="kw" style="color: #003B4F;">class</span> A <span class="op" style="color: #5E5E5E;">{</span> </span>
<span id="cb14-2">    <span class="kw" style="color: #003B4F;">var</span> <span class="va" style="color: #111111;">propertyInjectionVar</span> <span class="op" style="color: #5E5E5E;">:</span>V<span class="op" style="color: #5E5E5E;">!</span></span>
<span id="cb14-3"></span>
<span id="cb14-4">    <span class="kw" style="color: #003B4F;">init</span><span class="op" style="color: #5E5E5E;">(</span>networking<span class="op" style="color: #5E5E5E;">:</span> Networking<span class="op" style="color: #5E5E5E;">,</span> <span class="op" style="color: #5E5E5E;">...)</span> </span>
<span id="cb14-5">  <span class="op" style="color: #5E5E5E;">}</span></span>
<span id="cb14-6"></span>
<span id="cb14-7"> <span class="co" style="color: #5E5E5E;">// To create A we have to create networking and also fill out any property injection vars it needs</span></span>
<span id="cb14-8"></span>
<span id="cb14-9"> <span class="kw" style="color: #003B4F;">class</span> NetworkService<span class="op" style="color: #5E5E5E;">:</span> <span class="dt" style="color: #AD0000;">Networking</span> <span class="op" style="color: #5E5E5E;">{</span> <span class="kw" style="color: #003B4F;">init</span><span class="op" style="color: #5E5E5E;">(</span>x<span class="op" style="color: #5E5E5E;">:</span> X<span class="op" style="color: #5E5E5E;">,</span> y<span class="op" style="color: #5E5E5E;">:</span> Y<span class="op" style="color: #5E5E5E;">,</span> z<span class="op" style="color: #5E5E5E;">:</span> Z<span class="op" style="color: #5E5E5E;">)</span> <span class="op" style="color: #5E5E5E;">{}</span> <span class="op" style="color: #5E5E5E;">}</span></span>
<span id="cb14-10"></span>
<span id="cb14-11"> <span class="co" style="color: #5E5E5E;">// Now networking will inturn have its dependencies , which inturn have more ..</span></span>
<span id="cb14-12"> <span class="co" style="color: #5E5E5E;">// So to create A, You have to create NetworkService, X, Y, Z </span></span>
<span id="cb14-13"></span>
<span id="cb14-14">A<span class="op" style="color: #5E5E5E;">(</span>networking NetworkService<span class="op" style="color: #5E5E5E;">(</span>x<span class="op" style="color: #5E5E5E;">:</span> X<span class="op" style="color: #5E5E5E;">(),</span> y<span class="op" style="color: #5E5E5E;">:</span> Y<span class="op" style="color: #5E5E5E;">(),</span> z<span class="op" style="color: #5E5E5E;">:</span> Z<span class="op" style="color: #5E5E5E;">()))</span></span></code></pre></div>
<p>To make this process easier we can build something to store list of dependency type, and their concrete implementation. We will have a table of mappings.</p>
<table class="table">
<thead>
<tr class="header">
<th>Dependency Type</th>
<th>Implementation type</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Networking</td>
<td>NetworkService</td>
</tr>
<tr class="even">
<td>X</td>
<td>XService</td>
</tr>
<tr class="odd">
<td>Y</td>
<td>YService</td>
</tr>
<tr class="even">
<td>Z</td>
<td>ZSubClass</td>
</tr>
<tr class="odd">
<td>A</td>
<td>A</td>
</tr>
</tbody>
</table>
<p>We can register a protocol Dependency Type to concrete implementation, like Networking, X, Y to NetworkService, XService, YService respectively. Or map concrete type to its concrete implementation which can be exactly same type, like A, or subclass like mapping Z to ZSubClass. Now what the container does is when A has to be created, it auto resolves each dependency of A from the table.</p>
<p>You can either implement a container, or use a DI Framework which does that for you.</p>
<p>Containers also allow you to autofill property injection, handle lifecycle of dependencies like marking them as singleton, so that your whole container has only one object of that type created and other fancy features to make your life easy.</p>
<blockquote class="blockquote">
<h3 id="a-key-point-to-remember-is-that-your-classes-should-not-depend-on-di-framework-container-ie-its-not-good-idea-to-pass-the-container-to-your-class-as-a-dependency." class="anchored">A key point to remember is that your classes should not depend on DI framework container ie its not good idea to pass the container to your class as a dependency.</h3>
</blockquote>
<section id="dip-framework---swift" class="level2">
<h2 class="anchored" data-anchor-id="dip-framework---swift"><a href="https://github.com/AliSoftware/Dip">Dip Framework - Swift</a></h2>
<p>Among the DI frameworks exiting now for swift, I recommend Dip. Dip has some nifty features to make your DI pain free.</p>
<section id="features-of-dip" class="level3">
<h3 class="anchored" data-anchor-id="features-of-dip">Features of Dip</h3>
<ul>
<li><strong>Scopes</strong>. Dip supports 5 different scopes (or life cycle strategies): <em>Unique</em>, <em>Shared</em>, <em>Singleton</em>, <em>EagerSingleton</em>, <em>WeakSingleton</em>;</li>
<li><strong>Auto-wiring</strong> &amp; <strong>Auto-injection</strong>. Dip can infer your components’ dependencies injected in constructor and automatically resolve them as well as dependencies injected with properties.</li>
<li><strong>Resolving optionals</strong>. Dip is able to resolve constructor or property dependencies defined as optionals.</li>
<li><strong>Type forwarding</strong>. You can register the same factory to resolve different types implemeted by a single class.</li>
<li><strong>Circular dependencies</strong>. Dip will be able to resolve circular dependencies if you will follow some simple rules;</li>
<li><strong>Storyboards integration</strong>. You can easily use Dip along with storyboards and Xibs without ever referencing container in your view controller’s code;</li>
<li><strong>Named definitions</strong>. You can register different factories for the same protocol or type by registering them with <a href="">tags</a>;</li>
<li><strong>Runtime arguments</strong>. You can register factories that accept up to 6 runtime arguments (and extend it if you need);</li>
<li><strong>Easy configuration</strong> &amp; <strong>Code generation</strong>. No complex containers hierarchy, no unneeded functionality. Tired of writing all registrations by hand? There is a <a href="https://github.com/ilyapuchka/dipgen">cool code generator</a> that will create them for you. The only thing you need is to annotate your code with some comments.</li>
<li><strong>Weakly typed components</strong>. Dip can resolve “weak” types when they are unknown at compile time.</li>
<li><strong>Thread safety</strong>. Registering and resolving components is thread safe;</li>
<li><strong>Helpful error messages and configuration validation</strong>. You can validate your container configuration. If something can not be resolved at runtime Dip throws an error that completely describes the issue;</li>
</ul>
</section>
<section id="annotations-in-dip" class="level3">
<h3 class="anchored" data-anchor-id="annotations-in-dip">Annotations in Dip</h3>
<p>Java has good frameworks like <a href="https://square.github.io/dagger/">Dagger</a>, and <a href="https://github.com/google/guice">Guice</a> that use annotations to make the job even simpler compared to swift. Dip allows you to leave annotations of dependencies in comments and also generate code for DI from it. How cool is that?</p>
</section>
</section>
<section id="poor-mans-di-in-swift" class="level2">
<h2 class="anchored" data-anchor-id="poor-mans-di-in-swift">Poor man’s DI in Swift</h2>
<p>Though I recommend the framework approach, supposing you don’t want to use framework initially and still want the to do DI, Fear not. You can use Swift’s default parameters to do constructor/method injection, and just use variable properties for property injection. You can maintain a Seperate DI singleton and fill dependencies using that.</p>
<div class="sourceCode" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode swift code-with-copy"><code class="sourceCode swift"><span id="cb15-1"></span>
<span id="cb15-2"></span>
<span id="cb15-3"><span class="kw" style="color: #003B4F;">class</span> UserModel <span class="op" style="color: #5E5E5E;">{</span></span>
<span id="cb15-4">    <span class="kw" style="color: #003B4F;">static</span> <span class="kw" style="color: #003B4F;">let</span> <span class="va" style="color: #111111;">serializer</span><span class="op" style="color: #5E5E5E;">:</span> Serializing</span>
<span id="cb15-5"></span>
<span id="cb15-6">    <span class="kw" style="color: #003B4F;">init</span><span class="op" style="color: #5E5E5E;">(</span>_ serializer<span class="op" style="color: #5E5E5E;">:</span> Serializing <span class="op" style="color: #5E5E5E;">=</span> PoormanDIContainer<span class="op" style="color: #5E5E5E;">.</span>instance<span class="op" style="color: #5E5E5E;">.</span>getSerializer<span class="op" style="color: #5E5E5E;">())</span> <span class="op" style="color: #5E5E5E;">{</span> <span class="co" style="color: #5E5E5E;">// Poor man DI</span></span>
<span id="cb15-7">        <span class="kw" style="color: #003B4F;">self</span><span class="op" style="color: #5E5E5E;">.</span>serializer <span class="op" style="color: #5E5E5E;">=</span> serializer</span>
<span id="cb15-8">    <span class="op" style="color: #5E5E5E;">}</span></span>
<span id="cb15-9"></span>
<span id="cb15-10">    <span class="kw" style="color: #003B4F;">var</span> <span class="va" style="color: #111111;">name</span><span class="op" style="color: #5E5E5E;">:</span> String <span class="op" style="color: #5E5E5E;">{</span></span>
<span id="cb15-11">        <span class="kw" style="color: #003B4F;">get</span> <span class="op" style="color: #5E5E5E;">{</span></span>
<span id="cb15-12">            <span class="kw" style="color: #003B4F;">return</span> serializer<span class="op" style="color: #5E5E5E;">.</span>string<span class="op" style="color: #5E5E5E;">(</span>forKey<span class="op" style="color: #5E5E5E;">:</span> <span class="st" style="color: #20794D;">"userName"</span><span class="op" style="color: #5E5E5E;">)</span>  <span class="op" style="color: #5E5E5E;">??</span> <span class="st" style="color: #20794D;">""</span></span>
<span id="cb15-13">         <span class="op" style="color: #5E5E5E;">}</span></span>
<span id="cb15-14">        <span class="kw" style="color: #003B4F;">set</span> <span class="op" style="color: #5E5E5E;">{</span></span>
<span id="cb15-15">            serializer<span class="op" style="color: #5E5E5E;">.</span>setObject<span class="op" style="color: #5E5E5E;">(</span>newValue<span class="op" style="color: #5E5E5E;">,</span> forKey<span class="op" style="color: #5E5E5E;">:</span><span class="st" style="color: #20794D;">"userName"</span><span class="op" style="color: #5E5E5E;">)</span></span>
<span id="cb15-16">        <span class="op" style="color: #5E5E5E;">}</span></span>
<span id="cb15-17">    <span class="op" style="color: #5E5E5E;">}</span></span>
<span id="cb15-18"><span class="op" style="color: #5E5E5E;">}</span></span>
<span id="cb15-19"></span>
<span id="cb15-20"></span>
<span id="cb15-21"><span class="kw" style="color: #003B4F;">class</span> PoorManDIContainer <span class="op" style="color: #5E5E5E;">{</span></span>
<span id="cb15-22">    <span class="kw" style="color: #003B4F;">let</span> <span class="va" style="color: #111111;">instance</span> <span class="op" style="color: #5E5E5E;">=</span> PoorManDIContainer<span class="op" style="color: #5E5E5E;">()</span> </span>
<span id="cb15-23">    <span class="kw" style="color: #003B4F;">func</span> <span class="fu" style="color: #4758AB;">getSerializer</span><span class="op" style="color: #5E5E5E;">()</span> -&gt; <span class="fu" style="color: #4758AB;">Serializing</span> <span class="op" style="color: #5E5E5E;">{</span></span>
<span id="cb15-24">        <span class="co" style="color: #5E5E5E;">// If Serializer has some dependencies, it will again use default constructor to obtain it from PoorManDIContainer</span></span>
<span id="cb15-25">        <span class="kw" style="color: #003B4F;">return</span> Serializer<span class="op" style="color: #5E5E5E;">()</span> </span>
<span id="cb15-26">    <span class="op" style="color: #5E5E5E;">}</span></span>
<span id="cb15-27"><span class="op" style="color: #5E5E5E;">}</span></span>
<span id="cb15-28"></span></code></pre></div>
<p>But if you use the above method, beware of circular dependencies.</p>
</section>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<p>So in conclusion DI is great. It allows you to progressively make your code better remove singletons, make your code modular, testable and also allow you to evolve good design patterns. Try it out in your existing code base, it will be one of the easiest way to refactor legacy OOP code, without modifying internal logic initially.If you have any doubts, suggestions, constructive criticisms, comment below.</p>


</section>

 ]]></description>
  <category>iOS</category>
  <category>Swift</category>
  <guid>saiprasanna.in/posts/dependency-injection-what-why-and-how/index.html</guid>
  <pubDate>Mon, 20 Mar 2017 00:00:00 GMT</pubDate>
  <media:content url="https://ih0.redbubble.net/image.5150955.4141/flat,1000x1000,075,f.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Manjaro Linux - My current daily driver arch based distro</title>
  <link>saiprasanna.in/posts/Manjaro-Linux/index.html</link>
  <description><![CDATA[ 




<p>The partition having arch in my secondary hard disk had died. The invisible grime which settles in your mind because of using non free windows and osX at home and work had started to bother me.</p>
<p>And as I got a new mechanical keyboard (Reddragon KUMARA, cheap as cherry MX patents on mechanical switches have died out) and a gaming mouse (Logitech G402) recently, I wanted to use my desktop as main programming machine, instead of my old macbook air.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="saiprasanna.in/posts/Manjaro-Linux/2017-03-20-my-desktop.JPG" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">My Current PC with keyboard and mouse</figcaption><p></p>
</figure>
</div>
<p>So to inevitably I was going to reinstall GNU/Linux.Choosing a new linux distro as you all know leads to paralysis due to infinite choice.</p>
<p>Of all the distros that I had installed, I love Arch Linux the most. Arch with its rolling update model, and the infinite repositories of aur, always hit the sweet spot. But I was feeling a bit lazy, as in Arch Linux one has to setup everything from scratch (which I recommend at least once), and for some reason my font settings was always bad in vanilla Arch Setup. I know that is not exactly an insurmountable problem, but as I said before, was feeling lazy.</p>
<p>After a love/hate relationship with mac OS, I really wanted a good GUI experience. I am KDE kind of a guy, and love KDE plasma environment. I shopped around a bit, and found elementaryOS based on new desktop environment called the Pantheon. Thought I would give it a shot, though it wasn’t Arch based. But eventually decided against it, because it didn’t seem very customizable, which is rather the point of elementaryOS.</p>
<p>So my endless distro search, ended up with deciding between variants based on arch linux. <a href="https://manjaro.org/get-manjaro/">Manjaro</a>, Antergos , ApricityOS, I eventually decided to install Manjaro, because I heard that other two are basically GUI installers of Arch, while Manjaro was a distro based on arch with customizations to make it a more smooth ride.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="saiprasanna.in/posts/Manjaro-Linux/https:/manjaro.org/wp-content/uploads/2017/03/kde-170.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Image of Manjaro KDE Desktop</figcaption><p></p>
</figure>
</div>
<p>Boy, Manjaro with KDE, had refreshing feel, it felt like what Linux mint was when the first time I installed it. And propriety driver support was out of the box. I would recommend Manjaro as daily development OS, where you want arch, but don’t feel like configuring it perfectly.</p>



 ]]></description>
  <category>KDE</category>
  <category>Manjaro</category>
  <category>Arch Linux</category>
  <guid>saiprasanna.in/posts/Manjaro-Linux/index.html</guid>
  <pubDate>Fri, 17 Mar 2017 00:00:00 GMT</pubDate>
  <media:content url="saiprasanna.in/posts/Manjaro-Linux/2017-03-20-my-desktop.JPG" medium="image"/>
</item>
<item>
  <title>Crashes are Optional! && Write Less, Do More</title>
  <link>saiprasanna.in/posts/crashes-are-optional/index.html</link>
  <description><![CDATA[ 




<p>It was a boring tuesday/wednesday afternoon, co-worker <a href="https://twitter.com/giridharvc7">Giridhar</a> pinged me. He was the organizer for <a href="https://swiftindia.github.io/swiftindia/">Swift India</a> meetups. He invited me to give a talk. As I am in trying to doing things that I haven’t done before, I accepted readily.</p>
<p>About 60-66 people turned out for the <a href="https://www.meetup.com/SwiftChennai/events/236837342/">meetup</a>. It was fun and great learning experience to share what you know with others. It also exposed where I have to concentrate to develop my public speaking skills.</p>
<p>Here are the slides for the talk. I will try to put up the playground file later.</p>
<iframe width="560" height="315" src="https://docs.zoho.com/show/publish/15oj06f25e1965b3a40388575dcd418c8954e" frameborder="0">
</iframe>



 ]]></description>
  <category>iOS</category>
  <category>swift</category>
  <guid>saiprasanna.in/posts/crashes-are-optional/index.html</guid>
  <pubDate>Sun, 29 Jan 2017 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Open for Collaboration, Closed for Disturbance</title>
  <link>saiprasanna.in/posts/open-for-collaboration-closed-for-disturbance-rethinking-open-office-layout/index.html</link>
  <description><![CDATA[ 




<p>I have been working at open office environment for past 1 and a half years. The first office was a semi open one where workspaces had some degree of separation, not exactly a cubicle, honey comb like structure where there is circle of 4 people at center, and 6 or so people around. The one thing I liked about our office is that there are no corner offices for managers. Even CEO had to sit with others. Though this is my first job, I can appreciate the egalitarian and practical aspects of this. There was some degree of isolation to do deep work (though I seriously crave for more), while preserving the collaborative aspect of open office.</p>
<p>But after few months we moved to new workspace, things changed to the worse. It was the dreaded Complete open office plan, our building consists of floors with more than 200 people in each, and there is no separation between rows of tables. It is a complete nightmare, taking the good design to extreme and making it completely hard to bear.</p>
<p>This made me think about O of S.O.L.I.D principles of software design, which suggest that a good design should be open for extension and closed for modification.I think this applies allegorically to office layout plan.</p>
<p>Some companies have moved to ditching offices completely, but there are many still not taking such a drastic step. So the principle which I arrive on workplace design, Open for Collaboration and Closed for Disturbance. A good office design should make it accessible for people to collaborate and avoid creating unnecessary old age hierarchy bullshit that came in form of corner office perks. At the same time it should provide the much needed c isolation to get shit done. In general the design should at least have some barrier between people.It wouldn’t hurt to provide some “Deep think rooms” which can function as isolation chambers which can be opposite of meeting rooms.</p>
<p>Many studies are pointing to decreased level of productivity in open office plans. It is common sense that deep work requires some degree of isolation, which completely open spaces hardly provides. Every interruption completely derails the thought process which were happening till then, productivity.</p>
<p>Lets look at some facts,studies,opinions which point out to the decrease in productivity, happiness, and involvement in complete open office:</p>
<ol type="1">
<li>http://blog.ninlabs.com/2013/01/programmer-interrupted/</li>
</ol>
<blockquote class="blockquote">
<p>Based on a analysis of 10,000 programming sessions recorded from 86 programmers using Eclipse and Visual Studio and a survey of 414 programmers (Parnin:10), we found: A programmer takes between 10–15 minutes to start editing code after resuming work from an interruption. When interrupted during an edit of a method, only 10% of times did a programmer resume work in less than a minute. A programmer is likely to get just one uninterrupted 2-hour session in a day We also looked at some of the ways programmers coped with interruption: Most sessions programmers navigated to several locations to rebuild context before resuming an edit. Programmers insert intentional compile errors to force a “roadblock” reminder. A source diff is seen as a last resort way to recover state but can be cumbersome to review</p>
</blockquote>
<ol start="2" type="1">
<li>http://www.forbes.com/sites/davidburkus/2016/06/21/why-your-open-office-workspace-doesnt-work/</li>
</ol>
<blockquote class="blockquote">
<p>Thus while noise was a problem, the greater noise level didn’t appear to be from all of the collective collaboration buzzing around the open room. The researchers then took their analysis one step further, using regression to calculate how important each dimension was to employees’ overall satisfaction. One of the dimensions most strongly related to overall satisfaction was ease of interaction, despite the fact that it was judged to be no better or worse in open office plans than in private offices. In other words, the desire for more collaboration among employees was shared by all, but those in open office plans may not have found it to be worth all of the stress and distraction from the bombardment of noise.</p>
</blockquote>
<ol start="3" type="1">
<li>http://www.newyorker.com/business/currency/the-open-office-trap</li>
</ol>
<blockquote class="blockquote">
<p>But the most problematic aspect of the open office may be physical rather than psychological: simple noise. In laboratory settings, noise has been repeatedly tiedto reduced cognitive performance. The psychologist Nick Perham, who studies the effect of sound on how we think, has found that office commotion impairsworkers’ ability to recall information, and even to do basic arithmetic. Listening to music to block out the office intrusion doesn’t help: even that, Perham found, impairs our mental acuity. Exposure to noise in an office may also take a toll on the health of employees. In a study by the Cornell University psychologists Gary Evans and Dana Johnson, clerical workers who were exposed to open-office noise for three hours had increased levels of epinephrine — a hormone that we often call adrenaline, associated with the so-called fight-or-flight response. What’s more, Evans and Johnson discovered that people in noisy environments made fewer ergonomic adjustments than they would in private, causing increased physical strain. The subjects subsequently attempted to solve fewer puzzles than they had after working in a quiet environment; in other words, they became less motivated and less creative.</p>
</blockquote>
<ol start="4" type="1">
<li>http://www.inc.com/geoffrey-james/why-your-company-will-benefit-from-getting-rid-of-open-office-spaces-first-90.html</li>
</ol>
<blockquote class="blockquote">
<p>They decrease productivity. Contrary to popular belief, open offices don’t increase collaboration or make people more productive. An Exeter University study showed they create a 32 percent drop in “workers’ well-being” and 15 percent reduction in productivity. .They create time-consuming distractions.Office workers lose an average of 86 minutes per day due to distractions associated with open-plan offices. As a result, many employees are “unmotivated, unproductive, and overly stressed,” according to the study funded by Steelcase. They make employees sick.A study at Queensland University of Technology’s Institute of Health and Biomedical Innovation found that working in environments without offices “caus[es] high levels of stress, conflict, high blood pressure, and a high staff turnover.” This comic describes it in a funny way on what is the cost of interruptions on programmers. This can apply to any field that involves some thinking to be done at work.</p>
</blockquote>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="saiprasanna.in/posts/open-for-collaboration-closed-for-disturbance-rethinking-open-office-layout/https:/i.stack.imgur.com/oYpue.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Why Programmers shouldn’t be interrupted? Comic strip</figcaption><p></p>
</figure>
</div>



 ]]></description>
  <category>work place</category>
  <category>design</category>
  <guid>saiprasanna.in/posts/open-for-collaboration-closed-for-disturbance-rethinking-open-office-layout/index.html</guid>
  <pubDate>Fri, 06 Jan 2017 00:00:00 GMT</pubDate>
  <media:content url="https://i.stack.imgur.com/oYpue.png" medium="image" type="image/png"/>
</item>
<item>
  <title>Function Currying, Composition in Redux Middleware</title>
  <link>saiprasanna.in/posts/function-currying-and-composition/index.html</link>
  <description><![CDATA[ 




<p>I am new to functional programming paradigm, though I do understand something about closures ,pure functions and have used some programming structures related to functional concepts like map, reduce, blocks in my iOS applications at work, I don’t have much experience with functional paradigm.</p>
<p>I started learning about react js, and eventually ended up learning about Redux. I would assume you to have basic knowledge about redux , if not read about it here. It is very well documented, and the whole api is small, If you are feeling adventurous read the redux source code, it is a few hundred lines.</p>
<p>Redux seems to be a more elegant implementation of Flux methodology without some components of flux like multiple stores communication via a dispatcher which is made redundant in redux pattern.</p>
<p>So as I was playing with redux by implementing a small application mentioned in this tutorial , I came across something called middlewares. These middleware intercept the actions to store and do some extra processing on it. I couldn’t wrap my head around how a middleware for handling Promises mentioned in that post worked.</p>
<p>{% highlight js %} export default function promiseMiddleware() { return next =&gt; action =&gt; { const { promise, type, …rest } = action;</p>
<pre><code>if (!promise) return next(action);

const SUCCESS = type;
const REQUEST = type + '_REQUEST';
const FAILURE = type + '_FAILURE';
next({ ...rest, type: REQUEST });
return promise
  .then(res =&gt; {
    next({ ...rest, res, type: SUCCESS });

    return true;
  })
  .catch(error =&gt; {
    next({ ...rest, error, type: FAILURE });

    // Another benefit is being able to log all failures here
    console.log(error);
  return false;
  });</code></pre>
<p>}; } {% endhighlight %}</p>
<p>It seemed to use functional magic which somehow allowed waiting for a promise to resolve and then dispatch the action or call next middleware. S o I was wondering how that worked and came across this excellent post titled <a href="https://medium.com/@meagle/understanding-87566abcfb7a#.l7hsexyp7">“Understanding Redux Middleware”</a>. I encourage you to check it out here if you haven’t already. This post is meant to clarify some things in it a bit further. From that article I came to understand that middleware functions got composed in the following fashion</p>
<pre><code>Middleware1(Middleware2(…))</code></pre>
<p>Then by chance I was reading through documentation of redux-logger middleware which said that for it to log state , actions properly from it has to be passed as parameter after any asynchronous middleware. So from what I read in redux source for applyMiddleware I gathered that logger middleware will be a parameter for Async middleware for it to log correctly, Assume we do this , then</p>
<pre><code>applyMiddleware(PromiseMiddleware, LoggerMiddleware)</code></pre>
<p>will compose them in this order</p>
<pre><code>PromiseMiddleware(LoggerMiddleware)</code></pre>
<p>This seemed to make less sense as I wrongly thought that promise middleware will wait for LoggerMiddeware to process and then execute.</p>
<p>But after meditating on the redux source code ,creating curried functions and composing them , I got it. The actual execution order of middleware when dispatch action occurs, starts from PromiseMiddleware, which resolves the promise and only then calls the LoggerMiddleware.</p>
<p>This is because LoggerMiddleware is a curried function , so it doesn’t return any value and it is simply a function that is passed to Promisemiddleware. The Promisemiddleware takes up the action, resolves it, and inside “then” of promise calls the LoggerMiddleware using</p>
<pre><code>next({ ...rest, res, type: SUCCESS });</code></pre>
<p>and hence passes the action to LoggerMiddleware Function. The PromiseMiddleware can refer to the next function inside it using the properly named param “next”. Now lets make some curried functions with asynchronous operations and uncurry them with parameters . Type these in js console We are first creating a curried function that takes another function as input and also some params.</p>
<p>{% highlight js %} var asyncFunction = nextFunction =&gt; params =&gt; window.setTimeout( function() { alert(“In Async Callback”); nextFunction(params); }, 1000); {% endhighlight %}</p>
<p>This is similar to the promise middleware. asyncFunction takes in parameter a function called nextFunction. From the signature of nextFunction(params) used inside , we can get it that it simply takes a object. Now let us create a function to pass into async</p>
<p>{% highlight js %} var alertFunction = parameters =&gt; alert(parameter); {% endhighlight %}</p>
<p>alertFunction simply alerts the parameter passed to it. COMPOSING Alert and asyncFunction manually, and calling the result of composition</p>
<p>{% highlight js %} var composedFunction = asyncFunction(alertFunction); composedFuntion(1); {% endhighlight %}</p>
<p>Output Alert:- In Async Callback Alert:- 1</p>
<p>VOILA! Our alertFunction executes inside the async callback To clarify replacing the nextFunction with body of alertFunction</p>
<p>{% highlight js %} function params =&gt; window.setTimeout(function() { alert(“In Async Handler”); // nextFunction(params) becomes alert(params); }, 1000) {% endhighlight %}</p>
<p>I couldn’t grok async code in middleware because I was making similar mistake as in thinking that as alertFunction is innermost function in composition and so its body would execute first.But in actuality it doesn’t evaluate to value and hence the order of execution starts from outermost function .</p>
<p>This seems trivial after seeing the above example but it wrecked my understanding of middleware code. if G is a function and F is higher order function , ie F(someInputFunction) is also a function , then when you “uncurry” by calling F(G)(x) the evaluation begins from operations defined inside/ to perform F, and operations defined inside/to perform G may or may not be used by F.</p>
<p>So we can wrapping asynchronous operations in a chain of composed functions, define them in seemingly synchronous way. And I think this is how Promises are actually implemented, will have to read about that later.</p>
<p>Anyway this really excites me, how mind bending functional jui jutsu can accomplish a lot with less code. Will post more when I learn more.. Eager to hear your experience in learning functional programming inception.</p>
<p>Reposted from <a href="https://medium.com/@sai_prasanna/order-of-execution-in-function-currying-composition-in-context-of-asynchronous-operation-722575cb382c#.4y6l1uttu">medium.com</a></p>



 ]]></description>
  <category>Javascript</category>
  <category>Redux</category>
  <category>Functional Programming</category>
  <guid>saiprasanna.in/posts/function-currying-and-composition/index.html</guid>
  <pubDate>Sun, 10 Jul 2016 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Creation and Consumption</title>
  <link>saiprasanna.in/posts/creation-and-consumption/index.html</link>
  <description><![CDATA[ 




<p>The amount of content available for consumption in our digital age is staggering. Movies, TV, video games, videos, blogs, emails, chat messages all compete for our limited time.</p>
<p>I think mindful consumption is of great importance. We can start being aware of what our consumption inlets are. Just the awareness of this can create great impact on how we choose to spend our time.</p>
<p>I am not arguing against consuming media, but for having healthy awareness on wether it has negative effects on one’s creative spirit. It is important in my opinion to keep the act of creation balanced with consumption.</p>
<p>To contemplate, and engage actively with the world through some outlet is a form of habit I hope we can get into. This outlet can be anything, writing, music, coding for open source, some personal project etc.</p>
<p>I think it is more fulfilling to move towards a balanced consumption to creation ratio.</p>



 ]]></description>
  <category>Note to self</category>
  <guid>saiprasanna.in/posts/creation-and-consumption/index.html</guid>
  <pubDate>Sun, 10 Jul 2016 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Speed up iOS dev using XCode Injection Plugin</title>
  <link>saiprasanna.in/posts/speed-up-ios-development/index.html</link>
  <description><![CDATA[ 




<p><a href="http://injectionforxcode.com/">Injection for XCode</a>.</p>
<p><img src="saiprasanna.in/posts/speed-up-ios-development/https:/camo.githubusercontent.com/95389a5ccf548360add8cf7270c84030cc874971/687474703a2f2f672e7265636f726469742e636f2f69366b5166544d4570672e676966" width="640"></p>
<p>One of the most boring/unproductive part of any development cycle is waiting for your project to compile. And if you have done iOS development, you know how much time is wasted for XCode to recompile the project. And even if compile time is less, you have to follow a bunch of taps, long presses etc to get to the desired app state before even testing your changes. This plugin will reduces reduce these steps considerably. As you change your code , you can inject the new class definition using this XCode plugin. It recompiles just changed file, and injects it into the live running app. The great thing is it works for real device as well.</p>
<section id="set-up" class="level3">
<h3 class="anchored" data-anchor-id="set-up">Set up</h3>
<ol type="1">
<li>Install <a href="http://alcatraz.io/">Alcatraz</a> package manager for XCode . Alcatraz allows you to install and remove XCode plugins hazzle free</li>
<li>Open using Alcatraz with package manager option in projects menu.</li>
<li>Search for injection plugin, and install it. Restart XCode after installations</li>
<li>To make it work in real device you need to click on <em>Product -&gt; Injection Plugin -&gt; Patch Project</em> for Injection. It will add couple of lines to your main.m of your project. If it is a swift project just create a empty main.m and do the above.</li>
</ol>
</section>
<section id="inject-code" class="level3">
<h3 class="anchored" data-anchor-id="inject-code">Inject Code</h3>
<ol type="1">
<li>Run your project, make some changes to the code in a file, press ^ + = .</li>
<li>Your changed code in that file will get compiled and injected into the app live.</li>
<li>Now you can simply have to somehow make your program create new object of the changed class to see the changes. For example, tap back button and then again go the view.</li>
<li>There are few limitations on what can be injected, refer the Injection for XCode’s <a href="https://github.com/johnno1962/injectionforxcode">github project</a>. And a small limitation currently is it wont work if you have more than 128 source files in your project due to a XCode limitation. Follow this and other issues in github issue tracker.</li>
<li>You can also set it up to automatically inject changes by enabling File Watcher in <em>Product-&gt; Injection Plugin -&gt; Tunable App Parameters</em></li>
<li>If you want it to make changes visible as you inject the changes in you have to modify your normal code. You have to listen for callbacks after injection , and reload the view or do something else.Read the <a href="https://github.com/johnno1962/injectionforxcode#callbacks-in-your-code">instructions</a>.</li>
</ol>
<p>This plugin will add a new folder to your project which you can add to .gitignore to avoid source control.</p>
<p>It is great that the developer has open sourced it. To know how it does its magic see <a href="https://github.com/johnno1962/injectionforxcode#how-it-works">here</a> The author has released it under “nagware” license, where he requests you to pay after using it for two weeks.</p>


</section>

 ]]></description>
  <category>iOS</category>
  <guid>saiprasanna.in/posts/speed-up-ios-development/index.html</guid>
  <pubDate>Sun, 17 Apr 2016 23:55:00 GMT</pubDate>
</item>
<item>
  <title>MIT OCW 6.006 Algorithms Course</title>
  <link>saiprasanna.in/posts/MIT-OCW-Algorithms/index.html</link>
  <description><![CDATA[ 




<p>MIT OCW course is one among the best introductory algorithm courses online. It introduces techniques to analyse, and understand how algorithms work.</p>
<p><a href="http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-006-introduction-to-algorithms-fall-2011/">MIT OCW 6.006 Introduction to Algorithms Fall 2011</a> course is packed with awesome content. The open course ware website contains video recording of the MIT course conducted at MIT and all the accompanying notes, assignments, test content. It does not contain interactive content like udacity courses, but the content is in different level.You need to know python to do this course’s assignments. If you know how to code in any language , you can pick up basics of python in a day or two as it has simple syntax.</p>
<p>Prof.&nbsp;Erik Demaine ,Prof.&nbsp;Srinivas Devadas do a great job introducing concepts of algorithms and its analysis in the main lectures. Victor Costan does exemplary job in recitation videos in explaining the concepts introduced by Erik and Srini in easily understandable way. So don’t miss out on recitation if you plan to look into this course.</p>
<p>The assignments are one of the fun parts of the whole course. They allow you to see how efficient algorithms can really make a difference in running time of your code. These guys have done a awesome job in designing each assignment in such a way that you can visualise the impact of algorithms. For example in assignment 3 where you are supposed to write code to detect crossings among wires, they have designed the assignment in such a way that you can see the result of your code running in the browser.</p>
The course videos are also available in youtube as playlist. Checkout the first video.
<p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/HtSuA80QTyo?list=PLUl4u3cNGP61Oq3tWYp6V_F-5jb5L2iHb" frameborder="0">
</iframe>
</p>



 ]]></description>
  <category>Algorithms</category>
  <category>Computer Science</category>
  <category>MOOC</category>
  <guid>saiprasanna.in/posts/MIT-OCW-Algorithms/index.html</guid>
  <pubDate>Sat, 02 Apr 2016 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Blog using Github pages</title>
  <link>saiprasanna.in/posts/new-blog/index.html</link>
  <description><![CDATA[ 




<p>Github.com allows us to upload just templates, config and theme for jekyll static blog generator, and it generates a static website for you.</p>
<p>Now you can just write a markdown file, and commit it, or even create a file online even in github’s source browser, and it is automatically generated into a blog post just after the commit is made.</p>
<p>I didn’t notice this feature in github.com before. I thought we had to run jekyll locally to build static site, and commit it for github to serve stuff. So rather went for python static blog generator called pelican, but gave up because of it being a pain to generate blog every time. I even tried to setup travis-ci to automagically generate the blog, but it was slow and was still a pain.</p>
<p>So if you want to create a blog, just use github , it is quite easy to setup. clone a theme, edit _config.yml and blog on.</p>



 ]]></description>
  <category>Jekyll</category>
  <category>Blogging</category>
  <guid>saiprasanna.in/posts/new-blog/index.html</guid>
  <pubDate>Wed, 23 Mar 2016 00:00:00 GMT</pubDate>
</item>
</channel>
</rss>
