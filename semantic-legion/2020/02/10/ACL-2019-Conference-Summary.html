<p>My colleague Ananda and I attended <a href="https://www.aclweb.org/portal/">ACL 2019 conference</a> at the enchanting city of Florence.
All the accepted papers can be accessed <a href="https://www.aclweb.org/anthology/events/acl-2019/">here</a>.
Here’s the summary of interesting trends and also specific research work that caught my eye at the conference. 
A note of thanks to my employer at Zoho for sponsoring us to attend.</p>

<p><em>I wrote this summary an many months ago and forgot posting it. Better late than never I guess.</em></p>

<h2 id="grammatical-error-correction">Grammatical Error Correction</h2>

<ul>
  <li>Among the ACL workshops, Building Educational Applications (BEA) Workshop had a <a href="https://www.cl.cam.ac.uk/research/nl/bea2019st/">Grammar Error Correction competition</a>.</li>
</ul>

<p>The system description papers for this competition were presented as posters in the conference.</p>

<ul>
  <li>Three tracks were present in the competition.
<strong>Restricted track</strong> - Only organizer provided human labelled parallel (error and corrected sentence pairs) data can be used. (No restriction on synthetic data)
<strong>Unrestricted track</strong> - Any data including private data can be used.
<strong>Low Resource track</strong> - No human labelled data can be used.</li>
  <li>Interestingly, the winning team (Edinburgh + Microsoft)’s <a href="https://www.aclweb.org/anthology/W19-4427/">submission</a> for Track 1 also beat Track 2 without using additional restricted data.</li>
  <li>Synthetic data generated by corrupting good grammatical sentences from news, books and wikipedia are the techniques used overall by top performing teams.</li>
</ul>

<h2 id="multi-lingual-models">Multi-Lingual Models</h2>

<p>MultiLingual models is a hot area of research now. Earlier results where using single model to perform tasks on multiple languages has shown promising results.</p>

<ul>
  <li>Lots of papers on multi-lingual shared models were presented.</li>
  <li>Paper - <a href="https://www.aclweb.org/anthology/papers/P/P19/P19-1301/">Choosing Transfer Languages for Cross-Lingual Learning</a></li>
</ul>

<h2 id="rise-of-automated-metrics">Rise of Automated Metrics</h2>

<p>Until recently, we compare model outputs with human written sentences for translation, summarization etc. 
This can artificially penalize models that generate sentences with equivalent meaning but not same words.
There are couple of papers that train models to score quality of the output. Then use these model scores 
as reward for reinforcement learning. (FYI reinforcement learning is only used for fine tuning, none of 
the seq2seq models can be trained from scratch using it)</p>

<ul>
  <li>Paper - <a href="https://www.aclweb.org/anthology/papers/P/P19/P19-1043/">This Email Could Save Your Life: Introducing the Task of Email Subject Line Generation</a> 
This paper uses automated score instead of typical NGram match (ROUGE) score for summarization task.</li>
  <li>Paper - <a href="https://www.aclweb.org/anthology/papers/P/P19/P19-1427/">Beyond BLEU:Training Neural Machine Translation with Semantic Similarity</a></li>
  <li>Paper - <a href="https://www.aclweb.org/anthology/papers/P/P19/P19-1264/">Sentence Mover’s Similarity: Automatic Evaluation for Multi-Sentence Texts</a></li>
</ul>

<h2 id="statistical-evaluation">Statistical Evaluation</h2>

<p>If we have two architectures and couple of datasets, how to say empirically one is better than the other?
Few questions are how to compare two models on the same dataset, across multiple datasets, across various hyperparameter configurations.
Problems in applying frequentist tests on the metrics such as accuracy, f1-score etc 
are that assumptions such as Independent and Identically distributed (IID) cannot be made for deep learning datasets.
So we cannot assume that the score the model gets in one dataset is “independent” of the score on another dataset. 
Statistical tests that don’t assume underlying distribution are needed.
Recent statistical methods/tests to do so are being developed and some were presented at the conference.</p>

<ul>
  <li>Paper - <a href="https://www.aclweb.org/anthology/papers/P/P19/P19-1266/%20">Deep Dominance - How to Properly Compare Deep Neural Models</a></li>
  <li>Paper - <a href="https://www.aclweb.org/anthology/papers/P/P19/P19-1405/">Bayes Test of Precision, Recall, and F1 Measure for Comparison of Two Natural Language Processing Models</a></li>
</ul>

<h2 id="bayesian-methods">Bayesian Methods</h2>

<ul>
  <li>Attended a very detailed tutorial on it. The presenter has summarized the evolution of research in this area and the current papers. Here’s link to the detailed <a href="https://drive.google.com/file/d/1SgNVpspG-m0O_k-_qAbxg-3HSZg3FOec/view?usp=sharing">slides</a> for fellow Bayesians.</li>
</ul>

<h2 id="analyzing-neural-nets-and-interpretability">Analyzing Neural Nets and Interpretability</h2>

<p>There is an entire sub-fields of research into analyzing and interpreting neural networks.</p>

<h3 id="bertology">BERTology</h3>

<p>“BERT-ology” papers that explore what linguistic structures do pre-trained models like BERT learn.</p>

<ul>
  <li>Paper - <a href="https://www.aclweb.org/anthology/papers/W/W19/W19-4828/">What Does BERT Look at? An Analysis of BERT’s Attention</a></li>
</ul>

<h3 id="blackboxnlp-workshop"><a href="https://www.aclweb.org/anthology/volumes/W19-48/">BlackBoxNLP Workshop</a></h3>

<p>An entire workshop devoted for analyzing what Neural Networks learn.</p>

<ul>
  <li>Paper - <a href="https://www.aclweb.org/anthology/papers/W/W19/W19-4814/">On the Realization of Compositionality in Neural Networks</a>
Interesting paper studying what is required for neural models to compose two very trivial functions.</li>
  <li>Paper - <a href="https://www.aclweb.org/anthology/papers/W/W19/W19-4826/">GEval: Tool for Debugging NLP Datasets and Models</a></li>
</ul>

<h3 id="formal-languages-workshop"><a href="https://www.aclweb.org/anthology/volumes/W19-39/%20">Formal Languages Workshop</a></h3>

<p>An entire small workshop devoted to finding what Formal Languages (Finite state Automata, etc) neural networks can learn.
e.g. Can we reduce a RNN to Weighted Finite State Machine (which is far more interpretable, amenable to theory etc).
Although this area sounds exciting to me, I was unable to attend it as I was in an another workshop.
Slides from talk of Noah Smith’s talk on <a href="https://homes.cs.washington.edu/~nasmith/slides/rrnn-dlfl-2019-08-02.pdf">Rational Recurrences</a> at this workshop.</p>

<h3 id="neuroscience-and-nlp">Neuroscience and NLP</h3>

<p>Neuroscience labs have started to use deep learning. An interesting conjunction of research in NLP and neuroscience research in correlating
ANN representations with brain signals was presented.</p>

<ul>
  <li>Paper - <a href="https://www.aclweb.org/anthology/papers/P/P19/P19-1507/">Relating Simple Sentence Representations in Deep Neural Networks and the Brain</a>
The researchers try to find relationship between deep learning language representations and brain signals.
Paper of interest is where they predict neural brain patterns using pre-trained ANN models like BERT.</li>
</ul>

<h3 id="language-emergence-in-multi-agent-systems">Language Emergence in Multi-Agent systems</h3>

<p>In this frontier, people try train models to solve some task by communicating symbols. Researchers analyze the properties of 
language used by the agents to solve the task and how it compares with properties of human language.</p>

<ul>
  <li>Paper - <a href="https://www.aclweb.org/anthology/papers/P/P19/P19-1509/">Word-order Biases in Deep-agent Emergent Communication</a></li>
</ul>

<h2 id="conversational-ai">Conversational AI</h2>

<ul>
  <li>Neural Models for selecting conversation from past history, detecting intent and slot fitting are all increasingly being deployed by companies.</li>
  <li>PolyAI (a startup at Singapore shipping conversational AI) shared three <a href="https://twitter.com/poly_ai/status/1154027323810861057/photo/1">interesting papers</a>. Their slides are also <a href="https://www.matthen.com/assets/pdf/Neural%2520Models%2520of%2520Response%2520Selection%2520for%2520Bootstrapping%2520Dialogue%2520Systems.pdf">interesting</a>.</li>
  <li>On a related note, Baidu has is doing impressive research and engineering on meeting transcription. They have a stack that does speech to text, translating the text as its spoken (a problem that
needed separate research as the text would be incomplete), detecting english phrases being spoken (code switching) and then NLP over the transcribed text.</li>
</ul>

<h2 id="translation">Translation</h2>

<ul>
  <li>Lots of new work on adapting translation models for low-resource languages.</li>
  <li>Unsupervised translation, Multi-lingual translation models are few areas of research.</li>
  <li>Unbabel a YC funded startup doing translation systems shared lots of interesting and important results.
<a href="https://www.aclweb.org/anthology/W18-2103">Slides from their talk</a>. This company employs a hybrid system where human translators do “post-edits” on machine translations.
And some of their system work in real-time.</li>
</ul>

<h2 id="contextual-search-using-neural-representations-at-scale">Contextual Search using Neural Representations at scale</h2>

<p>This <a href="https://arxiv.org/abs/1906.05807">paper</a> has demonstrated a system which does dense vector search on entire wikipedia for open domain QA.</p>

<p>Scaling search on neural vectors to do question answering on entire wikipedia on CPU - <a href="https://github.com/uwnlp/denspi">https://github.com/uwnlp/denspi</a></p>

<p>Demo - <a href="http://allgood.cs.washington.edu:15001/">http://allgood.cs.washington.edu:15001/</a></p>

