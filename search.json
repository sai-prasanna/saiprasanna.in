[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dao of Sai",
    "section": "",
    "text": "Learn how to efficiently batch large datasets with varied sequence length for training using infinibatch.\n\n\n\n\n\n\nSep 13, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMar 25, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMar 2, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 10, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDec 8, 2019\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 7, 2019\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 4, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJun 5, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJun 4, 2018\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMar 20, 2017\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMar 17, 2017\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJan 29, 2017\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJan 6, 2017\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJul 10, 2016\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJul 10, 2016\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 17, 2016\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 2, 2016\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMar 23, 2016\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I have previously worked on applied research in NLP to build solutions for agara.ai and Zoho. I also have research experience, publishing two papers to top NLP conferences (EMNLP) and workshops (SEMEVAL)."
  },
  {
    "objectID": "posts/dependency-injection-what-why-and-how/index.html",
    "href": "posts/dependency-injection-what-why-and-how/index.html",
    "title": "Dependency Injection - What, Why and How?",
    "section": "",
    "text": "We are going to explore dependency injection with emphasis on swift iOS development. But the concept applies to most object oriented languages. We will also see some practical considerations on applying DI in iOS environment. This article is result of my deep dive into implementing DI, and learning about various practical, theoretical aspects of it."
  },
  {
    "objectID": "posts/dependency-injection-what-why-and-how/index.html#scenario---koala-koder",
    "href": "posts/dependency-injection-what-why-and-how/index.html#scenario---koala-koder",
    "title": "Dependency Injection - What, Why and How?",
    "section": "Scenario - Koala Koder!",
    "text": "Scenario - Koala Koder!\nSay you have an app with a user login. A user model struct/class encapsulates the logged in user data.\nSuppose you are a Koala Koder( a programmer who is as lazy as a koala bear). You perhaps made a solution which is quick and dirty. Store the user model inside NSUserDefaults, and fetch it via properties. And as we all know how apple loves its singleton classes, we follow them making UserModel a singleton.\nclass UserModel {\n\n    static let sharedInstance = UserModel()\n\n    var name: String {\n        get {\n            return NSUserDefaults.standard.string(forKey: \"userName\")  ?? \"\"\n        }\n        set {\n            NSUserDefaults.standard.set(newValue, forKey:\"userName\")\n        }\n    }\n\n    func greet() -> String {\n        return \"\\(name), Vannakam :)\"\n    }\n}\nWe write our app with this usermodel in mind, and UserModel.sharedInstance is everywhere in our app.\n\n\n\nKoala lazing off"
  },
  {
    "objectID": "posts/dependency-injection-what-why-and-how/index.html#problems-problems-everywhere",
    "href": "posts/dependency-injection-what-why-and-how/index.html#problems-problems-everywhere",
    "title": "Dependency Injection - What, Why and How?",
    "section": "Problems, problems everywhere…",
    "text": "Problems, problems everywhere…\n\n1. Unit Tester Vader strikes!\nA senior developer suddenly turns to the dark side, and starts ranting about unit testing. He/She will not let apps which are not unit tested pass the code review.\n\n\n\nMeme: Darth Vader says “I find your lack of unit testing disturbing”\n\n\n\n\n2. A wild New Use case appears!\nAnd if that isn’t enough a new use case should be supported. Our app should now support multiple users.\n\n\n\nMeme: Back to future - Dr Brown says “New Usecase in no time? I’ve an extra flux capacitor”\n\n\n\n\n3. “Lets move to <insert any serialization library here>”\nNow we also reached a point where userdefaults didn’t scale and wish to migrate to new data serialization method. Now even our getters and setters inside the UserProfile is not safe."
  },
  {
    "objectID": "posts/dependency-injection-what-why-and-how/index.html#why-are-there-problems",
    "href": "posts/dependency-injection-what-why-and-how/index.html#why-are-there-problems",
    "title": "Dependency Injection - What, Why and How?",
    "section": "Why are there problems?",
    "text": "Why are there problems?\nSo we find ourselves in deep trouble. Lets analyze why so.\n\n1. Singletons are hard to test\nNow if you want to unit test a viewcontroller that uses this singleton.\n\nclass UserProfile: UIViewController {\n\n    func viewDidLoad() {\n        greetingLabel.text = UserModel.sharedInstance.greet()\n        //...\n    }\n}\nIn unit testing you create a UserProfile object, call viewDidLoad or other methods manually. Now you have to verify whether UserModel.sharedInstance.greet() was called.\nSince UserModel.sharedInstance is immutable, either we can’t replace it with our mock class extending UserModel, which overrides greetUser, and sets a flag which can be checked. So our testing coverage comes down.\n\n\n2. Instantiation inside our code constraints creates strong coupling, creating harmful constraints\nIn our scenario, by using a singleton, we tied our codebase to a single UserModel but now our app needs multiple user models. So in general, using singletons will make it hard to adapt to new use cases. What you thought as a singleton suddenly is not so single anymore.\nBut consider that we didn’t use singleton, but instantiated UserModel, by calling UserModel() wherever we needed it.\n\nclass UserProfile: UIViewController {\n\n    let userModel = UserModel()\n\n    func viewDidLoad() {\n        userNameLabel.text = userModel\n        //...\n    }\n}\nWe can’t support our new usecase of multiple users, without userprofile class knowing about multi-usermodel, or some other global allowing UserModel() to return the correct usermodel, these solutions are ugly hacks, which add complexity by either giving too much knowledge to classes or using globals and forgoing object oriented encapsulation.\n\n\n3. Concrete Type usage creates strong coupling\nConsider the UserProfile, it uses NSUserDefaults, now suppose we move to coredata to save our data, we are again in trouble because of using singleton inside. Our UserProfile rather needed only just a way to serialize some data, it didn’t need to know about WHAT we use for serialization. This is the key insight to keep in mind when thinking about dependency injection."
  },
  {
    "objectID": "posts/dependency-injection-what-why-and-how/index.html#di-to-the-rescue",
    "href": "posts/dependency-injection-what-why-and-how/index.html#di-to-the-rescue",
    "title": "Dependency Injection - What, Why and How?",
    "section": "DI to the rescue",
    "text": "DI to the rescue\nDI helps to solve the variety of issues that we face above, with regard to Unit Testablity, Singletons and strong coupling we face above."
  },
  {
    "objectID": "posts/dependency-injection-what-why-and-how/index.html#constructor-injection",
    "href": "posts/dependency-injection-what-why-and-how/index.html#constructor-injection",
    "title": "Dependency Injection - What, Why and How?",
    "section": "Constructor Injection",
    "text": "Constructor Injection\nSo we will be injecting a serializer into UserModel via constructor.\nMove dependency to constructor, and if possible make it a interface/protocol type instead of concrete class/struct\nSo we remove singleton access to userdefaults. And rather pass a serializer protocol which has methods we require for serialization to constructor.\n\nprotocol Serializing {\n    func string(forKey defaultName: String) -> String?\n    func set(_ value: Any?, forKey defaultName: String)\n}\n\nclass UserModel {\n    static let serializer: Serializing\n\n    init(_ serializer: Serializing) {\n        self.serializer = serializer\n    }\n\n    var name: String {\n        get {\n            return serializer.string(forKey: \"userName\")  ?? \"\"\n         }\n        set {\n            serializer.setObject(newValue, forKey:\"userName\")\n        }\n    }\n\n}\nSince we are Koala Koder, we just make the Serializing protocol methods to same ones in NSUserdefaults, so we can make NSUserDefaults conform easily by\nextension NSUserDefaults: Serializing {}\n.\nNow to use user defaults as our serializer, we do the following.\nUserModel(serializer: NSUserDefaults.standard)\nFor our fancy multiple user use case we can also do this. (Note: did this in a hurry, it may have edge cases, just providing it as a illustration)\n\nstruct MultiUserSerializer: Serializing {\n\n    let serializer: Serializing\n    init(_ serializer: Serializing) {\n        self.serializer = serializer\n    }\n\n    // So we fetch the current currentUserId and use that to prefix stored data\n    private func fetchCurrentUserId() -> String {\n        return serializer.string(forKey: \"CURRENT_USER_ID\") ?? \"0\"\n    }\n\n    func string(forKey defaultName: String) -> String? {\n        return serializer.string(forKey: fetchCurrentUserId() + \":\" + defaultName)\n    }\n\n    func set(_ value: Any?, forKey defaultName: String) {\n        return serializer.set(value, forKey defaultName: fetchCurrentUserId() + \":\" + defaultName)\n    }\n\n}\n\n//Instantiate using userdefaults, assume we implemented contextFetcher somewhere else\n\nlet multiUserSerializer = MultiUserSerializer(serializer: NSUserDefaults.standard)\n\nlet userModel = UserModel(serializer: multiUserSerializer)\nSo now userModel fetches data, and sets data to the current userID without even knowing about it. We could also use something other than NSUserDefaults.standard to serialize the data in top level.\nThe point is by removing replacing the concrete dependency NSUserDefaults.standard out of UserModel and swapping it to Serializing protocol we can now satisfy the new usecase of multi user modelling easily. This is the core idea behind of loose coupling.\nAlso we can now unit test User Model by just passing a Mock implementation of Serializing\n\nDependency injection works even with only concrete types.\nSometimes you don’t have the time, or are sure that concrete type used will not have to be changed. You can still DI the concrete type without bothering with protocol creation, just for the sake of it.\n\nFor example:\n\nclass UserProfile: UIViewController {\n\n    let userModel: UserModel\n\n    // This works only if you don't use storyboards\n    init(userModel: UserModel) {\n        self.userModel = userModel\n    }\n\n    func viewDidLoad() {\n        userNameLabel.text = userModel.name\n    }\n}\nWe move UserModel creation out of the controller, without creating any protocol for UserModel properties.\nWe still gain advantages of unit testability using ordinary mock objects, and also we are free to use our multi user serialized UserModel , hence making UserProfile support multi user model without changing any logic in user profile.\n(But if you have to do something to notify changes in data, that is seperate topic)\nlet multiUserSerializer = MultiUserSerializer(serializer: NSUserDefaults.standard)\nlet multiUserModel = UserModel(serializer: multiUserSerializer)\nlet userProfile = UserProfile(userModel: multiUserModel)\n\n\n\nMy constructor is a monster now :(\nA common problem which you will come across is, your UIViewController (if you don’t use storyboards) or any class where you do DI becomes a to big.\nclass UserProfile: UIViewController {\n\ninit(userModel: UserModel, apiService: APIService, x: XService, y:YService, z: ZService, ....) \nThis is a good thing, it points out clearly that your class is violating Single Responsibility rule . Single responsibility rule states that a class should have singe responsibility.\nWe can solve this by moving some of the current dependencies to a new class, and pass the new class as dependency to UserProfile.\nGuess what if we do this we properly we would be re-inventing design patterns l MVVM(Nodel View ViewModel) or MVP. Whew! DI just solved Huge ViewController problem!!.\n\nDI can easily help refactoring code to better design, in a gradual manner."
  },
  {
    "objectID": "posts/dependency-injection-what-why-and-how/index.html#property-injection",
    "href": "posts/dependency-injection-what-why-and-how/index.html#property-injection",
    "title": "Dependency Injection - What, Why and How?",
    "section": "Property Injection",
    "text": "Property Injection\nWe seemed to have solved all our above problems, why bother with more types of injections?\n\nIf you don’t control the creation of a object, your best bet is property injection. But prefer constructor injection if that’s not the case.\n\nSometimes you don’t create objects of classes, some messy framework does it. For example, if you use Storyboards, you can’t do stuff like let userProfileViewController = UserProfile(multiUserModel). You would have to refactor to something like\n\nclass UserProfile: UIViewController {\n\n    var userModel: UserModel!\n\n    func viewDidLoad() {\n        userNameLabel.text = userModel.name\n    }\n}\n\nlet multiUserSerializer = MultiUserSerializer(serializer: NSUserDefaults.standard)\nlet multiUserModel = UserModel(serializer: multiUserSerializer)\n\nlet storyboard = UIStoryboard(name: \"main\", bundle: nil)\nlet userProfile = storyboard.instantiateViewController(withIdentifier: \"UserProfile\") as! UserProfile\nuserProfile.userModel = multiUserModel\nBy using implicitly unwrapped Optional property, and setting it from outside, we achive the same effects of constructor injection. But is not perfect, as userModel property can be mutated from outside. Butthis is as good as it can get."
  },
  {
    "objectID": "posts/dependency-injection-what-why-and-how/index.html#method-injection",
    "href": "posts/dependency-injection-what-why-and-how/index.html#method-injection",
    "title": "Dependency Injection - What, Why and How?",
    "section": "Method Injection",
    "text": "Method Injection\nMethod injection is just replacing instantiating inside method by one of its parameters.\n// Before injection\n\nfunc someMethod() {\n    let x = X()\n    let y = x.something() + 10\n    return y\n}\n\n// After injection\n\nfunc someMethod(_ x: XService) {\n    let y = x.something() + 10\n    return y\n}"
  },
  {
    "objectID": "posts/dependency-injection-what-why-and-how/index.html#runtime-injection---factory-pattern",
    "href": "posts/dependency-injection-what-why-and-how/index.html#runtime-injection---factory-pattern",
    "title": "Dependency Injection - What, Why and How?",
    "section": "Runtime Injection - Factory pattern",
    "text": "Runtime Injection - Factory pattern\nSometimes you want to create a object of a particular class in runtime. But you want to use only protocol type(or a super type) instead of actual implementation (or subclass). Let’s say you need a networking service, which you set based on a user action, but as you are going to need it in runtime, you may think that you can’t inject it.\nBut what you can do is inject a factory Networking object or closure, that constructs it in runtime. This factory can fill the dependency of the concrete Networking class, so your class can be unaware of this.\n\nprotocol Networking {\n // Some methods ..\n}\n\nclass ProxyNetworking: Networking  {\n    init(a: A) {\n\n    }\n}\n\nclass NormalNetworking: Networking {\n    init(a: A) {\n\n    }\n}\n\n\n// Injection Via Factory Class\n\nclass NetworkingFactory {\n\n    let a: A\n    init(a: A) {\n        self.a = A\n    }\n\n    func create(_ withProxy: Bool) -> Networking {\n         if (withProxy) {\n            return ProxyNetworking(a: a)\n         } else {\n            return NormalNetworking(a: a)\n         }\n    }\n}\n\n\nclass Some :UIViewController {\n\n  // This has to be injected via property injection\n    var networkingFactory: NetworkingFactory! \n\n    var networking: Networking?\n\n    @IBOutlet weak var proxySwitch: UISwitch!\n\n    @IBAction func userTappedSubmit(sender: UIButton) {\n        networking = networkingFactory.create(withProxy: proxySwitch.isOn)\n    }\n}\n\n\n// Injection using closure\n\nclass Some :UIViewController {\n\n    // This has to be injected via property injection\n    var networkingFactory: ((Bool) -> Networking)!\n\n    var networking: Networking?\n\n    @IBOutlet weak var proxySwitch: UISwitch!\n\n    @IBAction func userTappedSubmit(sender: UIButton) {\n        networking = networkingFactory(proxySwitch.isOn)\n    }\n}"
  },
  {
    "objectID": "posts/dependency-injection-what-why-and-how/index.html#dip-framework---swift",
    "href": "posts/dependency-injection-what-why-and-how/index.html#dip-framework---swift",
    "title": "Dependency Injection - What, Why and How?",
    "section": "Dip Framework - Swift",
    "text": "Dip Framework - Swift\nAmong the DI frameworks exiting now for swift, I recommend Dip. Dip has some nifty features to make your DI pain free.\n\nFeatures of Dip\n\nScopes. Dip supports 5 different scopes (or life cycle strategies): Unique, Shared, Singleton, EagerSingleton, WeakSingleton;\nAuto-wiring & Auto-injection. Dip can infer your components’ dependencies injected in constructor and automatically resolve them as well as dependencies injected with properties.\nResolving optionals. Dip is able to resolve constructor or property dependencies defined as optionals.\nType forwarding. You can register the same factory to resolve different types implemeted by a single class.\nCircular dependencies. Dip will be able to resolve circular dependencies if you will follow some simple rules;\nStoryboards integration. You can easily use Dip along with storyboards and Xibs without ever referencing container in your view controller’s code;\nNamed definitions. You can register different factories for the same protocol or type by registering them with tags;\nRuntime arguments. You can register factories that accept up to 6 runtime arguments (and extend it if you need);\nEasy configuration & Code generation. No complex containers hierarchy, no unneeded functionality. Tired of writing all registrations by hand? There is a cool code generator that will create them for you. The only thing you need is to annotate your code with some comments.\nWeakly typed components. Dip can resolve “weak” types when they are unknown at compile time.\nThread safety. Registering and resolving components is thread safe;\nHelpful error messages and configuration validation. You can validate your container configuration. If something can not be resolved at runtime Dip throws an error that completely describes the issue;\n\n\n\nAnnotations in Dip\nJava has good frameworks like Dagger, and Guice that use annotations to make the job even simpler compared to swift. Dip allows you to leave annotations of dependencies in comments and also generate code for DI from it. How cool is that?"
  },
  {
    "objectID": "posts/dependency-injection-what-why-and-how/index.html#poor-mans-di-in-swift",
    "href": "posts/dependency-injection-what-why-and-how/index.html#poor-mans-di-in-swift",
    "title": "Dependency Injection - What, Why and How?",
    "section": "Poor man’s DI in Swift",
    "text": "Poor man’s DI in Swift\nThough I recommend the framework approach, supposing you don’t want to use framework initially and still want the to do DI, Fear not. You can use Swift’s default parameters to do constructor/method injection, and just use variable properties for property injection. You can maintain a Seperate DI singleton and fill dependencies using that.\n\n\nclass UserModel {\n    static let serializer: Serializing\n\n    init(_ serializer: Serializing = PoormanDIContainer.instance.getSerializer()) { // Poor man DI\n        self.serializer = serializer\n    }\n\n    var name: String {\n        get {\n            return serializer.string(forKey: \"userName\")  ?? \"\"\n         }\n        set {\n            serializer.setObject(newValue, forKey:\"userName\")\n        }\n    }\n}\n\n\nclass PoorManDIContainer {\n    let instance = PoorManDIContainer() \n    func getSerializer() -> Serializing {\n        // If Serializer has some dependencies, it will again use default constructor to obtain it from PoorManDIContainer\n        return Serializer() \n    }\n}\n\nBut if you use the above method, beware of circular dependencies."
  },
  {
    "objectID": "posts/neural-module-networks/index.html",
    "href": "posts/neural-module-networks/index.html",
    "title": "Neural Module Networks",
    "section": "",
    "text": "Authors - Jacob Andreas, Marcus Rohrbach, Trevor Darrell, Dan Klein ## Key Idea Parse questions of visual QA into a description of compositions of functions. These functions are neural networks called Neural Modules. Execute the neural networks and reweigh the resulting label using question representation.  ## Task - Visual Question Answering Given a question like “What color is the coffee mug?” and an image we want to predict the answer.  ## Prior approaches\nEnd to End neural networks\nUse a CNN to vectorize the image and RNN to vectorize the question and use a feed forward network to classify the answer. This is a black box trying to answer in one shot.\nSemantic Parsing approach\nParse the question into logical expressions, image into logical representation of the world and use logic based reasoning to solve the problem. This is more compositional. ## Motivation Combine the representational capacity of neural nets and compositionality of symbolic approach.\n\n“Rather than thinking of question answering as a problem of learning a single function to map from questions and contexts to answers, it’s perhaps useful to think of it as a highly-multitask learning setting, where each problem instance is associated with a novel task, and the identity of that task is expressed only noisily in language.”\n\nSimple example - “Is this a truck?” - Needs single task to be performed, namely truck or not classification.\nCompositional example - “What is the object to the left of the tea pot?” - Needs one to find the teapot, detect object to its left, then classify the object. ## Architecture ### Neural Modules Identify set of modules that can be composed to solve all/most tasks. Modules can be thought of as a function parametrized by a neural network, with a type signature. Data Types - Image, Unnormalized attention map, labels      ### Strings -> Modules Parsing Use few rules on dependency parse of the question to convert it into a structured query. e.g. “Is there a circle next to a square?” -> is(circle, next-to(square)) Layout “All leaves become attend modules, all internal nodes become re-attend or combine modules dependent on their arity, and root nodes become measure modules for yes/no questions and classify modules for all other question types.” The queries could come from anywhere not just natural language question. As long as they can be converted to a layout in the end. ### Answering An RNN is used to process the question and predict a label directly without looking into the image. This is combined with the final label from the root node of the Neural Modules using geometric mean to get the final result. This is done for 2 reasons Syntactic Regularity/Prior When converting to structured query, certain syntactic elements are lost. For e.g. What is in the sky? and What are in the sky? both result in what(fly). But answer varies from kite to kites. Semantic Regularity/Prior Some answers are unreasonable just by inspecting the question. For example, What colour is the bear? eliminates all non-colour answers. ## Benchmarks They try this in vqa dataset - https://visualqa.org/ a huge dataset with natural images and questions with answers.  Since VQA doesn’t have many deep compositional questions, they use shapes a synthetically generated dataset.  ## Examples What colour is his tie?"
  },
  {
    "objectID": "posts/neural-open-information-extraction/index.html",
    "href": "posts/neural-open-information-extraction/index.html",
    "title": "Neural Open Information Extraction (ACL 2018)",
    "section": "",
    "text": "TLDR; Models Open information extraction as Sequence to Sequence problem using neural nets.\n\nWhat is Open information extraction?\nOpen Information Extraction aims to extract one or more (Entity 1, Relationship, Entity 2) tuples from sentences.\nExample\n\"Deep learning is a subfield of machine learning.\" \n                   |\n                   v\n(Deep learning, is a subfield of , machine learning) \nExisting methods use handcrafted rules written on syntatic parsers which have poor perfomance and suffer from cascading of errors.\nThis paper applies neural networks to get better accuracy and alleviate errors.\n\n\nHow is the problem modeled?\nSequence 2 Sequence task, where you take a source sentence as input and output the information tuple as sequence separated by special tokens (open arg1, arg2, arg3 and close arg1, arg2 arg3). Currently the model is trained for single tuple extraction.\n“Deep learning is a subfield of machine learning.” -> “ Deep learning   is a subfield of   machine learning ”\nThe paper uses a LSTM based Sequence to Sequence model with attention. The source and target vocabulary is same. If unknown word target is found, the model forms the target probability vector by placing attention on of source sequence as probability of the corresponding source words occurring.\n\n\nWhat are the benchmarks?\nEvaluating on a large benchmark dataset, this model gets 0.473 AUC (Area under curve) for Precision-Recall on top 5 predictions of this model (I think generated from beam search) has better performance , significantly higher than existing systems. Among existing systems OpenIE has the best score of 0.373 AUC .\nOne observation of authors is only 11 % of the predictions of Neural model matches with Open IE (rule based) model, but the performance is higher. This could be due to neural model generalizing on some of the patterns hard to capture by rules."
  },
  {
    "objectID": "posts/roam-research-software-for-building-a-second-brain/index.html",
    "href": "posts/roam-research-software-for-building-a-second-brain/index.html",
    "title": "Roam Research - Software for building a Second Brain",
    "section": "",
    "text": "Roam Research is a revolutionary note-taking/knowledge management software. It is designed with the idea that data structure for a second brain should be associative (graph) rather than a rigid hierarchy. It is meant for everyone who needs to manage their knowledge effectively. Their founder aims for the product to become more or less excel for knowledge management. Professor Balaji Srinivas introduced it to me. I have been using this for about a month and fell in love with its features. Canadian philosopher who predicted the web tells that “Medium is the message”. The properties of a medium in which communication occurs can impact society a lot. I believe that roam is one such tool that shifts the medium of note-taking in a groundbreaking manner. The features of Roam allow you to arrange for the serendipity of ideas, unexpected connections with your past, present and future selves. It is built on simple building blocks that come together (emergent property) to make the whole greater than the sum of its parts. Each building block of Roam might look simple if looked separately, but together they become very powerful. Note: It is still in beta with a pricing yet to be announced. But if you try it I think you will share this sentiment.  Here is the Roam white paper written by founders on a Public roam database about why they think Roam is revolutionary."
  },
  {
    "objectID": "posts/roam-research-software-for-building-a-second-brain/index.html#failure-of-files-inside-folders-way-of-organization.",
    "href": "posts/roam-research-software-for-building-a-second-brain/index.html#failure-of-files-inside-folders-way-of-organization.",
    "title": "Roam Research - Software for building a Second Brain",
    "section": "Failure of Files inside Folders way of organization.",
    "text": "Failure of Files inside Folders way of organization.\nMost note-taking software or even physical note-taking follow a file-folder system or a single hierarchy of bullets inside bullets (Outlining tools) for organizing notes. I will refer to this as “files inside folder” model going forward now. This system creates nested hierarchy of categories, sub-categories and so on. Inside which your notes are put.\n\nWhy this approach fails?\nYou might have started taking notes/journaling etc for a few days and abandon it after some time. The following are the main reasons I think this happens.\nFriction caused by a static hierarchy of the folder system\nEvery time you want to write a note you have to decide where to put it in the hierarchy. You have to ask yourselves which notebook/folder/file should I write this, for it to be useful?\nPoor Return in investment for good note-taking\nYou painstakingly take notes or journal your exercise regime or note down something. But it never surfaces automatically when you write something related. Most notes are passive and useless unless you look for them. Most notes are hidden uselessly in the hierarchy where you put it in.\n\n\nPractical Scenario where folder model fails\nSay you had a discussion with your friend about note-taking while sipping a coffee in a cafe. You talk about some personal stuff which say you want to put in your journal. S/he brings about about a new book which you find interesting and want to read. Also, you really like the coffee in the cafe and want to note it down in your list of the favourite coffee shops.\nNow how do you take notes in this case? In a rigid hierarchy, for this note to be useful you have to file it under multiple places “To read books”, “Daily Journal”, “Meets”, “Friend’s name” But that is highly impractical because of the effort/redundancy involved. You would have to copy paste same information in multiple places or manually create links in multiple pages. And if those pages don’t exist you would have to create them."
  },
  {
    "objectID": "posts/roam-research-software-for-building-a-second-brain/index.html#simple-tags-dont-solve-this-problem",
    "href": "posts/roam-research-software-for-building-a-second-brain/index.html#simple-tags-dont-solve-this-problem",
    "title": "Roam Research - Software for building a Second Brain",
    "section": "Simple Tags don’t solve this problem",
    "text": "Simple Tags don’t solve this problem\nMost note-taking software provide tagging to solve this problem. But this creates friction of adding tags to everything you write. And more importantly the tags are flat. i.e. They have no hierarchy between them. Tags solve searching for notes, but not disovery. This makes it problematic if you want to discover notes when you stumble through a specific context automatically."
  },
  {
    "objectID": "posts/roam-research-software-for-building-a-second-brain/index.html#we-think-in-a-associative-graphs-manner-not-as-a-rigid-hierarchy-trees",
    "href": "posts/roam-research-software-for-building-a-second-brain/index.html#we-think-in-a-associative-graphs-manner-not-as-a-rigid-hierarchy-trees",
    "title": "Roam Research - Software for building a Second Brain",
    "section": "We think in a associative (Graphs) manner not as a rigid hierarchy (Trees)",
    "text": "We think in a associative (Graphs) manner not as a rigid hierarchy (Trees)\nEvery thought/idea in our brain has a bunch of associations. Associations like people who introduced us to the idea, what we want to do with it, associations with books we read about it, tasks we completed based on it, tasks we want to do, date in which we did it etc. So this forms a “graph” (in computer science) where every idea is a node and is linked to many other ideas. When you think of an idea naturally you get reminded of the stuff it is associated with. But the problem is we forget stuff, which is the reason why we are doing note-taking in the first place. What if a note-taking software allows you to mirror how the brain works?"
  },
  {
    "objectID": "posts/roam-research-software-for-building-a-second-brain/index.html#basic-layout",
    "href": "posts/roam-research-software-for-building-a-second-brain/index.html#basic-layout",
    "title": "Roam Research - Software for building a Second Brain",
    "section": "Basic Layout",
    "text": "Basic Layout\n Roam’s page layout is like any other “outlining” tool. ie It has bullets which can be nested within each other infinitely and the bullets can be collapsed. There is no folder system. All notes are be seen from “All Pages” view in left side-bar, but it is rarely needed because of way roam’s navigation is organized. Roam’s home page is the Daily notes page where you can see pages titled by dates. This is the basic dumping ground for all your quick note-taking and journaling, daily tasks, habit tracking for the day."
  },
  {
    "objectID": "posts/roam-research-software-for-building-a-second-brain/index.html#friction-free-link-creation",
    "href": "posts/roam-research-software-for-building-a-second-brain/index.html#friction-free-link-creation",
    "title": "Roam Research - Software for building a Second Brain",
    "section": "Friction Free Link Creation",
    "text": "Friction Free Link Creation\n Basic way roam allows linking between pages is typing the note title inside two square brackets “[[]]” which includes autocomplete/search to all notes page titles. One Important thing here is if the note page doesn’t exist a new page gets create. This would seem weird when coming from other applications. But it serves a big purpose you will see next."
  },
  {
    "objectID": "posts/roam-research-software-for-building-a-second-brain/index.html#bi-directional-linking",
    "href": "posts/roam-research-software-for-building-a-second-brain/index.html#bi-directional-linking",
    "title": "Roam Research - Software for building a Second Brain",
    "section": "Bi-directional Linking",
    "text": "Bi-directional Linking\n This is the key feature of Roam. When a page is linked to, when you visit the page you can see all the places where it has been referred.\nSay you write “I was reading this cool article on [[Deep Learning]]”, The Deep Learning page will have a back link to all the places it has been mentioned. So every page becomes akin to a tag, but associations between them form a dynamic/organic hierarchy. > “Every page is a tag, and every tag is a page” - Nat Elison’s blog on Roam"
  },
  {
    "objectID": "posts/roam-research-software-for-building-a-second-brain/index.html#un-linked-references",
    "href": "posts/roam-research-software-for-building-a-second-brain/index.html#un-linked-references",
    "title": "Roam Research - Software for building a Second Brain",
    "section": "Un-linked References",
    "text": "Un-linked References\n If you have already mentioned a topic in lots of notes, but didn’t create a page for it and link them. Roam has got your back with its super cool un-linked references. When you create a new page, you can easily bulk link every other page that has mentions of the current page."
  },
  {
    "objectID": "posts/roam-research-software-for-building-a-second-brain/index.html#ability-to-refer-or-embed-any-blockbullet-anywhere",
    "href": "posts/roam-research-software-for-building-a-second-brain/index.html#ability-to-refer-or-embed-any-blockbullet-anywhere",
    "title": "Roam Research - Software for building a Second Brain",
    "section": "Ability to refer or embed any block/bullet anywhere",
    "text": "Ability to refer or embed any block/bullet anywhere\n You can link to or embed any block written anywhere in roam notes. Roam prompts a search/autocomplete to any block you have written in all the pages. How awesome is that? To create a block reference two open Parentheses (( Or type /Block Reference. You can also embed the entire block using Block embed. Type /Block Embed When embedding or linking to a block you can see the places it has been used by clicking on the number which appears at top right side of a block."
  },
  {
    "objectID": "posts/roam-research-software-for-building-a-second-brain/index.html#all-back-links-and-block-embeds-are-editable",
    "href": "posts/roam-research-software-for-building-a-second-brain/index.html#all-back-links-and-block-embeds-are-editable",
    "title": "Roam Research - Software for building a Second Brain",
    "section": "All back-links and block embeds are editable",
    "text": "All back-links and block embeds are editable\n The coolest part of roam is all blocks (bullets) displayed by back-links and block embeds are editable. You can edit embedded notes and back-links with no duplication. So you easily remix (refactor?) notes by creating a new note with just embeds from multiple other notes in other places."
  },
  {
    "objectID": "posts/roam-research-software-for-building-a-second-brain/index.html#navigating-with-full-text-search",
    "href": "posts/roam-research-software-for-building-a-second-brain/index.html#navigating-with-full-text-search",
    "title": "Roam Research - Software for building a Second Brain",
    "section": "Navigating with Full-text Search",
    "text": "Navigating with Full-text Search\n Roam provides full text search of all the blocks and titles. You can create a brand new page which is not linked to any other page directly from search."
  },
  {
    "objectID": "posts/roam-research-software-for-building-a-second-brain/index.html#graph-view",
    "href": "posts/roam-research-software-for-building-a-second-brain/index.html#graph-view",
    "title": "Roam Research - Software for building a Second Brain",
    "section": "Graph View",
    "text": "Graph View\n Since every note is basically a node in graph, roam easily allows a bird’s eye of your entire graph in the “Graph Overview” page. A more helpful feature is ability to view what nodes current page has connections to and navigate visually."
  },
  {
    "objectID": "posts/roam-research-software-for-building-a-second-brain/index.html#side-bar",
    "href": "posts/roam-research-software-for-building-a-second-brain/index.html#side-bar",
    "title": "Roam Research - Software for building a Second Brain",
    "section": "Side Bar",
    "text": "Side Bar\n Roam sidebar allows you to open multiple notes at a time. This is really useful when you want to aggregate knowledge across notes."
  },
  {
    "objectID": "posts/roam-research-software-for-building-a-second-brain/index.html#filters-on-bi-directional-links",
    "href": "posts/roam-research-software-for-building-a-second-brain/index.html#filters-on-bi-directional-links",
    "title": "Roam Research - Software for building a Second Brain",
    "section": "Filters on Bi-directional Links",
    "text": "Filters on Bi-directional Links\n When you have too many bi-directional links in a page, you can filter them to include or exclude other links."
  },
  {
    "objectID": "posts/roam-research-software-for-building-a-second-brain/index.html#note-taking",
    "href": "posts/roam-research-software-for-building-a-second-brain/index.html#note-taking",
    "title": "Roam Research - Software for building a Second Brain",
    "section": "Note taking",
    "text": "Note taking\nIf you read a lot and want to retain the knowledge. Reading stuff and writing it in your own words will be a good way to test gaps in understanding. This article a effective way to take smart notes using roam. I found this summary of a book called “[[How to Take Smart Notes - Book]] really helpful."
  },
  {
    "objectID": "posts/roam-research-software-for-building-a-second-brain/index.html#journal",
    "href": "posts/roam-research-software-for-building-a-second-brain/index.html#journal",
    "title": "Roam Research - Software for building a Second Brain",
    "section": "Journal",
    "text": "Journal\nWriting a journal is a bread and (peanut) butter of roam. Daily notes encourages you to write daily at anytime. It feels really encouraging to journal in roam as unlike other apps because of the back-links. Anything recorded will automatically get associated with all the topics."
  },
  {
    "objectID": "posts/roam-research-software-for-building-a-second-brain/index.html#task-management",
    "href": "posts/roam-research-software-for-building-a-second-brain/index.html#task-management",
    "title": "Roam Research - Software for building a Second Brain",
    "section": "Task management",
    "text": "Task management\n Roam supports basic todos, you can use links to link tasks to the date it has to be done using date-picker. The key advantage here is your project management tasks can easily be linked to the meeting notes, research notes, and journal etc. Getting things done (GTD) is a popular method to manage tasks. It is very easy to implement that in roam with the aid of back-links. You can read how to adopt GTD in roam here."
  },
  {
    "objectID": "posts/roam-research-software-for-building-a-second-brain/index.html#bookmarks",
    "href": "posts/roam-research-software-for-building-a-second-brain/index.html#bookmarks",
    "title": "Roam Research - Software for building a Second Brain",
    "section": "Bookmarks",
    "text": "Bookmarks\n You can easily put links into roam with creating tags/links. Tags are same as “[[]]” links, just that the font is greyed out."
  },
  {
    "objectID": "posts/roam-research-software-for-building-a-second-brain/index.html#personal-crm",
    "href": "posts/roam-research-software-for-building-a-second-brain/index.html#personal-crm",
    "title": "Roam Research - Software for building a Second Brain",
    "section": "Personal CRM",
    "text": "Personal CRM\nPersonal crm is for maintaining a list of people, their contact, birthdays, how you met them or anything else you want to maintain about them. In roam you can easily create pages for people, and refer it in your daily notes. So when you go to the person’s page, you can see all the places h/she has been mentioned."
  },
  {
    "objectID": "posts/roam-research-software-for-building-a-second-brain/index.html#content-creation",
    "href": "posts/roam-research-software-for-building-a-second-brain/index.html#content-creation",
    "title": "Roam Research - Software for building a Second Brain",
    "section": "Content Creation",
    "text": "Content Creation\nFor writing new content, you can easily remix stuff which you wrote across different pages in a new page. So you will never have the feeling of starting at a blank page when you have done your research and taken notes on it."
  },
  {
    "objectID": "posts/roam-research-software-for-building-a-second-brain/index.html#query",
    "href": "posts/roam-research-software-for-building-a-second-brain/index.html#query",
    "title": "Roam Research - Software for building a Second Brain",
    "section": "Query",
    "text": "Query\n This is an advanced feature, where you can query on your graph to show blocks that satisfy boolean conditions. Like show me all the blocks with todos with high priority etc."
  },
  {
    "objectID": "posts/roam-research-software-for-building-a-second-brain/index.html#shortcuts",
    "href": "posts/roam-research-software-for-building-a-second-brain/index.html#shortcuts",
    "title": "Roam Research - Software for building a Second Brain",
    "section": "Shortcuts",
    "text": "Shortcuts\n This is useful to keep the most important projects"
  },
  {
    "objectID": "posts/roam-research-software-for-building-a-second-brain/index.html#tables-diagrams-kanban-boards",
    "href": "posts/roam-research-software-for-building-a-second-brain/index.html#tables-diagrams-kanban-boards",
    "title": "Roam Research - Software for building a Second Brain",
    "section": "Tables, Diagrams, Kanban boards",
    "text": "Tables, Diagrams, Kanban boards\nTables can be created using /Table command followed by nested bullets."
  },
  {
    "objectID": "posts/roam-research-software-for-building-a-second-brain/index.html#embedding-media",
    "href": "posts/roam-research-software-for-building-a-second-brain/index.html#embedding-media",
    "title": "Roam Research - Software for building a Second Brain",
    "section": "Embedding Media",
    "text": "Embedding Media\nEmbed tweet by just pasting a twitter link. Type backslash followed by image markdown to insert a markdown for image embed. /Image Markdown Alternatively images can be uploaded by pasting directly or with backslash command /Upload a image backslash command for it to be uploaded and the markdown inserted automatically. For youtube videos, use /Embed Youtube Video command. Interesting point here is all the embeds are markdown or markdown like plain text syntax. No proprietary garbage."
  },
  {
    "objectID": "posts/roam-research-software-for-building-a-second-brain/index.html#publishing-your-roam-notes",
    "href": "posts/roam-research-software-for-building-a-second-brain/index.html#publishing-your-roam-notes",
    "title": "Roam Research - Software for building a Second Brain",
    "section": "Publishing your Roam Notes",
    "text": "Publishing your Roam Notes\nYou can share a note by its URL to public as read-only or even allow public edits."
  },
  {
    "objectID": "posts/roam-research-software-for-building-a-second-brain/index.html#sharing-your-second-brain-to-form-a-hive-mind",
    "href": "posts/roam-research-software-for-building-a-second-brain/index.html#sharing-your-second-brain-to-form-a-hive-mind",
    "title": "Roam Research - Software for building a Second Brain",
    "section": "Sharing your second Brain to form a Hive mind",
    "text": "Sharing your second Brain to form a Hive mind\nCurrently you can share the entire roam database and collabrate with peers. More fine-grained controls of sharing parts of the graph are in the works."
  },
  {
    "objectID": "posts/roam-research-software-for-building-a-second-brain/index.html#works-offline-progressive-web-app",
    "href": "posts/roam-research-software-for-building-a-second-brain/index.html#works-offline-progressive-web-app",
    "title": "Roam Research - Software for building a Second Brain",
    "section": "Works offline (Progressive Web App)",
    "text": "Works offline (Progressive Web App)\nThe mobile apps are coming shortly, but the web app is designed so well that it can function offline."
  },
  {
    "objectID": "posts/roam-research-software-for-building-a-second-brain/index.html#exports-to-plain-text",
    "href": "posts/roam-research-software-for-building-a-second-brain/index.html#exports-to-plain-text",
    "title": "Roam Research - Software for building a Second Brain",
    "section": "Exports to plain text",
    "text": "Exports to plain text\nThis is important for anyone who cares about not getting locked out of your data. Since the product is in development better to take regular backups if you plan to use it."
  },
  {
    "objectID": "posts/ACL-2019-Conference-Summary/index.html",
    "href": "posts/ACL-2019-Conference-Summary/index.html",
    "title": "ACL 2019 Conference Summary",
    "section": "",
    "text": "My colleague Ananda and I attended ACL 2019 conference at the enchanting city of Florence. All the accepted papers can be accessed here. Here’s the summary of interesting trends and also specific research work that caught my eye at the conference. A note of thanks to my employer at Zoho for sponsoring us to attend.\nI wrote this summary an many months ago and forgot posting it. Better late than never I guess."
  },
  {
    "objectID": "posts/ACL-2019-Conference-Summary/index.html#grammatical-error-correction",
    "href": "posts/ACL-2019-Conference-Summary/index.html#grammatical-error-correction",
    "title": "ACL 2019 Conference Summary",
    "section": "Grammatical Error Correction",
    "text": "Grammatical Error Correction\n\nAmong the ACL workshops, Building Educational Applications (BEA) Workshop had a Grammar Error Correction competition.\n\nThe system description papers for this competition were presented as posters in the conference.\n\nThree tracks were present in the competition. Restricted track - Only organizer provided human labelled parallel (error and corrected sentence pairs) data can be used. (No restriction on synthetic data) Unrestricted track - Any data including private data can be used. Low Resource track - No human labelled data can be used.\nInterestingly, the winning team (Edinburgh + Microsoft)’s submission for Track 1 also beat Track 2 without using additional restricted data.\nSynthetic data generated by corrupting good grammatical sentences from news, books and wikipedia are the techniques used overall by top performing teams."
  },
  {
    "objectID": "posts/ACL-2019-Conference-Summary/index.html#multi-lingual-models",
    "href": "posts/ACL-2019-Conference-Summary/index.html#multi-lingual-models",
    "title": "ACL 2019 Conference Summary",
    "section": "Multi-Lingual Models",
    "text": "Multi-Lingual Models\nMultiLingual models is a hot area of research now. Earlier results where using single model to perform tasks on multiple languages has shown promising results.\n\nLots of papers on multi-lingual shared models were presented.\nPaper - Choosing Transfer Languages for Cross-Lingual Learning"
  },
  {
    "objectID": "posts/ACL-2019-Conference-Summary/index.html#rise-of-automated-metrics",
    "href": "posts/ACL-2019-Conference-Summary/index.html#rise-of-automated-metrics",
    "title": "ACL 2019 Conference Summary",
    "section": "Rise of Automated Metrics",
    "text": "Rise of Automated Metrics\nUntil recently, we compare model outputs with human written sentences for translation, summarization etc. This can artificially penalize models that generate sentences with equivalent meaning but not same words. There are couple of papers that train models to score quality of the output. Then use these model scores as reward for reinforcement learning. (FYI reinforcement learning is only used for fine tuning, none of the seq2seq models can be trained from scratch using it)\n\nPaper - This Email Could Save Your Life: Introducing the Task of Email Subject Line Generation This paper uses automated score instead of typical NGram match (ROUGE) score for summarization task.\nPaper - Beyond BLEU:Training Neural Machine Translation with Semantic Similarity\nPaper - Sentence Mover’s Similarity: Automatic Evaluation for Multi-Sentence Texts"
  },
  {
    "objectID": "posts/ACL-2019-Conference-Summary/index.html#statistical-evaluation",
    "href": "posts/ACL-2019-Conference-Summary/index.html#statistical-evaluation",
    "title": "ACL 2019 Conference Summary",
    "section": "Statistical Evaluation",
    "text": "Statistical Evaluation\nIf we have two architectures and couple of datasets, how to say empirically one is better than the other? Few questions are how to compare two models on the same dataset, across multiple datasets, across various hyperparameter configurations. Problems in applying frequentist tests on the metrics such as accuracy, f1-score etc are that assumptions such as Independent and Identically distributed (IID) cannot be made for deep learning datasets. So we cannot assume that the score the model gets in one dataset is “independent” of the score on another dataset. Statistical tests that don’t assume underlying distribution are needed. Recent statistical methods/tests to do so are being developed and some were presented at the conference.\n\nPaper - Deep Dominance - How to Properly Compare Deep Neural Models\nPaper - Bayes Test of Precision, Recall, and F1 Measure for Comparison of Two Natural Language Processing Models"
  },
  {
    "objectID": "posts/ACL-2019-Conference-Summary/index.html#bayesian-methods",
    "href": "posts/ACL-2019-Conference-Summary/index.html#bayesian-methods",
    "title": "ACL 2019 Conference Summary",
    "section": "Bayesian Methods",
    "text": "Bayesian Methods\n\nAttended a very detailed tutorial on it. The presenter has summarized the evolution of research in this area and the current papers. Here’s link to the detailed slides for fellow Bayesians."
  },
  {
    "objectID": "posts/ACL-2019-Conference-Summary/index.html#analyzing-neural-nets-and-interpretability",
    "href": "posts/ACL-2019-Conference-Summary/index.html#analyzing-neural-nets-and-interpretability",
    "title": "ACL 2019 Conference Summary",
    "section": "Analyzing Neural Nets and Interpretability",
    "text": "Analyzing Neural Nets and Interpretability\nThere is an entire sub-fields of research into analyzing and interpreting neural networks.\n\nBERTology\n“BERT-ology” papers that explore what linguistic structures do pre-trained models like BERT learn.\n\nPaper - What Does BERT Look at? An Analysis of BERT’s Attention\n\n\n\nBlackBoxNLP Workshop\nAn entire workshop devoted for analyzing what Neural Networks learn.\n\nPaper - On the Realization of Compositionality in Neural Networks Interesting paper studying what is required for neural models to compose two very trivial functions.\nPaper - GEval: Tool for Debugging NLP Datasets and Models\n\n\n\nFormal Languages Workshop\nAn entire small workshop devoted to finding what Formal Languages (Finite state Automata, etc) neural networks can learn. e.g. Can we reduce a RNN to Weighted Finite State Machine (which is far more interpretable, amenable to theory etc). Although this area sounds exciting to me, I was unable to attend it as I was in an another workshop. Slides from talk of Noah Smith’s talk on Rational Recurrences at this workshop.\n\n\nNeuroscience and NLP\nNeuroscience labs have started to use deep learning. An interesting conjunction of research in NLP and neuroscience research in correlating ANN representations with brain signals was presented.\n\nPaper - Relating Simple Sentence Representations in Deep Neural Networks and the Brain The researchers try to find relationship between deep learning language representations and brain signals. Paper of interest is where they predict neural brain patterns using pre-trained ANN models like BERT.\n\n\n\nLanguage Emergence in Multi-Agent systems\nIn this frontier, people try train models to solve some task by communicating symbols. Researchers analyze the properties of language used by the agents to solve the task and how it compares with properties of human language.\n\nPaper - Word-order Biases in Deep-agent Emergent Communication"
  },
  {
    "objectID": "posts/ACL-2019-Conference-Summary/index.html#conversational-ai",
    "href": "posts/ACL-2019-Conference-Summary/index.html#conversational-ai",
    "title": "ACL 2019 Conference Summary",
    "section": "Conversational AI",
    "text": "Conversational AI\n\nNeural Models for selecting conversation from past history, detecting intent and slot fitting are all increasingly being deployed by companies.\nPolyAI (a startup at Singapore shipping conversational AI) shared three interesting papers. Their slides are also interesting.\nOn a related note, Baidu has is doing impressive research and engineering on meeting transcription. They have a stack that does speech to text, translating the text as its spoken (a problem that needed separate research as the text would be incomplete), detecting english phrases being spoken (code switching) and then NLP over the transcribed text."
  },
  {
    "objectID": "posts/ACL-2019-Conference-Summary/index.html#translation",
    "href": "posts/ACL-2019-Conference-Summary/index.html#translation",
    "title": "ACL 2019 Conference Summary",
    "section": "Translation",
    "text": "Translation\n\nLots of new work on adapting translation models for low-resource languages.\nUnsupervised translation, Multi-lingual translation models are few areas of research.\nUnbabel a YC funded startup doing translation systems shared lots of interesting and important results. Slides from their talk. This company employs a hybrid system where human translators do “post-edits” on machine translations. And some of their system work in real-time."
  },
  {
    "objectID": "posts/ACL-2019-Conference-Summary/index.html#contextual-search-using-neural-representations-at-scale",
    "href": "posts/ACL-2019-Conference-Summary/index.html#contextual-search-using-neural-representations-at-scale",
    "title": "ACL 2019 Conference Summary",
    "section": "Contextual Search using Neural Representations at scale",
    "text": "Contextual Search using Neural Representations at scale\nThis paper has demonstrated a system which does dense vector search on entire wikipedia for open domain QA.\nScaling search on neural vectors to do question answering on entire wikipedia on CPU - https://github.com/uwnlp/denspi\nDemo - http://allgood.cs.washington.edu:15001/"
  },
  {
    "objectID": "posts/open-for-collaboration-closed-for-disturbance-rethinking-open-office-layout/index.html",
    "href": "posts/open-for-collaboration-closed-for-disturbance-rethinking-open-office-layout/index.html",
    "title": "Open for Collaboration, Closed for Disturbance",
    "section": "",
    "text": "But after few months we moved to new workspace, things changed to the worse. It was the dreaded Complete open office plan, our building consists of floors with more than 200 people in each, and there is no separation between rows of tables. It is a complete nightmare, taking the good design to extreme and making it completely hard to bear.\nThis made me think about O of S.O.L.I.D principles of software design, which suggest that a good design should be open for extension and closed for modification.I think this applies allegorically to office layout plan.\nSome companies have moved to ditching offices completely, but there are many still not taking such a drastic step. So the principle which I arrive on workplace design, Open for Collaboration and Closed for Disturbance. A good office design should make it accessible for people to collaborate and avoid creating unnecessary old age hierarchy bullshit that came in form of corner office perks. At the same time it should provide the much needed c isolation to get shit done. In general the design should at least have some barrier between people.It wouldn’t hurt to provide some “Deep think rooms” which can function as isolation chambers which can be opposite of meeting rooms.\nMany studies are pointing to decreased level of productivity in open office plans. It is common sense that deep work requires some degree of isolation, which completely open spaces hardly provides. Every interruption completely derails the thought process which were happening till then, productivity.\nLets look at some facts,studies,opinions which point out to the decrease in productivity, happiness, and involvement in complete open office:\n\nhttp://blog.ninlabs.com/2013/01/programmer-interrupted/\n\n\nBased on a analysis of 10,000 programming sessions recorded from 86 programmers using Eclipse and Visual Studio and a survey of 414 programmers (Parnin:10), we found: A programmer takes between 10–15 minutes to start editing code after resuming work from an interruption. When interrupted during an edit of a method, only 10% of times did a programmer resume work in less than a minute. A programmer is likely to get just one uninterrupted 2-hour session in a day We also looked at some of the ways programmers coped with interruption: Most sessions programmers navigated to several locations to rebuild context before resuming an edit. Programmers insert intentional compile errors to force a “roadblock” reminder. A source diff is seen as a last resort way to recover state but can be cumbersome to review\n\n\nhttp://www.forbes.com/sites/davidburkus/2016/06/21/why-your-open-office-workspace-doesnt-work/\n\n\nThus while noise was a problem, the greater noise level didn’t appear to be from all of the collective collaboration buzzing around the open room. The researchers then took their analysis one step further, using regression to calculate how important each dimension was to employees’ overall satisfaction. One of the dimensions most strongly related to overall satisfaction was ease of interaction, despite the fact that it was judged to be no better or worse in open office plans than in private offices. In other words, the desire for more collaboration among employees was shared by all, but those in open office plans may not have found it to be worth all of the stress and distraction from the bombardment of noise.\n\n\nhttp://www.newyorker.com/business/currency/the-open-office-trap\n\n\nBut the most problematic aspect of the open office may be physical rather than psychological: simple noise. In laboratory settings, noise has been repeatedly tiedto reduced cognitive performance. The psychologist Nick Perham, who studies the effect of sound on how we think, has found that office commotion impairsworkers’ ability to recall information, and even to do basic arithmetic. Listening to music to block out the office intrusion doesn’t help: even that, Perham found, impairs our mental acuity. Exposure to noise in an office may also take a toll on the health of employees. In a study by the Cornell University psychologists Gary Evans and Dana Johnson, clerical workers who were exposed to open-office noise for three hours had increased levels of epinephrine — a hormone that we often call adrenaline, associated with the so-called fight-or-flight response. What’s more, Evans and Johnson discovered that people in noisy environments made fewer ergonomic adjustments than they would in private, causing increased physical strain. The subjects subsequently attempted to solve fewer puzzles than they had after working in a quiet environment; in other words, they became less motivated and less creative.\n\n\nhttp://www.inc.com/geoffrey-james/why-your-company-will-benefit-from-getting-rid-of-open-office-spaces-first-90.html\n\n\nThey decrease productivity. Contrary to popular belief, open offices don’t increase collaboration or make people more productive. An Exeter University study showed they create a 32 percent drop in “workers’ well-being” and 15 percent reduction in productivity. .They create time-consuming distractions.Office workers lose an average of 86 minutes per day due to distractions associated with open-plan offices. As a result, many employees are “unmotivated, unproductive, and overly stressed,” according to the study funded by Steelcase. They make employees sick.A study at Queensland University of Technology’s Institute of Health and Biomedical Innovation found that working in environments without offices “caus[es] high levels of stress, conflict, high blood pressure, and a high staff turnover.” This comic describes it in a funny way on what is the cost of interruptions on programmers. This can apply to any field that involves some thinking to be done at work.\n\n\n\n\nWhy Programmers shouldn’t be interrupted? Comic strip"
  },
  {
    "objectID": "posts/gods-own-programming-language/index.html",
    "href": "posts/gods-own-programming-language/index.html",
    "title": "God’s own Programming Language",
    "section": "",
    "text": "Found this interesting blog post explores why many programmers hold a high regard for an ancient programming language which you might not have heard about or use daily.\n{: style=“text-align: center;”}\nPoem from twobithistory.org {: style=“color:gray; font-size: 80%; text-align: center;”}\nXKCD Comics {: style=“color:gray; font-size: 80%; text-align: center;”}\nAlso a good read are the posts on LISP by Paul Graham of YCombinator/HackerNews fame."
  },
  {
    "objectID": "posts/gods-own-programming-language/index.html#how-to-attain-nirvana-with-the-gods-own-language",
    "href": "posts/gods-own-programming-language/index.html#how-to-attain-nirvana-with-the-gods-own-language",
    "title": "God’s own Programming Language",
    "section": "How to attain Nirvana with the God’s own language?",
    "text": "How to attain Nirvana with the God’s own language?\nTo attain programming nirvana - Start reading the SICP Book.\nSICP (Structure and Interpretation of Computer Programs) is a introduction to computer science book. It can change how you view even simple constructs we use for code (like loops, if, etc). A book that will teach timeless concepts in programming which you would never come across easily. I started reading it ages ago, haven’t completed it , the first few chapters themselves were sufficiently mind blowing.\nOriginal book\nA modern interactive version Now you can run the examples of SICP book God’s own language in the browser with godforsaken javascript. And laugh morosely on the irony of running LISP in a half baked language (javascript) which was inspired from it.\nA Distilled version with illustrations - Smaller, condensed version.\n\nTo use an analogy, if SICP were about automobiles, it would be for the person who wants to know how cars work, how they are built, and how one might design fuel-efficient, safe, reliable vehicles for the 21st century. The people who hate SICP are the ones who just want to know how to drive their car on the highway, just like everyone else. - Peter Norvig on SICP\n\n\n(How to Write a (Lisp) Interpreter (in Python))\nFollow this guide to implement your own LISP Interpreter in an hundred lines of python. You might think, “Is he crazy to ask a language beginner to implement the interpreter before learning it?” Answer is while I am partly crazy LISP is not, it has the simplest structure of all programming languages. It’s just lists duh! (LiSt Processing)."
  },
  {
    "objectID": "posts/gods-own-programming-language/index.html#lisp-for-ai",
    "href": "posts/gods-own-programming-language/index.html#lisp-for-ai",
    "title": "God’s own Programming Language",
    "section": "LISP for AI",
    "text": "LISP for AI\nYou can practise LISP for Artificial Intelligence algorithms with Peter Norvig’s book on “Paradigms of Artificial Intelligence Programming”."
  },
  {
    "objectID": "posts/gods-own-programming-language/index.html#say-you-dont-want-nirvana-but-something-more-pragmatic",
    "href": "posts/gods-own-programming-language/index.html#say-you-dont-want-nirvana-but-something-more-pragmatic",
    "title": "God’s own Programming Language",
    "section": "Say you don’t want Nirvana, but something more pragmatic",
    "text": "Say you don’t want Nirvana, but something more pragmatic\nYou can learn Clojure to use God’s Language to do some real world wizardry. Clojure is a form of LISP which is modern, functional and runs on JVM. Brave Clojure is one of the best sources out there for Clojure. For front end development/nodejs there is clojure-script which compiles down to javascript.\n\nLearning Clojure is the best way you can improve as a programmer because it introduces you to powerful concepts implemented in a simple, cohesive, and practical language. You learn Clojure here. Therefore, Brave Clojure is your very best friend when it comes to programming.” And lo, the syllogism was born!\n\n XKCD Comics {: style=“color:gray; font-size: 80%; text-align: center;”}"
  },
  {
    "objectID": "posts/gods-own-programming-language/index.html#a-note-and-a-zen-koan",
    "href": "posts/gods-own-programming-language/index.html#a-note-and-a-zen-koan",
    "title": "God’s own Programming Language",
    "section": "A Note and a Zen Koan",
    "text": "A Note and a Zen Koan\nUse any language that solves your problem and learn about others which have different paradigms conceptually like LISP, etc when you find the time. It a enjoyable exercise if you are curious about digging deeply into what makes computers tick.\nAs Master Foo says says,\n\nMaster Foo once said to a visiting programmer: “There is more Unix-nature in one line of shell script than there is in ten thousand lines of C.”\nThe programmer, who was very proud of his mastery of C, said: “How can this be? C is the language in which the very kernel of Unix is implemented!”\nMaster Foo replied: “That is so. Nevertheless, there is more Unix-nature in one line of shell script than there is in ten thousand lines of C.”\nThe programmer grew distressed. “But through the C language we experience the enlightenment of the Patriarch Ritchie! We become as one with the operating system and the machine, reaping matchless performance!”\nMaster Foo replied: “All that you say is true. But there is still more Unix-nature in one line of shell script than there is in ten thousand lines of C.”\nThe programmer scoffed at Master Foo and rose to depart. But Master Foo nodded to his student Nubi, who wrote a line of shell script on a nearby whiteboard, and said: “Master programmer, consider this pipeline. Implemented in pure C, would it not span ten thousand lines?”\nThe programmer muttered through his beard, contemplating what Nubi had written. Finally he agreed that it was so.\n“And how many hours would you require to implement and debug that C program?” asked Nubi.\n“Many,” admitted the visiting programmer. “But only a fool would spend the time to do that when so many more worthy tasks await him.”\n“And who better understands the Unix-nature?” Master Foo asked. “Is it he who writes the ten thousand lines, or he who, perceiving the emptiness of the task, gains merit by not coding?”\nUpon hearing this, the programmer was enlightened.\n\nFor more funny hacker koans, visit here and here.\nI am currently learning emacs-lisp for I have become a convert/evangelizer of the Church of Emacs on a Starship called Spacemacs. But that’s a post/sermon (;P) for another time."
  },
  {
    "objectID": "posts/function-currying-and-composition/index.html",
    "href": "posts/function-currying-and-composition/index.html",
    "title": "Function Currying, Composition in Redux Middleware",
    "section": "",
    "text": "I started learning about react js, and eventually ended up learning about Redux. I would assume you to have basic knowledge about redux , if not read about it here. It is very well documented, and the whole api is small, If you are feeling adventurous read the redux source code, it is a few hundred lines.\nRedux seems to be a more elegant implementation of Flux methodology without some components of flux like multiple stores communication via a dispatcher which is made redundant in redux pattern.\nSo as I was playing with redux by implementing a small application mentioned in this tutorial , I came across something called middlewares. These middleware intercept the actions to store and do some extra processing on it. I couldn’t wrap my head around how a middleware for handling Promises mentioned in that post worked.\n{% highlight js %} export default function promiseMiddleware() { return next => action => { const { promise, type, …rest } = action;\nif (!promise) return next(action);\n\nconst SUCCESS = type;\nconst REQUEST = type + '_REQUEST';\nconst FAILURE = type + '_FAILURE';\nnext({ ...rest, type: REQUEST });\nreturn promise\n  .then(res => {\n    next({ ...rest, res, type: SUCCESS });\n\n    return true;\n  })\n  .catch(error => {\n    next({ ...rest, error, type: FAILURE });\n\n    // Another benefit is being able to log all failures here\n    console.log(error);\n  return false;\n  });\n}; } {% endhighlight %}\nIt seemed to use functional magic which somehow allowed waiting for a promise to resolve and then dispatch the action or call next middleware. S o I was wondering how that worked and came across this excellent post titled “Understanding Redux Middleware”. I encourage you to check it out here if you haven’t already. This post is meant to clarify some things in it a bit further. From that article I came to understand that middleware functions got composed in the following fashion\nMiddleware1(Middleware2(…))\nThen by chance I was reading through documentation of redux-logger middleware which said that for it to log state , actions properly from it has to be passed as parameter after any asynchronous middleware. So from what I read in redux source for applyMiddleware I gathered that logger middleware will be a parameter for Async middleware for it to log correctly, Assume we do this , then\napplyMiddleware(PromiseMiddleware, LoggerMiddleware)\nwill compose them in this order\nPromiseMiddleware(LoggerMiddleware)\nThis seemed to make less sense as I wrongly thought that promise middleware will wait for LoggerMiddeware to process and then execute.\nBut after meditating on the redux source code ,creating curried functions and composing them , I got it. The actual execution order of middleware when dispatch action occurs, starts from PromiseMiddleware, which resolves the promise and only then calls the LoggerMiddleware.\nThis is because LoggerMiddleware is a curried function , so it doesn’t return any value and it is simply a function that is passed to Promisemiddleware. The Promisemiddleware takes up the action, resolves it, and inside “then” of promise calls the LoggerMiddleware using\nnext({ ...rest, res, type: SUCCESS });\nand hence passes the action to LoggerMiddleware Function. The PromiseMiddleware can refer to the next function inside it using the properly named param “next”. Now lets make some curried functions with asynchronous operations and uncurry them with parameters . Type these in js console We are first creating a curried function that takes another function as input and also some params.\n{% highlight js %} var asyncFunction = nextFunction => params => window.setTimeout( function() { alert(“In Async Callback”); nextFunction(params); }, 1000); {% endhighlight %}\nThis is similar to the promise middleware. asyncFunction takes in parameter a function called nextFunction. From the signature of nextFunction(params) used inside , we can get it that it simply takes a object. Now let us create a function to pass into async\n{% highlight js %} var alertFunction = parameters => alert(parameter); {% endhighlight %}\nalertFunction simply alerts the parameter passed to it. COMPOSING Alert and asyncFunction manually, and calling the result of composition\n{% highlight js %} var composedFunction = asyncFunction(alertFunction); composedFuntion(1); {% endhighlight %}\nOutput Alert:- In Async Callback Alert:- 1\nVOILA! Our alertFunction executes inside the async callback To clarify replacing the nextFunction with body of alertFunction\n{% highlight js %} function params => window.setTimeout(function() { alert(“In Async Handler”); // nextFunction(params) becomes alert(params); }, 1000) {% endhighlight %}\nI couldn’t grok async code in middleware because I was making similar mistake as in thinking that as alertFunction is innermost function in composition and so its body would execute first.But in actuality it doesn’t evaluate to value and hence the order of execution starts from outermost function .\nThis seems trivial after seeing the above example but it wrecked my understanding of middleware code. if G is a function and F is higher order function , ie F(someInputFunction) is also a function , then when you “uncurry” by calling F(G)(x) the evaluation begins from operations defined inside/ to perform F, and operations defined inside/to perform G may or may not be used by F.\nSo we can wrapping asynchronous operations in a chain of composed functions, define them in seemingly synchronous way. And I think this is how Promises are actually implemented, will have to read about that later.\nAnyway this really excites me, how mind bending functional jui jutsu can accomplish a lot with less code. Will post more when I learn more.. Eager to hear your experience in learning functional programming inception.\nReposted from medium.com"
  },
  {
    "objectID": "posts/semantic-legion-1/index.html",
    "href": "posts/semantic-legion-1/index.html",
    "title": "Semantic Legion",
    "section": "",
    "text": "I am guilty of spamming people in the degree one of my network with too many links in topics that fancy the Legion of varied interests that haunt me. Following the suggestion of Ananda Seelan, I am consolidating my link blasts into a considated blog post format, thus begins the “Semantic Legion”. This exercise might help organize the “Legion” in my head and maybe lead to more focused blog posts."
  },
  {
    "objectID": "posts/semantic-legion-1/index.html#machine-learning",
    "href": "posts/semantic-legion-1/index.html#machine-learning",
    "title": "Semantic Legion",
    "section": "Machine Learning",
    "text": "Machine Learning\n\nThe Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks\nThe lottery ticket hypothesis suggests that big Deep Neural Nets train better than smaller nets because they get lucky. Essentially like someone who has purchased more number of lottery tickets.\n\nPrune a large neural network by zeroing the bottom x% of weights by magnitude. This can be done one shot or iteratively while training.\nReset the obtained subnetwork weights to the exact weights you randomly intialized before training the large neural network.\nThe pruned subnetwork converges to similar test error rate as the full network or even better in the same number of epochs.\nThe authors notice that if you were try some other initialization for the subnetwork or even sample from similar distribution it doesn’t work. Hence they hypothesise that the larger network essentially got lucky.\n\nThough the subnetwork is smaller, computations will need sparse matrix multiplication optimizations to be faster.\n\n\nUnderstanding the generalization of ‘lottery tickets’ in neural networks\nFacebook extends the study and checks it for various architectures, tasks and optimizer setting. The lottery ticket phenomena seems to occur in most places. The lottery ticket subnetworks generalize across datasets. This blog post is a summary of multiple papers by Facebook AI group in analyzing this phenomena.\n\n\nNew Theory Cracks Open the Black Box of Deep Learning\nInformation bottleneck theory a hypothesis about how neural nets learn is creating some buzz. One of the claims is that the output of earlier layers have more mutual information with the inputs while final layer outputs have more mutual information with the outputs than the inputs. The information about input gets compressed in each layer.\n {: style=“text-align: center;”} Lucy Reading-Ikkanda/Quanta Magazine; adapted from arXiv:1703.00810 [cs.LG] {: style=“color:gray; font-size: 80%; text-align: center;”}\n\n\nEvolution of Representations in the Transformer\nThis is a great practical example of using information bottlenecks to analyze neural nets behaviour. This research (accompanied by inspirationally well written blog post) compares the evolution of representations in three different NLP encoder models. And in part explains some empirical findings such as why de-noising objective works better than casual language model objective or encoders from translation objective for transfer learning.\n\n\nUniversal Adversarial Triggers for Attacking and Analyzing NLP (Wallace et al. EMNLP 19)\nThis paper finds magic spells that make your NLP models malfunction. They find phrases that cause a specific model prediction when concatenated to 𝘢𝘯𝘺 input from a dataset. These phrases are reported to work across architectures for the same dataset.\nTriggers cause:\n\n1. GPT-2 to spew racism\n2. SQuAD models to answer \"to kill american people\" for 72% of questions asking \"Why...\"\n3. Classification models to drop from 90% accuracy to 1%\n\n\nAllenNLP Interpret\nThis is a great set of features for interpretability added to AllenNLP library.\n\nWe present AllenNLP Interpret, a toolkit built on top of AllenNLP for interactive model interpretations. The toolkit makes it easy to apply gradient-based saliency maps and adversarial attacks to new models, as well as develop new interpretation methods. AllenNLP interpret contains three components: a suite of interpretation techniques applicable to most models, APIs for developing new interpretation methods (e.g., APIs to obtain input gradients), and reusable front-end components for visualizing the interpretation results.\n\nThe amazing thing here is with implementing a simple interface in your model predictor allows you to apply a suite of interpretability techniques for our models.\n\n\nAIDungeon2 is here\nThis is a real fun application of langauge model generation. Nick Walton has adapted GPT2 to generate user guided “Choose your own” text RPG type games. Now you can try out anything you fancy by just issuing commands like “Cast a spell to Reverse entropy”. A truly open world RPG with a AI dungeon master. The model weaves your actions to generalte plausible/surreal story continuations. Hacker News discussion about it. The nature of the model make them generate surreal dream like scenarios. There are glaring consistency issues in the generated story lines. This points to a symbolic gap that is yet to be filled.\n {: style=“text-align: center;”} Source: aiweirdness.com {: style=“color:gray; font-size: 80%; text-align: center;”}\n\n\nControlling Text Generation with Plug and Play Language Models\nOn the topic of controlling language models, uber research has found a way to control the generation of models like GPT2 without fine-tuning."
  },
  {
    "objectID": "posts/semantic-legion-1/index.html#quantum-computing-linear-algebra-tools-for-learning",
    "href": "posts/semantic-legion-1/index.html#quantum-computing-linear-algebra-tools-for-learning",
    "title": "Semantic Legion",
    "section": "Quantum Computing, Linear Algebra, Tools for Learning",
    "text": "Quantum Computing, Linear Algebra, Tools for Learning\n\nQuantum Computing for the Very Curious\nI wanted to try out Micheal Nielsen’s (of neuralnetworksanddeeplearning.com fame) Quantum computing article. This long-form educational article attempts a unique teaching method by embedding flash cards (anki cards) and reminding readers via email to revisit the cards. I got around doing it at behest of the amzing Professor Balaji (a teacher of mine) who gave this as an exercise to test Linear Algebra understanding. Prior knowledge of the truly abstract nature of linear algebra (basis, linear transformations, linear combinations) really helped me to grok the essay.\nThe learning approach taken by this article (embedding flash cards + reminders) article shows how computing medium can be extended to augment our understanding. This scratches the surface of Alan Kay’s vision of computers being tools that extend our mind.\n\n\nAugmenting Long-term Memory\nIf you’re curious about spaced repition flash card approach to learn new math theorems, machine learning concepts etc Micheal has written extensively about it in the above link.\n\n\nAnki Flash Cards with Spaced Repitition\nThe free app Anki is example of good software aimed at expanding our capabilites rather than popular objective of draining attention. It has web, desktop and mobile versions for creating Anki (flash) cards with spaced repitition tracking. I am in the process of adopting it for my learning. Not yet successful in integrating it fully, will blog more about my experience in future.\n\n\nPolar App\nRelated learning tool I found is Polar. > “A powerful document manager for web pages, textbooks, PDFs, and anything you want to read. Supports tagging, annotation, highlighting and keeps track of your reading progress.”\nIt doesn’t have a firefox extension yet. But it allows creating anki cards that can be synced to Anki app from web highlights. This helps in creating a learning expereince like the quantum computing blog for any document."
  },
  {
    "objectID": "posts/semantic-legion-1/index.html#philosophy",
    "href": "posts/semantic-legion-1/index.html#philosophy",
    "title": "Semantic Legion",
    "section": "Philosophy",
    "text": "Philosophy\n\nWould aliens understand lambda calculus?\nPlatonism vs Aristotelianism is an age old debate in philosophy. Professor Balaji (a teacher of mine) had a strong notion that the current mathematics we have is strongly influenced by our spatio-visual sense. Stumbled upon the above post which makes similar claims. It claims that certain cognitive priors are necessary to converge upon ideas which some consider as universal.\nI don’t know enough to lean on any side of the debate heavily. But my intution lies with universality/platonism of physics, mathematics and computatability. I think even if Alien’s use some other metaphors to arrive at Lambda Calculus, the underlying notion of universal computability (if correct) will be the same.\n\n\nNew AI Strategy Mimics How Brains Learn to Smell\nI am now exploring search systems over neural net generated representation (vector spaces). This generally involves approximate methods such as Locality senstive Hashing. The method described in this post was interestingly derived from the sense of smell of fruit-flies. This lends some weight top the notion that our cognitive reliance on certain senses (vision) makes some ideas intutive, but exploring outside it can expand our horizons. (Purely my speculation to be taken with a grain of salt.)"
  },
  {
    "objectID": "posts/semantic-legion-1/index.html#programming-languages",
    "href": "posts/semantic-legion-1/index.html#programming-languages",
    "title": "Semantic Legion",
    "section": "Programming Languages",
    "text": "Programming Languages\n\nType State Pattern\nTo eliminate errors make them impossible in runtime is a mantra I stand behind. Programming Patterns that are finally entering mainstream (after stewing in the academic functional world) such as Optional are moving errors to compile time. Among the patterns, type state caught my eye. Using rust’s borrow checker and other langauge features allows one to build compile time state machines. They can be as simple as allowing the compiler to disallow methods such as read on file references that are closed. Or it can be taken one step beyond to write a full blown state machines that track the current state in compile time. ie Say you have an API that needs a handshake to be performed before sending, you can ensure in compile time that the “send” method can be called only after “handshake” is called. How awesome is that.\n\n\nWhy Monads matter?\nThis article explains what the usally hyped functional programming concept of monad solves for a imperative programmer. I have not dived deeply into any functional language yet. Seeing how even weakly adopted fucntional programming concepts such as Optionals (algebraic data types) and Optional Chaining (which is a monad) makes me question what is the cost with which the programming world is ignoring Functional paradigmn. Are the functional languages difficult to learn, or is it exposure bias towards imperative languages? Or do we need the functional abstractions to be put in better terms for people to grok them? Only time will tell."
  },
  {
    "objectID": "posts/Manjaro-Linux/index.html",
    "href": "posts/Manjaro-Linux/index.html",
    "title": "Manjaro Linux - My current daily driver arch based distro",
    "section": "",
    "text": "And as I got a new mechanical keyboard (Reddragon KUMARA, cheap as cherry MX patents on mechanical switches have died out) and a gaming mouse (Logitech G402) recently, I wanted to use my desktop as main programming machine, instead of my old macbook air.\n\n\n\nMy Current PC with keyboard and mouse\n\n\nSo to inevitably I was going to reinstall GNU/Linux.Choosing a new linux distro as you all know leads to paralysis due to infinite choice.\nOf all the distros that I had installed, I love Arch Linux the most. Arch with its rolling update model, and the infinite repositories of aur, always hit the sweet spot. But I was feeling a bit lazy, as in Arch Linux one has to setup everything from scratch (which I recommend at least once), and for some reason my font settings was always bad in vanilla Arch Setup. I know that is not exactly an insurmountable problem, but as I said before, was feeling lazy.\nAfter a love/hate relationship with mac OS, I really wanted a good GUI experience. I am KDE kind of a guy, and love KDE plasma environment. I shopped around a bit, and found elementaryOS based on new desktop environment called the Pantheon. Thought I would give it a shot, though it wasn’t Arch based. But eventually decided against it, because it didn’t seem very customizable, which is rather the point of elementaryOS.\nSo my endless distro search, ended up with deciding between variants based on arch linux. Manjaro, Antergos , ApricityOS, I eventually decided to install Manjaro, because I heard that other two are basically GUI installers of Arch, while Manjaro was a distro based on arch with customizations to make it a more smooth ride.\n\n\n\nImage of Manjaro KDE Desktop\n\n\nBoy, Manjaro with KDE, had refreshing feel, it felt like what Linux mint was when the first time I installed it. And propriety driver support was out of the box. I would recommend Manjaro as daily development OS, where you want arch, but don’t feel like configuring it perfectly."
  },
  {
    "objectID": "posts/crashes-are-optional/index.html",
    "href": "posts/crashes-are-optional/index.html",
    "title": "Crashes are Optional! && Write Less, Do More",
    "section": "",
    "text": "About 60-66 people turned out for the meetup. It was fun and great learning experience to share what you know with others. It also exposed where I have to concentrate to develop my public speaking skills.\nHere are the slides for the talk. I will try to put up the playground file later."
  },
  {
    "objectID": "posts/speed-up-ios-development/index.html",
    "href": "posts/speed-up-ios-development/index.html",
    "title": "Speed up iOS dev using XCode Injection Plugin",
    "section": "",
    "text": "One of the most boring/unproductive part of any development cycle is waiting for your project to compile. And if you have done iOS development, you know how much time is wasted for XCode to recompile the project. And even if compile time is less, you have to follow a bunch of taps, long presses etc to get to the desired app state before even testing your changes. This plugin will reduces reduce these steps considerably. As you change your code , you can inject the new class definition using this XCode plugin. It recompiles just changed file, and injects it into the live running app. The great thing is it works for real device as well.\n\nSet up\n\nInstall Alcatraz package manager for XCode . Alcatraz allows you to install and remove XCode plugins hazzle free\nOpen using Alcatraz with package manager option in projects menu.\nSearch for injection plugin, and install it. Restart XCode after installations\nTo make it work in real device you need to click on Product -> Injection Plugin -> Patch Project for Injection. It will add couple of lines to your main.m of your project. If it is a swift project just create a empty main.m and do the above.\n\n\n\nInject Code\n\nRun your project, make some changes to the code in a file, press ^ + = .\nYour changed code in that file will get compiled and injected into the app live.\nNow you can simply have to somehow make your program create new object of the changed class to see the changes. For example, tap back button and then again go the view.\nThere are few limitations on what can be injected, refer the Injection for XCode’s github project. And a small limitation currently is it wont work if you have more than 128 source files in your project due to a XCode limitation. Follow this and other issues in github issue tracker.\nYou can also set it up to automatically inject changes by enabling File Watcher in Product-> Injection Plugin -> Tunable App Parameters\nIf you want it to make changes visible as you inject the changes in you have to modify your normal code. You have to listen for callbacks after injection , and reload the view or do something else.Read the instructions.\n\nThis plugin will add a new folder to your project which you can add to .gitignore to avoid source control.\nIt is great that the developer has open sourced it. To know how it does its magic see here The author has released it under “nagware” license, where he requests you to pay after using it for two weeks."
  },
  {
    "objectID": "posts/creation-and-consumption/index.html",
    "href": "posts/creation-and-consumption/index.html",
    "title": "Creation and Consumption",
    "section": "",
    "text": "I think mindful consumption is of great importance. We can start being aware of what our consumption inlets are. Just the awareness of this can create great impact on how we choose to spend our time.\nI am not arguing against consuming media, but for having healthy awareness on wether it has negative effects on one’s creative spirit. It is important in my opinion to keep the act of creation balanced with consumption.\nTo contemplate, and engage actively with the world through some outlet is a form of habit I hope we can get into. This outlet can be anything, writing, music, coding for open source, some personal project etc.\nI think it is more fulfilling to move towards a balanced consumption to creation ratio."
  },
  {
    "objectID": "posts/learning-long-term-dependencies-rnn/index.html",
    "href": "posts/learning-long-term-dependencies-rnn/index.html",
    "title": "Learning Longer-term Dependencies in RNNs with Auxiliary Losses (ACL 2018)",
    "section": "",
    "text": "TLDR; RNNs can learn very long sequences when auxiliary loss for predicting input sequence (forward and backward) from different timesteps is added.\n\nProblem being addressed\nRecurrent neural nets in theory can learn arbitrarily long sequences, but in practice suffer from problems like vanishing gradients etc. Techniques to reduce vanishing gradients problem like LSTM alone don’t work for very long sequences.\nRNN has a better tradeoff when it comes to memory requirements compared to CNN based networks or vanilla Transformer nets. When processing very large sequences this becomes important.\n\n\nProposed Method\nTake the hidden state of main RNN used for a given task at sampled timesteps, use another RNN at sampled intervals, and try to predict the input sequence to certain time steps. Truncated BPTT (Back propagation through time) to few timesteps would give a new loss.\n\n\nEvaluation and results\nEvaluation is done on MNIST, CIFAR-10, Stanford dog dataset is given as sequence of pixels to an RNN with classification being the target. Since the pixels are flattened to sequential input, spatial location information is now across whole range of the sequence, requiring long dependencies to be formed to get good results. The authors also test it on character based classification on dbpedia. This technique achieves very significant results on long sequences compared to existing LSTMs, Transformers etc.\n\n\nOpinions\nThis paper makes a significant experiment to improve a crucial behavior of RNNs on long sequences. The ablation study is well done. This auxiliary loss reminded me of the World models paper where the task of predicting future states improves current tasks output."
  },
  {
    "objectID": "posts/MIT-OCW-Algorithms/index.html",
    "href": "posts/MIT-OCW-Algorithms/index.html",
    "title": "MIT OCW 6.006 Algorithms Course",
    "section": "",
    "text": "MIT OCW 6.006 Introduction to Algorithms Fall 2011 course is packed with awesome content. The open course ware website contains video recording of the MIT course conducted at MIT and all the accompanying notes, assignments, test content. It does not contain interactive content like udacity courses, but the content is in different level.You need to know python to do this course’s assignments. If you know how to code in any language , you can pick up basics of python in a day or two as it has simple syntax.\nProf. Erik Demaine ,Prof. Srinivas Devadas do a great job introducing concepts of algorithms and its analysis in the main lectures. Victor Costan does exemplary job in recitation videos in explaining the concepts introduced by Erik and Srini in easily understandable way. So don’t miss out on recitation if you plan to look into this course.\nThe assignments are one of the fun parts of the whole course. They allow you to see how efficient algorithms can really make a difference in running time of your code. These guys have done a awesome job in designing each assignment in such a way that you can visualise the impact of algorithms. For example in assignment 3 where you are supposed to write code to detect crossings among wires, they have designed the assignment in such a way that you can see the result of your code running in the browser.\nThe course videos are also available in youtube as playlist. Checkout the first video."
  },
  {
    "objectID": "posts/semeval-19-semi-supervised-domain-adaptation-for-suggestion-mining/index.html",
    "href": "posts/semeval-19-semi-supervised-domain-adaptation-for-suggestion-mining/index.html",
    "title": "SemEval 2019 - Semi-Supervised Domain Adaptation for Suggestion mining",
    "section": "",
    "text": "SemEval Workshop regularly has been conducting tasks in NLP to evaluate the progress in the field.\nI and my colleague Ananda Seelan participated in this year SemEval’s Suggestion mining task (Task 9). Here is our submission to be published in NAACL 2019 proceedings, and the code is on github.\nThis blog is a summary of the key techniques and ideas which influenced this work."
  },
  {
    "objectID": "posts/semeval-19-semi-supervised-domain-adaptation-for-suggestion-mining/index.html#suggestion-mining-task",
    "href": "posts/semeval-19-semi-supervised-domain-adaptation-for-suggestion-mining/index.html#suggestion-mining-task",
    "title": "SemEval 2019 - Semi-Supervised Domain Adaptation for Suggestion mining",
    "section": "Suggestion Mining Task",
    "text": "Suggestion Mining Task\nThe suggestion mining task in brief is a text classification task to find whether a sentence contains a suggestion.\nExample,\n\nSuggestion - It would be nice if they had vegan options.\nNon Suggestion - This restaurant has good vegan options.\n\nAbout 8k sentences scrapped from technical forumns were provided as training data. The task was divided into two subtasks.\n\nSubtask A - Evaluation on same domain - technical forums posts.\nSubtask B - Evaluation on out of domain - hotel reviews.\n\nThe catch for subtask B is human labelled data in hotel reviews domain is not allowed for training. Our model was placed third place in the leaderboard for Subtask B."
  },
  {
    "objectID": "posts/semeval-19-semi-supervised-domain-adaptation-for-suggestion-mining/index.html#key-techniques",
    "href": "posts/semeval-19-semi-supervised-domain-adaptation-for-suggestion-mining/index.html#key-techniques",
    "title": "SemEval 2019 - Semi-Supervised Domain Adaptation for Suggestion mining",
    "section": "Key Techniques",
    "text": "Key Techniques\nWe used simple convolutional neural networks for text classification. And we applied transfer learning and semi-supervised learning for the tasks.\n\nTransfer Learning\nThe current trend in machine learning for NLP is to using pre-trained language models. We used google’s recently published BERT model as our representation layer. Take a look at http://jalammar.github.io/illustrated-bert/ for a good description of how pre-trained models work for NLP.\n\n\nSemi-Supervised Learning\nIn ACL 2018 conference Melbourne, I attended two talks which impacted the work in this paper. One was Sebastien Ruder’s talk on Strong baselines for semi-supervised learning in NLP. The conclusion of Sebastian Ruder, Barbara Plank (2018) was that classic machine learning techniques for semi-supervised learning such as Tri-Training prove as strong baseline in NLP with neural nets. Sebastien has a very accessible and thorough blog post explaining the techniques. They have also made their code available on github.\n\n\nWe applied a variant of tri-training. We use three models of the same architecture trained initially on data bootstrap sampled from the tech reviews data. The three models are used to iteratively label unlabelled data from hotel reviews domain. Agreement of labels between two models is used as way to select sentences to be added to next iteration of training. Pseudo-code and detailed explanations can be found in the paper. Or you might as well look to the code, as its way simpler than a dry description of it might suggest."
  },
  {
    "objectID": "posts/semeval-19-semi-supervised-domain-adaptation-for-suggestion-mining/index.html#statistical-significance",
    "href": "posts/semeval-19-semi-supervised-domain-adaptation-for-suggestion-mining/index.html#statistical-significance",
    "title": "SemEval 2019 - Semi-Supervised Domain Adaptation for Suggestion mining",
    "section": "Statistical Significance",
    "text": "Statistical Significance\nThe other work published and presented in ACL 2018 that influenced this paper is Rotem Dror’s “The Hitchhiker’s Guide to Statistical Significance in NLP”.\n\n\nWe report confidence intervals for five random seeds for all our experiments. And we also do pair-wise significance testing via McNemar’s test to evaluate whether pair-wise model performance on the test set vary significantly.\n\nMetrics\n\n\n\nimg\n\n\n\n\nMcNemar’s Test\n\n\n\nimg"
  },
  {
    "objectID": "posts/efficient-dynamic-batching-of-large-datasets-with-infinibatch/index.html",
    "href": "posts/efficient-dynamic-batching-of-large-datasets-with-infinibatch/index.html",
    "title": "Efficient Dynamic Batching of Large Datasets with Infinibatch",
    "section": "",
    "text": "We will explore how to efficiently batch large datasets with varied sequence length for training using infinibatch. The focus will be on solving multiple challenges associated with this and making it work with dataloader abstraction in pytorch library. Though our focus is on pytorch, Infinibatch is a pure python library agnostic of the deep learning library.\nThis post was inspired by this thread on twitter. > twitter: https://twitter.com/marian_nmt/status/1292850875715604480"
  },
  {
    "objectID": "posts/efficient-dynamic-batching-of-large-datasets-with-infinibatch/index.html#loading-and-shuffling-large-datasets",
    "href": "posts/efficient-dynamic-batching-of-large-datasets-with-infinibatch/index.html#loading-and-shuffling-large-datasets",
    "title": "Efficient Dynamic Batching of Large Datasets with Infinibatch",
    "section": "1. Loading and shuffling large datasets",
    "text": "1. Loading and shuffling large datasets\nFor large datasets, loading the entire data into memory might not be possible. If we were to sample fully random batches we need to do random access on huge dataset. Depending on the disk latency this might be unfeasible.\nTo solve this we can do the following.\n\nShard the data into chunks larger than single instances so that it reduces the disk access.\nShuffle the chunks and load few of them and shuffle the data loaded from the chunks.\n\nIf we shard the pieces into too big chunks we might end up loosing statistical power in our training updates as we are essentially reducing the randomness of our samples used for training. But we can’t shard them too small either as that wouldn’t solve our disk access problem.\nWe need a flexible approach would make it easy to control how much data is to be loaded into memory for shuffling. To address this challenge in isolation, you can refer dataset sharding logic in NVIDIA’s MEGATRON language model training code. But infinibatch solves it in a more generalized manner along with our other challenges."
  },
  {
    "objectID": "posts/efficient-dynamic-batching-of-large-datasets-with-infinibatch/index.html#dynamic-batching",
    "href": "posts/efficient-dynamic-batching-of-large-datasets-with-infinibatch/index.html#dynamic-batching",
    "title": "Efficient Dynamic Batching of Large Datasets with Infinibatch",
    "section": "2. Dynamic Batching",
    "text": "2. Dynamic Batching\nNLP datasets generally have samples which are of varied lengths. When we batch the data for training on devices like GPU, we are forced to make them into n-dimensional tensors with fixed dimension. The most common type of input for NLP models is of the shape Mini-batch size x Sequence length. The sequence length is either a fixed value or is the length of longest sequence in that batch. The shorter sequences in the minii-batch are generally padded with a padding token. These padding tokens are wasteful in terms of computation as they don’t do anything useful.\nSome tutorials and examples you would find for pre-processing data would pad batches to a pre-determined sequence length independent of the elements in each batch. This is fully wasteful as many batches would have all the members less than the pre-determined length.\nA better option would be to pad the elements of each batch to the sequence length which is maximum in that batch. This dynamic padding can improve efficiency but it doesn’t solve the entire problem. Let’s see why with an example.\n\nTokenization and length distribution\nLet’s implement a typical dynamic padding workflow with pytorch dataloader and a subword level tokenizer. We use BERT-base-cased tokenizer from huggingface’s transformers library. This tokenizes words to subwords. The BERT model was pre-trained with maximum subword length of 512. We can theoretically use sequence lengths larger than that but for our purposes we will leave it as such at 512.\nWe will use torch’s dataset and dataloader abstraction for this. It will as both an illustration of real world usage and is convinent as it helps avoid having to entire tokenized dataset in memory. We still have to load the sentences into memory once. This is not a problem for small datasets, but for very large corpuses it’s a big problem as mentioned before.\n\n\nCode\n!pip install git+https://github.com/microsoft/infinibatch.git transformers torch\nfrom typing import Dict, List\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport tqdm\nfrom transformers import AutoTokenizer, PreTrainedTokenizerFast\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\n\n%matplotlib inline\n\n\n\nclass Wiki103(Dataset):\n    def __init__(self, tokenizer: PreTrainedTokenizerFast, data_path=\"wikitext-103-raw/wiki.train.raw\", max_length: int = 512):\n        self.tokenizer = tokenizer\n        with open(data_path) as dp:\n            # We are \n            self.sentences = [sentence.strip() for sentence in dp if len(sentence.strip()) > 2]\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.sentences)\n    \n    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n        return self.tokenizer(self.sentences[i], max_length=self.max_length, truncation=True)\n\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\", use_fast=True)\nwiki_dataset = Wiki103(tokenizer=tokenizer)\nsequence_lengths = []\nfor example in tqdm.tqdm(iter(wiki_dataset)):\n    sequence_lengths.append(len(example[\"input_ids\"]))\n\n\n\n\n\n\n\n\n\n\n\n\n\n1164464it [04:25, 4380.35it/s]\n\n\nBy plotting the truncated Subword sequence length vs Frequency we see a distribution with a large variance.\n\n\nCode\nwith plt.style.context('fivethirtyeight'):\n    plt.figure(figsize=[7,5])\n    n, bins, patches = plt.hist(x=sequence_lengths, bins=50, color='#0504aa',alpha=0.7, rwidth=0.85)\n    plt.grid(axis='y', alpha=0.75)\n    plt.xlabel('Subword Sequence Length',fontsize=15)\n    plt.ylabel('Frequency',fontsize=15)\n    plt.xticks(fontsize=15)\n    plt.yticks(fontsize=15)\n    plt.ylabel('Frequency',fontsize=15)\n    plt.title('Sequence Length Distribution',fontsize=15)\n    plt.show()\n\n\n\n\n\n\n\nDynamic Padding\nFrom the above graph we can intuit that if we draw random samples from the data to form a mini-batch, we would have few examples which are significantly longer than the rest. This would mean that we would add a lot of padding tokens. This holds even if we clean the very short length instances as noise.\nLet’s implement dynamic padding and measure how much. We can use torch’s DataLoader abstraction to do efficient batching with multi-processing. Since our tokenized outputs are of different lengths we have to implement a collate function to pad them dynamically together. We can pass the tokenizer.pad function implemented in huggingface’s tokenizer as the collate function.\n\ndef collate_fn(examples: List[Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:\n    # Since huggingface has already implemented this, this function is just to illustrate what a collator does.\n    return tokenizer.pad(examples, return_tensors='pt')\n\ndataloader = DataLoader(dataset=wiki_dataset, batch_size=32, collate_fn=collate_fn)\n\nLet’s assume that we can use maximum a batch size of 32 for max sequence length of 512 for our model in our training hardware without out-of-memory errors. The tokens per batch would be 512 * 32 = 16384. We can now compute how much of it is padding tokens and what is the distribution of the batch’s sequence length(which depends on the maximum element in the batch).\n\ntotal_tokens = 0\npadding_tokens = 0\nbatch_lengths = []\nfor batch in tqdm.tqdm(iter(dataloader)):\n    batched_input_ids = batch[\"input_ids\"]\n    batch_lengths.append(batched_input_ids.shape[1])\n    total_tokens += batched_input_ids.numel()\n    padding_tokens += batched_input_ids[batched_input_ids == tokenizer.pad_token_id].numel()\n\n100%|██████████| 36390/36390 [06:04<00:00, 99.96it/s] \n\n\n\nprint(f\"Total Batches    : {len(iter(dataloader))}\")\nprint(f\"Padding Tokens   : {padding_tokens}\")\nprint(f\"Input Tokens     : {total_tokens - padding_tokens}\")\nprint(f\"Total Tokens     : {total_tokens}\")\nprint(f\"Padding Tokens % : {(padding_tokens*100)/total_tokens}\")\n\nTotal Batches    : 36390\nPadding Tokens   : 244072396\nInput Tokens     : 119699332\nTotal Tokens     : 363771728\nPadding Tokens % : 67.09493267712108\n\n\nSurprise, surprise, 67% of our net tokens are padding tokens. This would imply that of all the computations that we do, only 33% of is done for useful work. This starkly highlights the problem with static batch lengths even when accounting for dynamic padding.\nLet’s also plot the distribution of batch lengths.\n\n\nCode\nwith plt.style.context('fivethirtyeight'):\n    plt.figure(figsize=[7,5])\n    n, bins, patches = plt.hist(x=batch_lengths, bins=50, color='#0504aa',alpha=0.7, rwidth=0.85)\n    plt.grid(axis='y', alpha=0.75)\n    plt.xlabel('Batch Sequence Length',fontsize=15)\n    plt.ylabel('Frequency',fontsize=15)\n    plt.xticks(fontsize=15)\n    plt.yticks(fontsize=15)\n    plt.ylabel('Frequency',fontsize=15)\n    plt.title('Static Batch - Dynamic Padding Length Distribution',fontsize=15)\n    plt.show()\n\n\n\n\n\nAs batches are randomly sampled, we see a normal distribution as we can should expect by the Central Limit Theorem. The frequency in the final bin is deviant because we have a significant number of sentences which we had truncated, hence batches with them will have the maximum sequence length.\n\n\nGeneral approach to dynamic batching\nInstead of drawing samples in random, had we sorted our dataset by length, then we can form batches by packing similar length sequences together into a batch till we reach the maximum number of tokens that we can fit. The maximum number of tokens that can be packed can be derived approximately from our previous memory limit static_batch x max_sequence_length. This allows us to pack more instances in one batch without much padding because the sequences would be of similar lengths after sorting.\nWe can’t sort the entire dataset because machine learning training is based on the assumption that our instances are drawn independently from an identical distribution (IID). If we were to sort the entire dataset this breaks the assumption as our samples are no longer drawn independently from each other. If sentence length were a confounding factor then the model might fit on this spurious correlation.\nWe have a trade-off here between statistical power derived from randomization of our samples and lesser error in gradient updates derived from larger batch sizes if we batch dynamically.\nGenerally, we can have a positive trade off by sampling a window of instances and sorting withing the window and forming batches.\nThe Dataset we implemented above is a map-style dataset. It implements length and random access to each individual data sample with index (__getitem__). The sampling into batches is taken care of a sampler passed to DataLoader.\nI don’t think there is a clean way to implement a map-style dataset and a collate function such that we get batches with dynamic batch sizes but same number of tokens per batch. This comes from the basic mismatch of number of dynamic batches which you can form keeps changing based on the larger window you sample.\nSo it turns out that we have to do all the shuffling, windowing, sorting and batching inside a iterable-style IterableDataset dataset abstraction. These features are implemented by infinibatch."
  },
  {
    "objectID": "posts/efficient-dynamic-batching-of-large-datasets-with-infinibatch/index.html#checkpointing",
    "href": "posts/efficient-dynamic-batching-of-large-datasets-with-infinibatch/index.html#checkpointing",
    "title": "Efficient Dynamic Batching of Large Datasets with Infinibatch",
    "section": "3. Checkpointing",
    "text": "3. Checkpointing\nIn large datasets, it’s typical not to wait for an entire epoch to checkpoint your model to recover from failures. So to be able to recover and continue training in a deterministic manner, such that it converges to same state if the failure hadn’t occured, we have to checkpoint the random state that controls the order in which our samples are generated."
  },
  {
    "objectID": "posts/efficient-dynamic-batching-of-large-datasets-with-infinibatch/index.html#loading-and-shuffling-large-datasets-1",
    "href": "posts/efficient-dynamic-batching-of-large-datasets-with-infinibatch/index.html#loading-and-shuffling-large-datasets-1",
    "title": "Efficient Dynamic Batching of Large Datasets with Infinibatch",
    "section": "1. Loading and shuffling large datasets",
    "text": "1. Loading and shuffling large datasets\nFollowing the infinibatch tutorial, we divide our dataset into multiple gzip chunks of 10000 sentences each.\n\n!mkdir -p wikitext-103-chunks\n!split  -a 4 --lines 10000  --numeric-suffixes --filter 'gzip > wikitext-103-chunks/$FILE.txt.gz' wikitext-103-raw/wiki.train.raw  train.\n\nWe can now create an iterator using infinibatch with a function that can deserialize a shard. Infinibatch takes care of loading multiple in a shuffled order. We can control the amount of deserialized individual examples from the shards be buffered using buffer_size parameter. The library returns a python iterable. We can call next(iterable) or iterate with a for to get the examples.\nNote: Passing train=True creates an infinite iterator that cycles after a full run on the dataset. The chunked_dataset_iterator method returns a composition of iterators, you can refer the source code here\n\nimport gzip, glob\nfrom functools import partial\n\nfrom infinibatch import datasets, iterators\n\ndef read_chunk(path):\n    with open(path, \"rb\") as fp:\n        lines = gzip.decompress(fp.read()).decode(encoding='utf-8').splitlines()\n        lines = [sentence.strip() for sentence in lines if len(sentence.strip()) > 2]\n    return iter(lines)\n\nsentence_it = datasets.chunked_dataset_iterator(\n    chunk_refs = glob.glob('wikitext-103-chunks/train.*.txt.gz'),\n    read_chunk_fn = read_chunk,\n    buffer_size = 100000, seed = 1337, train=True, shuffle=True)\n\nprint(next(sentence_it))\n\nIn 1993 , David Mirkin hired Scully to write for The Simpsons , as a replacement for the departing Conan O 'Brien , after reading some of his sample scripts . He began as a writer and producer for the show during its fifth season and wrote the episodes \" Lisa 's Rival \" , \" Two Dozen and One Greyhounds \" and \" Lisa on Ice \" which aired in season six . \" Lisa 's Rival \" was his first episode ; he wrote the script , but the original concept had been conceived by O 'Brien . Similarly , he wrote the script for \" Two Dozen and One Greyhounds \" , which was based on an idea by Al Jean and Mike Reiss . \" Lisa on Ice \" was inspired by Scully 's love of ice hockey and featured many experiences from his childhood , as was \" Marge Be Not Proud \" ( which he wrote for season seven ) which was based \" one of the most traumatic moments \" of his life , when he was caught shoplifting aged twelve . He jokingly told Variety that \" It 's great to be paid for reliving the horrors of your life . \" He also wrote \" Team Homer \" and \" Lisa 's Date with Density \" . Scully noted : \" I wrote a lot of Lisa 's shows . I have five daughters , so I like Lisa a lot . I like Homer , too . Homer comes very naturally to me : I don 't know if that 's a good or a bad thing . A lot of my favorite episodes are the ones when Homer and Lisa are in conflict with each other ... They 're very human , I think that 's their appeal . \"\n\n\n\nTensorize our dataset with a map iterator\nWe can now compose our tokenizer upon our sentence iterator. Infinibatch has two ways of doing this, 1. MapIterator 2. ParallelMapIterator\nIf you use pytorch and need multiprocessing to do costly transformations over your data on the fly, use the ParallelMap and set the num_processes with what you would have with num_workers. And set num_workers=0 in your dataloader.\n\ntokenize_fn = partial(tokenizer, max_length=512, truncation=True)\n\nfeatures_it = iterators.ParallelMapIterator(\n    source_iterator=sentence_it,\n    num_processes=4,\n    num_items_per_process=1000,\n    transform=tokenize_fn\n)\nnext(features_it)\n\n{'input_ids': [101, 1130, 1949, 117, 1681, 20522, 4314, 4327, 20452, 16125, 1106, 3593, 1111, 1109, 20726, 117, 1112, 170, 5627, 1111, 1103, 18646, 17727, 152, 112, 9620, 117, 1170, 3455, 1199, 1104, 1117, 6876, 15690, 119, 1124, 1310, 1112, 170, 2432, 1105, 2451, 1111, 1103, 1437, 1219, 1157, 3049, 1265, 1105, 1724, 1103, 3426, 107, 6516, 112, 188, 155, 15895, 107, 117, 107, 1960, 2091, 10947, 1105, 1448, 6285, 16930, 1116, 107, 1105, 107, 6516, 1113, 6172, 107, 1134, 4086, 1107, 1265, 1565, 119, 107, 6516, 112, 188, 155, 15895, 107, 1108, 1117, 1148, 2004, 132, 1119, 1724, 1103, 5444, 117, 1133, 1103, 1560, 3400, 1125, 1151, 10187, 1118, 152, 112, 9620, 119, 10321, 117, 1119, 1724, 1103, 5444, 1111, 107, 1960, 2091, 10947, 1105, 1448, 6285, 16930, 1116, 107, 117, 1134, 1108, 1359, 1113, 1126, 1911, 1118, 2586, 2893, 1105, 2639, 11336, 14788, 119, 107, 6516, 1113, 6172, 107, 1108, 3768, 1118, 20452, 16125, 112, 188, 1567, 1104, 2854, 4700, 1105, 2081, 1242, 5758, 1121, 1117, 5153, 117, 1112, 1108, 107, 9751, 2176, 4108, 1753, 5096, 4867, 107, 113, 1134, 1119, 1724, 1111, 1265, 1978, 114, 1134, 1108, 1359, 107, 1141, 1104, 1103, 1211, 23057, 4899, 107, 1104, 1117, 1297, 117, 1165, 1119, 1108, 2347, 4130, 18867, 4079, 4030, 119, 1124, 18114, 1193, 1500, 15526, 1115, 107, 1135, 112, 188, 1632, 1106, 1129, 3004, 1111, 1231, 2646, 3970, 1103, 5367, 1116, 1104, 1240, 1297, 119, 107, 1124, 1145, 1724, 107, 2649, 12353, 107, 1105, 107, 6516, 112, 188, 14265, 1114, 14760, 13730, 107, 119, 20452, 16125, 2382, 131, 107, 146, 1724, 170, 1974, 1104, 6516, 112, 188, 2196, 119, 146, 1138, 1421, 5421, 117, 1177, 146, 1176, 6516, 170, 1974, 119, 146, 1176, 12353, 117, 1315, 119, 12353, 2502, 1304, 8534, 1106, 1143, 131, 146, 1274, 112, 189, 1221, 1191, 1115, 112, 188, 170, 1363, 1137, 170, 2213, 1645, 119, 138, 1974, 1104, 1139, 5095, 3426, 1132, 1103, 3200, 1165, 12353, 1105, 6516, 1132, 1107, 4139, 1114, 1296, 1168, 119, 119, 119, 1220, 112, 1231, 1304, 1769, 117, 146, 1341, 1115, 112, 188, 1147, 5767, 119, 107, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
  },
  {
    "objectID": "posts/efficient-dynamic-batching-of-large-datasets-with-infinibatch/index.html#dynamic-batching-1",
    "href": "posts/efficient-dynamic-batching-of-large-datasets-with-infinibatch/index.html#dynamic-batching-1",
    "title": "Efficient Dynamic Batching of Large Datasets with Infinibatch",
    "section": "2. Dynamic Batching",
    "text": "2. Dynamic Batching\nNow comes the magic of dynamic batching with BucketedReadaheadBatchIterator. Let’s fix the maximum tokens per batch to 32 * 512 = 16384. This iterator allows you to compute dynamic batch size by iteratively applying a user given function over the current longest example (with length computed by user function) in a sorted read_ahead window. This window is sorted and batches are formed by using the user provided batch_size function iteratively.\n\nExample\nSay we want 50 tokens per batch. If we set a read ahead window of 6. Assume we fetch six items [a, b, c, d, e, f] in the first read ahead window.\n\n\n\nSequence id\nLength\n\n\n\n\na\n50\n\n\nb\n30\n\n\nc\n20\n\n\nd\n20\n\n\ne\n30\n\n\nf\n20\n\n\n\nFirst we sort this window with lengths in decreasing order. The sort order is stable. This preserves the shuffling of equal sized elements from previous iterator. So for our example it would be [a, b, e, c, d, f]\nNow we can Compute the dynamic batch sizes by applying the function batch_size iteratively till the window is exhausted. Assume our function is lambda longest_instance: 60 // len(longest_instance). Then applying it once we get first longest item a, current batch size will be 60 //50 = 1. The next longest item remaining can be used to calculate the size of the next batch and so on. So we will end up with [a], [b, e], [c, d, f]. Each of them will have 60 tokens.\nYou can take a look at the code that does this computation here.\n\ntokens_per_batch = 32 * 512\nbatches_it = iterators.BucketedReadaheadBatchIterator(\n    source_iterator=features_it,\n    # read_ahead is the number of items to be read from previous iterator,\n    # these are sorted and over which dynamic batches are formed.\n    read_ahead=10000, \n    # key determines the length used to sort and choose the longest remaining record.\n    key=lambda example: len(example['input_ids']), \n     # Determines the dynamic batch size\n    batch_size=lambda longest_example: tokens_per_batch // len(longest_example['input_ids']),\n    seed=0 \n)\ndynamic_batch_wo_padding = next(batches_it)\nprint(f\"Dynamic batch size: {len(dynamic_batch_wo_padding)}\")\ndynamic_batch_wo_padding = next(batches_it)\nprint(f\"Dynamic batch size: {len(dynamic_batch_wo_padding)}\")\nprint(dynamic_batch_wo_padding[:2])\n\nDynamic batch size: 124\nDynamic batch size: 94\n[{'input_ids': [101, 1109, 3500, 3039, 13025, 1126, 1903, 1104, 1492, 188, 1204, 121, 137, 119, 137, 8347, 5682, 113, 5787, 137, 119, 137, 125, 4023, 114, 117, 1166, 126, 137, 119, 137, 126, 6549, 113, 123, 137, 119, 137, 126, 4023, 114, 1679, 24773, 1167, 1190, 1147, 7741, 119, 3900, 112, 188, 3039, 4049, 1198, 170, 1423, 24773, 1114, 12936, 6398, 2541, 1107, 1103, 3499, 1526, 2084, 1105, 6625, 2188, 20394, 5773, 1633, 117, 1229, 3500, 1486, 2055, 11142, 117, 1681, 156, 10098, 2897, 1105, 1884, 1775, 4978, 11661, 1862, 119, 3396, 24773, 1116, 117, 1160, 1121, 1296, 2755, 117, 1127, 4410, 1112, 7474, 6635, 1107, 1103, 1886, 131, 3500, 112, 188, 4367, 155, 5792, 1108, 1925, 1229, 141, 119, 6511, 1108, 1237, 117, 1105, 3900, 112, 188, 139, 119, 3160, 1108, 1375, 2170, 1229, 1287, 25730, 1931, 17932, 1121, 1203, 2512, 119, 3500, 112, 188, 3499, 1526, 2084, 11311, 5157, 14618, 1125, 2856, 1471, 1121, 1103, 3039, 1160, 2277, 2988, 1106, 1103, 1886, 1112, 170, 1871, 1104, 170, 2960, 1104, 1532, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 2907, 1107, 1478, 117, 1119, 1125, 170, 4374, 1648, 1107, 1103, 28117, 21643, 1273, 1109, 156, 13622, 22273, 7443, 119, 1230, 1397, 1273, 1648, 1108, 1107, 1823, 20452, 1324, 10781, 1204, 2391, 112, 188, 11826, 6945, 1643, 4371, 113, 1478, 114, 119, 1130, 1103, 1273, 117, 17784, 1733, 19572, 1307, 1126, 1586, 23963, 1233, 117, 1150, 1110, 2802, 1106, 1712, 3542, 1104, 8125, 7782, 7895, 112, 188, 1959, 119, 6945, 1643, 4371, 1108, 12468, 1120, 170, 1957, 8685, 1120, 1103, 13631, 2683, 3506, 1570, 2352, 2263, 1107, 1478, 119, 2711, 1103, 3216, 3761, 117, 1103, 1273, 1108, 170, 2798, 2244, 117, 6957, 109, 22803, 1550, 4529, 117, 1543, 1122, 1117, 2439, 137, 118, 137, 19842, 1273, 1106, 1103, 1322, 1104, 1369, 119, 17784, 1733, 19572, 112, 188, 1397, 2672, 1108, 147, 1813, 3925, 113, 1478, 114, 117, 3714, 4387, 144, 7777, 7836, 2328, 1348, 119, 1109, 2523, 1110, 1359, 1113, 158, 119, 156, 119, 4620, 4140, 156, 12821, 3101, 6944, 112, 188, 1581, 5634, 1414, 14871, 1104, 1103, 1269, 1271, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}]\n\n\nNow we can collate our examples and see how much this scheme has saved us. Since a training iterator is infinite, we will recreate our iterators with a non-infinite iterator.\n\nsentence_it_finite = datasets.chunked_dataset_iterator(\n    chunk_refs = glob.glob('wikitext-103-chunks/train.*.txt.gz'),\n    read_chunk_fn = read_chunk,\n    buffer_size = 100000, \n    seed = 1337, \n    train=False, \n    shuffle=False\n)\nfeatures_it_finite = iterators.ParallelMapIterator(\n    source_iterator=sentence_it_finite,\n    num_processes=4,\n    num_items_per_process=1000,\n    transform=tokenize_fn\n)\nbatches_it_finite = iterators.BucketedReadaheadBatchIterator(\n    source_iterator=features_it_finite,\n    read_ahead=10000, # Determines the window for the bucket which\n    # will be sorted and  converted to batches.\n    key=lambda example: len(example['input_ids']), # Determines the length used\n    # to sort and choose the longest remaining record.\n    batch_size=lambda longest: tokens_per_batch // len(longest['input_ids']),\n    # Determines the dynamic batch size\n    seed=0 \n)\ncollate_fn = partial(tokenizer.pad, return_tensors='pt')\ntensors_it_finite = iterators.MapIterator(\n    batches_it_finite,\n    transform=collate_fn\n)\n\n\ntotal_batches_dynamic = 0 \ntotal_tokens_dynamic = 0\npadding_tokens_dynamic = 0\nbatch_lengths_dynamic = []\nfor batch in tqdm.tqdm(tensors_it_finite):\n    total_batches_dynamic += 1\n    batched_input_ids = batch[\"input_ids\"]\n    batch_lengths_dynamic.append(batched_input_ids.shape[1])\n    total_tokens_dynamic += batched_input_ids.numel()\n    padding_tokens_dynamic += batched_input_ids[batched_input_ids == tokenizer.pad_token_id].numel()\n\n7650it [08:11, 15.57it/s]\n\n\n\nprint(f\"Total Batches    : {total_batches_dynamic}\") # Seeing the tqdm stats.\nprint(f\"Padding Tokens   : {padding_tokens_dynamic}\")\nprint(f\"Input Tokens     : {total_tokens_dynamic - padding_tokens_dynamic}\")\nprint(f\"Total Tokens     : {total_tokens_dynamic}\")\nprint(f\"Padding Tokens % : {(padding_tokens_dynamic*100)/total_tokens_dynamic}\")\n\nTotal Batches    : 7650\nPadding Tokens   : 3838939\nInput Tokens     : 119699332\nTotal Tokens     : 123538271\nPadding Tokens % : 3.1074896620497463\n\n\nWe have reduced the % of padding tokens per epoch from 67% to just around 3%.The total batches needed to process it in the same max tokens per batch limitation hence got reduced nearly five times from 36390 to 7642.\nThe processing time is just one minute extra. I guess that might be due to IO, but you could try benchmarking that with more rigour.\nNow, plotting the length distribution for dynamic batches.\n\n\nCode\nwith plt.style.context('fivethirtyeight'):\n    plt.figure(figsize=[7,5])\n    n, bins, patches = plt.hist(x=batch_lengths_dynamic, bins=50, color='#0504aa',alpha=0.7, rwidth=0.85)\n    plt.grid(axis='y', alpha=0.75)\n    plt.xlabel('Batch Sequence Length',fontsize=15)\n    plt.ylabel('Frequency',fontsize=15)\n    plt.xticks(fontsize=15)\n    plt.yticks(fontsize=15)\n    plt.ylabel('Frequency',fontsize=15)\n    plt.title('Dynamic Batch - Dynamic Padding Length Distribution',fontsize=15)\n    plt.show()\n\n\n\n\n\nWe now see that the expected per batch sequence length has reduced from 300 to 200."
  },
  {
    "objectID": "posts/efficient-dynamic-batching-of-large-datasets-with-infinibatch/index.html#static-batch-sizes-with-sorted-window",
    "href": "posts/efficient-dynamic-batching-of-large-datasets-with-infinibatch/index.html#static-batch-sizes-with-sorted-window",
    "title": "Efficient Dynamic Batching of Large Datasets with Infinibatch",
    "section": "Static Batch Sizes with Sorted Window",
    "text": "Static Batch Sizes with Sorted Window\nIn practise using variable batch sizes could be a problem depending on your task. As tokens in an instance are already correlated, having few instances (though longer) in one batch might give a more noisy gradient update than many shorter instances in a batch. This is my speculation as to why I wasn’t able to make purely dynamic batch sizes work well with a token classification fine-tuning on BERT.\nBut all is not lost here, we can still use bucketing to have uniform length batches with fewer padding tokens. Let’s checkout the padding tokens % when we fix the batch size as 32 but sort a window of 10 batches (320 instances) and then form batches.\n\nsentence_it_finite = datasets.chunked_dataset_iterator(\n    chunk_refs = glob.glob('wikitext-103-chunks/train.*.txt.gz'),\n    read_chunk_fn = read_chunk,\n    buffer_size = 100000, \n    seed = 1337, \n    train=False, \n    shuffle=False\n)\nfeatures_it_finite = iterators.ParallelMapIterator(\n    source_iterator=sentence_it_finite,\n    num_processes=4,\n    num_items_per_process=1000,\n    transform=tokenize_fn\n)\nbatches_it_finite = iterators.BucketedReadaheadBatchIterator(\n    source_iterator=features_it_finite,\n    key=lambda example: len(example['input_ids']), # Determines the length used\n    read_ahead=320, # Setting this ten times the static batch size.\n    batch_size=32,\n    seed=0 \n)\ncollate_fn = partial(tokenizer.pad, return_tensors='pt')\ntensors_it_finite = iterators.MapIterator(\n    batches_it_finite,\n    transform=collate_fn\n)\ntotal_batches_bucket_sorted = 0 \ntotal_tokens_bucket_sorted= 0\npadding_tokens_bucket_sorted = 0\nbatch_lengths_bucket_sorted = []\nfor batch in tqdm.tqdm(tensors_it_finite):\n    total_batches_bucket_sorted += 1\n    batched_input_ids = batch[\"input_ids\"]\n    batch_lengths_bucket_sorted.append(batched_input_ids.shape[1])\n    total_tokens_bucket_sorted += batched_input_ids.numel()\n    padding_tokens_bucket_sorted += batched_input_ids[batched_input_ids == tokenizer.pad_token_id].numel()\nprint(f\"\\nTotal Batches    : {total_batches_bucket_sorted}\") # Seeing the tqdm stats.\nprint(f\"Padding Tokens   : {padding_tokens_bucket_sorted}\")\nprint(f\"Input Tokens     : {total_tokens_bucket_sorted - padding_tokens_bucket_sorted}\")\nprint(f\"Total Tokens     : {total_tokens_bucket_sorted}\")\nprint(f\"Padding Tokens % : {(padding_tokens_bucket_sorted*100)/total_tokens_bucket_sorted}\")\n\n36390it [08:26, 71.81it/s]\n\n\n\nTotal Batches    : 36390\nPadding Tokens   : 31806252\nInput Tokens     : 119699332\nTotal Tokens     : 151505584\nPadding Tokens % : 20.993451964120347\n\n\n\n\n\nSo we see that the % of total padding tokens has decreased from 67% to 20%. And we can see from the below the distribution of sequence length of batches is close to the distribution of individual sequence lengths. This is different from the case where we made static batches without sorting.\n\n\nCode\nwith plt.style.context('fivethirtyeight'):\n    plt.figure(figsize=[7,5])\n    n, bins, patches = plt.hist(x=batch_lengths_bucket_sorted, bins=50, color='#0504aa',alpha=0.7, rwidth=0.85)\n    plt.grid(axis='y', alpha=0.75)\n    plt.xlabel('Batch Sequence Length',fontsize=15)\n    plt.ylabel('Frequency',fontsize=15)\n    plt.xticks(fontsize=15)\n    plt.yticks(fontsize=15)\n    plt.ylabel('Frequency',fontsize=15)\n    plt.title('Window Sorted Static Batching + Dynamic Padding Length Distribution',fontsize=15)\n    plt.show()"
  },
  {
    "objectID": "posts/efficient-dynamic-batching-of-large-datasets-with-infinibatch/index.html#checkpointing-1",
    "href": "posts/efficient-dynamic-batching-of-large-datasets-with-infinibatch/index.html#checkpointing-1",
    "title": "Efficient Dynamic Batching of Large Datasets with Infinibatch",
    "section": "3. Checkpointing",
    "text": "3. Checkpointing\nOne cool feature of infinibatch is that you can checkpoint a particular state in which the composed iterators is at and restore (rewind?) it back to that state. This is very cool considering it works recursively on the composed iterators and even on infinite iterator. Let’s recreate our iterators and check this out.\n\n\nCode\nsentence_it = datasets.chunked_dataset_iterator(\n    chunk_refs = glob.glob('wikitext-103-chunks/train.*.txt.gz'),\n    read_chunk_fn = read_chunk,\n    buffer_size = 100000, \n    seed = 1337, \n    train=False, \n    shuffle=False\n)\nfeatures_it = iterators.ParallelMapIterator(\n    source_iterator=sentence_it,\n    num_processes=4,\n    num_items_per_process=1000,\n    transform=tokenize_fn\n)\nbatches_it = iterators.BucketedReadaheadBatchIterator(\n    source_iterator=features_it,\n    read_ahead=10000, # Determines the window for the bucket which\n    # will be sorted and  converted to batches.\n    key=lambda example: len(example['input_ids']), # Determines the length used\n    # to sort and choose the longest remaining record.\n    batch_size=lambda longest: tokens_per_batch // len(longest['input_ids']),\n    # Determines the dynamic batch size\n    seed=0 \n)\ncollate_fn = partial(tokenizer.pad, return_tensors='pt')\ntensors_it = iterators.MapIterator(\n    batches_it,\n    transform=collate_fn\n)\n\n\n\ninitial_state = tensors_it.getstate() \nprint(\"Initial State of composed iterators\", initial_state)\n# Draw 5 batches\nbatches = [next(tensors_it) for _ in range(5)]\nprint(f\"Current State after sampling 5 batches: {tensors_it.getstate()}\")\n\n# Reset the Iterator\ntensors_it.setstate(initial_state)\n# Redraw 5 batches\nredraw_batches = [next(tensors_it) for _ in range(5)]\nprint(f\"State after resampling 5 batches: {tensors_it.getstate()}\")\n\n\n# Check equal\nall_equal = True\nfor b1, b2 in zip(batches, redraw_batches):\n    for k in b1:\n        if torch.all(b1[k].eq(b2[k])):\n            continue\n        all_equal = False\n        break\n    if not all_equal:\n        break\nprint(f\"All items drawn after resetting are equal: {all_equal}\")\n\nInitial State of composed iterators {'source_state': None, 'random_state': None, 'num_served': 0}\nCurrent State after sampling 5 batches: {'source_state': {'source_state': None, 'flattened_items_yielded': 0}, 'random_state': (3, (2147483648, 766982754, 497961170, 3952298588, 2331775348, 1811986599, 3100132149, 3188119873, 3937547222, 215718963, 3315684082, 2978012849, 2428261856, 1298227695, 1704729580, 54668373, 3285201915, 3285178464, 1552935063, 988471319, 3135387943, 1691402966, 2757551880, 416056905, 907387413, 1072924981, 33903495, 2168419592, 2429050353, 831159753, 430343641, 3315943586, 1761671042, 864453023, 334804929, 1627478028, 2596811275, 3468733638, 3994375553, 1457139722, 3139722021, 1334790738, 2656639915, 3535811098, 1464315470, 2397423927, 885719490, 1140895889, 3284299483, 2854516462, 2734973817, 147484763, 792049954, 114360641, 3345458839, 1159898878, 1410498733, 2242989638, 453922141, 1344019764, 413870456, 3089405849, 1494382840, 470157779, 4266372830, 2831181573, 1361928602, 1589253513, 1381373062, 753045124, 987032420, 781978839, 2953638767, 3258570111, 3006718191, 1675218601, 1854232715, 3655829819, 1731242722, 2192104666, 1736665161, 740150002, 1195833394, 1610203160, 159492766, 4041488705, 3128952632, 2867295744, 3272632449, 886824304, 1791482600, 221114776, 3867175393, 4020804062, 1077871826, 1298953503, 996366221, 4149754679, 2483052703, 2615558283, 274318093, 1716359450, 4099129961, 1026774175, 288240973, 1459347562, 2365566296, 3690105224, 3065780221, 2050634722, 2652606621, 3185241207, 3026457375, 3456165734, 1880121515, 3398461093, 1795638629, 2379692076, 608668379, 1261955525, 84456522, 1913485156, 106878280, 757183891, 2913957588, 160418091, 2025664758, 141497907, 1657818026, 3053760160, 672193054, 4157546743, 223046484, 1623470498, 1201972930, 675008814, 684162366, 1738776330, 3025656654, 159760723, 1908867305, 3933381342, 2545706671, 467196949, 1427819885, 842150314, 4032903454, 2140851898, 3269883445, 975813755, 4177392955, 1556690684, 2535611513, 462962732, 67591358, 1729610528, 2025206740, 3153739740, 3255032049, 4186226368, 1070144624, 3107867195, 1621006038, 63742485, 835629717, 3189842019, 3950227584, 3184714559, 841836938, 1685394870, 657939920, 766156242, 1412314179, 1048281639, 4037161120, 2044490307, 1923947830, 3900790422, 907554295, 276417304, 860658646, 3574201134, 3508771399, 2110232300, 1636296241, 1405006077, 1093408401, 3243057343, 1519791182, 1994660136, 3829840937, 2644974199, 957955566, 3487641161, 1646922510, 1907939989, 3836029453, 3429168778, 201307778, 72550089, 2464394982, 1695794191, 3344785682, 996786130, 3589457196, 1241754792, 1291082245, 4224603667, 1194379475, 2693491244, 881186965, 2705535111, 445306946, 440274268, 1980827733, 2482488861, 3205215943, 2119332222, 2928713046, 1418736938, 652581136, 2474070665, 2208621536, 4171251876, 2303664214, 443762656, 2981912989, 2199228311, 2652261633, 3166738494, 3443009210, 3498764432, 424010848, 4065487566, 2262993542, 1756076712, 1477098233, 2742171915, 306185806, 3610666541, 923091830, 1034267993, 2336668648, 1880719718, 676878038, 3788797208, 3763351494, 3985428106, 1101865631, 1130501258, 3672967388, 3432003530, 4124438011, 1660392285, 4025484827, 2108074566, 3815409682, 42955331, 3248965569, 1643835718, 1246665668, 1071162194, 3814069229, 115491158, 985096811, 3311029186, 2990827378, 3101633320, 1648574497, 1470117052, 174145027, 2019894819, 2035501481, 459104123, 3507464599, 2093352659, 3369174406, 618767835, 4009895756, 935587447, 3956987426, 33753995, 307782427, 2473424805, 1440371818, 2382619594, 2138695812, 3164510238, 1318650933, 2910086616, 3886677510, 566832801, 3718063320, 1559818704, 183047272, 1142362855, 26306548, 645536402, 3875596208, 2272778168, 3512733409, 1897046338, 38248886, 2570759766, 1806313150, 860304898, 2433450338, 4124013408, 1216634590, 1275388896, 1169566669, 652504502, 761221427, 1448403764, 3129135949, 2513214949, 1269533687, 2413509541, 1226750363, 2450740925, 4094137910, 945759293, 3636927736, 3178020081, 2509964157, 3878869300, 1848504895, 2018369720, 1579755740, 1023627943, 924838836, 2653160914, 1812804174, 1521323076, 4012390528, 1338763317, 2608655937, 16022784, 1672945066, 2177189646, 2944458483, 2213810972, 1369873847, 1224017670, 130901785, 3595066712, 2259115284, 3316038259, 455873927, 2917250465, 3599550610, 1502173758, 684943436, 3079863840, 3144992244, 942855823, 1771140188, 2118780653, 3411494225, 2711180217, 4239611184, 1371891067, 3398566397, 3105518599, 1310665701, 3345178451, 2959821156, 242241789, 2148966880, 3192740583, 404401893, 3605380577, 1446464038, 3920522056, 2577523013, 1079274576, 286634372, 1752710796, 2351075979, 981312309, 3410516352, 3468455736, 1938779182, 1592494371, 1533303080, 88045436, 438252489, 1220512168, 3487004938, 3724852871, 1073434882, 3728218947, 2977555283, 4105408406, 3553772656, 1462006821, 3917158017, 119003006, 3470530198, 3439192457, 2829375771, 3555715155, 32324691, 588735808, 1459221702, 803072782, 2699519868, 1530797005, 79738580, 671990400, 4289511388, 3207115447, 2584684068, 832698998, 760958416, 1217440464, 2517898131, 2418819938, 3629956222, 3445024962, 206619378, 365007395, 522114139, 1707954431, 540423623, 1786750801, 369253262, 4239016754, 147889201, 1637777773, 236798285, 2806120188, 586972608, 2201782716, 1323327827, 819485723, 406078680, 3407345698, 1537169369, 1821691865, 527271655, 3751827102, 1465426495, 3321682429, 2179672664, 401355478, 1068871880, 24609462, 1403522408, 2311580015, 1532058170, 3877815340, 1768430711, 1619755157, 2832904331, 475102697, 354987331, 3295386430, 2816873951, 1039415736, 363972779, 1499307670, 2895506264, 3746345349, 2678027234, 3251899088, 955392878, 2329157295, 1343358773, 309573887, 2410178377, 2843173466, 361132917, 1755816798, 1319204283, 609284796, 1998842567, 1892325921, 223190385, 1483015769, 2876023365, 3876009312, 3199738344, 491524099, 160383137, 1219178873, 3870310498, 1114580266, 4279604166, 855339774, 1983818547, 2297848784, 4118592947, 4084409863, 2225095054, 4215601993, 946447434, 4205503762, 146088676, 778046685, 1876936928, 3157333726, 2173097090, 3215738813, 4135448234, 1219619643, 1936128689, 2897130162, 3336043946, 3779039524, 4200886837, 1359380925, 3402593091, 3140713935, 50855190, 3122065768, 1501584468, 2512255124, 687125154, 2666013386, 837819715, 3057258172, 3653455791, 2868624990, 322131992, 42534870, 4036564806, 798099710, 3533853670, 190914037, 3726947981, 2601169403, 602059656, 1365668439, 1918780004, 394790500, 277566007, 3891847777, 3365421094, 3139612253, 1380519090, 1183088424, 4203794803, 3049949521, 4214159484, 3446206962, 1875544460, 3207220027, 3288287026, 913535288, 178159620, 1410694581, 4190575040, 880731713, 1427805121, 404869072, 3413191414, 2865934056, 2899472677, 4239222733, 688404529, 3923323887, 933651074, 1199453686, 642723732, 2850614853, 3104368451, 3054041024, 3129913503, 2805843726, 1829781129, 3479062313, 650272704, 4224852052, 4085038685, 2616580676, 1793860711, 585126334, 2995262791, 520446536, 3855655015, 1571815563, 2240778227, 2051010344, 1694977983, 788402852, 1988089041, 2035558649, 1800063056, 1234412692, 2490862867, 417320514, 2415019489, 3374117797, 136034611, 898704236, 1247106941, 3923519397, 3563607190, 2454738671, 3522360389, 2672645476, 146828884, 3985140042, 4233949333, 1184742586, 860278824, 2815489967, 983483427, 3190081845, 3288865305, 3575181235, 1292151129, 4007823805, 4049420597, 3499391972, 1611182906, 1721268432, 2944249577, 2487212557, 789127738, 4027610014, 1057334138, 2902720905, 624), None), 'num_served': 5}\nState after resampling 5 batches: {'source_state': {'source_state': None, 'flattened_items_yielded': 0}, 'random_state': (3, (2147483648, 766982754, 497961170, 3952298588, 2331775348, 1811986599, 3100132149, 3188119873, 3937547222, 215718963, 3315684082, 2978012849, 2428261856, 1298227695, 1704729580, 54668373, 3285201915, 3285178464, 1552935063, 988471319, 3135387943, 1691402966, 2757551880, 416056905, 907387413, 1072924981, 33903495, 2168419592, 2429050353, 831159753, 430343641, 3315943586, 1761671042, 864453023, 334804929, 1627478028, 2596811275, 3468733638, 3994375553, 1457139722, 3139722021, 1334790738, 2656639915, 3535811098, 1464315470, 2397423927, 885719490, 1140895889, 3284299483, 2854516462, 2734973817, 147484763, 792049954, 114360641, 3345458839, 1159898878, 1410498733, 2242989638, 453922141, 1344019764, 413870456, 3089405849, 1494382840, 470157779, 4266372830, 2831181573, 1361928602, 1589253513, 1381373062, 753045124, 987032420, 781978839, 2953638767, 3258570111, 3006718191, 1675218601, 1854232715, 3655829819, 1731242722, 2192104666, 1736665161, 740150002, 1195833394, 1610203160, 159492766, 4041488705, 3128952632, 2867295744, 3272632449, 886824304, 1791482600, 221114776, 3867175393, 4020804062, 1077871826, 1298953503, 996366221, 4149754679, 2483052703, 2615558283, 274318093, 1716359450, 4099129961, 1026774175, 288240973, 1459347562, 2365566296, 3690105224, 3065780221, 2050634722, 2652606621, 3185241207, 3026457375, 3456165734, 1880121515, 3398461093, 1795638629, 2379692076, 608668379, 1261955525, 84456522, 1913485156, 106878280, 757183891, 2913957588, 160418091, 2025664758, 141497907, 1657818026, 3053760160, 672193054, 4157546743, 223046484, 1623470498, 1201972930, 675008814, 684162366, 1738776330, 3025656654, 159760723, 1908867305, 3933381342, 2545706671, 467196949, 1427819885, 842150314, 4032903454, 2140851898, 3269883445, 975813755, 4177392955, 1556690684, 2535611513, 462962732, 67591358, 1729610528, 2025206740, 3153739740, 3255032049, 4186226368, 1070144624, 3107867195, 1621006038, 63742485, 835629717, 3189842019, 3950227584, 3184714559, 841836938, 1685394870, 657939920, 766156242, 1412314179, 1048281639, 4037161120, 2044490307, 1923947830, 3900790422, 907554295, 276417304, 860658646, 3574201134, 3508771399, 2110232300, 1636296241, 1405006077, 1093408401, 3243057343, 1519791182, 1994660136, 3829840937, 2644974199, 957955566, 3487641161, 1646922510, 1907939989, 3836029453, 3429168778, 201307778, 72550089, 2464394982, 1695794191, 3344785682, 996786130, 3589457196, 1241754792, 1291082245, 4224603667, 1194379475, 2693491244, 881186965, 2705535111, 445306946, 440274268, 1980827733, 2482488861, 3205215943, 2119332222, 2928713046, 1418736938, 652581136, 2474070665, 2208621536, 4171251876, 2303664214, 443762656, 2981912989, 2199228311, 2652261633, 3166738494, 3443009210, 3498764432, 424010848, 4065487566, 2262993542, 1756076712, 1477098233, 2742171915, 306185806, 3610666541, 923091830, 1034267993, 2336668648, 1880719718, 676878038, 3788797208, 3763351494, 3985428106, 1101865631, 1130501258, 3672967388, 3432003530, 4124438011, 1660392285, 4025484827, 2108074566, 3815409682, 42955331, 3248965569, 1643835718, 1246665668, 1071162194, 3814069229, 115491158, 985096811, 3311029186, 2990827378, 3101633320, 1648574497, 1470117052, 174145027, 2019894819, 2035501481, 459104123, 3507464599, 2093352659, 3369174406, 618767835, 4009895756, 935587447, 3956987426, 33753995, 307782427, 2473424805, 1440371818, 2382619594, 2138695812, 3164510238, 1318650933, 2910086616, 3886677510, 566832801, 3718063320, 1559818704, 183047272, 1142362855, 26306548, 645536402, 3875596208, 2272778168, 3512733409, 1897046338, 38248886, 2570759766, 1806313150, 860304898, 2433450338, 4124013408, 1216634590, 1275388896, 1169566669, 652504502, 761221427, 1448403764, 3129135949, 2513214949, 1269533687, 2413509541, 1226750363, 2450740925, 4094137910, 945759293, 3636927736, 3178020081, 2509964157, 3878869300, 1848504895, 2018369720, 1579755740, 1023627943, 924838836, 2653160914, 1812804174, 1521323076, 4012390528, 1338763317, 2608655937, 16022784, 1672945066, 2177189646, 2944458483, 2213810972, 1369873847, 1224017670, 130901785, 3595066712, 2259115284, 3316038259, 455873927, 2917250465, 3599550610, 1502173758, 684943436, 3079863840, 3144992244, 942855823, 1771140188, 2118780653, 3411494225, 2711180217, 4239611184, 1371891067, 3398566397, 3105518599, 1310665701, 3345178451, 2959821156, 242241789, 2148966880, 3192740583, 404401893, 3605380577, 1446464038, 3920522056, 2577523013, 1079274576, 286634372, 1752710796, 2351075979, 981312309, 3410516352, 3468455736, 1938779182, 1592494371, 1533303080, 88045436, 438252489, 1220512168, 3487004938, 3724852871, 1073434882, 3728218947, 2977555283, 4105408406, 3553772656, 1462006821, 3917158017, 119003006, 3470530198, 3439192457, 2829375771, 3555715155, 32324691, 588735808, 1459221702, 803072782, 2699519868, 1530797005, 79738580, 671990400, 4289511388, 3207115447, 2584684068, 832698998, 760958416, 1217440464, 2517898131, 2418819938, 3629956222, 3445024962, 206619378, 365007395, 522114139, 1707954431, 540423623, 1786750801, 369253262, 4239016754, 147889201, 1637777773, 236798285, 2806120188, 586972608, 2201782716, 1323327827, 819485723, 406078680, 3407345698, 1537169369, 1821691865, 527271655, 3751827102, 1465426495, 3321682429, 2179672664, 401355478, 1068871880, 24609462, 1403522408, 2311580015, 1532058170, 3877815340, 1768430711, 1619755157, 2832904331, 475102697, 354987331, 3295386430, 2816873951, 1039415736, 363972779, 1499307670, 2895506264, 3746345349, 2678027234, 3251899088, 955392878, 2329157295, 1343358773, 309573887, 2410178377, 2843173466, 361132917, 1755816798, 1319204283, 609284796, 1998842567, 1892325921, 223190385, 1483015769, 2876023365, 3876009312, 3199738344, 491524099, 160383137, 1219178873, 3870310498, 1114580266, 4279604166, 855339774, 1983818547, 2297848784, 4118592947, 4084409863, 2225095054, 4215601993, 946447434, 4205503762, 146088676, 778046685, 1876936928, 3157333726, 2173097090, 3215738813, 4135448234, 1219619643, 1936128689, 2897130162, 3336043946, 3779039524, 4200886837, 1359380925, 3402593091, 3140713935, 50855190, 3122065768, 1501584468, 2512255124, 687125154, 2666013386, 837819715, 3057258172, 3653455791, 2868624990, 322131992, 42534870, 4036564806, 798099710, 3533853670, 190914037, 3726947981, 2601169403, 602059656, 1365668439, 1918780004, 394790500, 277566007, 3891847777, 3365421094, 3139612253, 1380519090, 1183088424, 4203794803, 3049949521, 4214159484, 3446206962, 1875544460, 3207220027, 3288287026, 913535288, 178159620, 1410694581, 4190575040, 880731713, 1427805121, 404869072, 3413191414, 2865934056, 2899472677, 4239222733, 688404529, 3923323887, 933651074, 1199453686, 642723732, 2850614853, 3104368451, 3054041024, 3129913503, 2805843726, 1829781129, 3479062313, 650272704, 4224852052, 4085038685, 2616580676, 1793860711, 585126334, 2995262791, 520446536, 3855655015, 1571815563, 2240778227, 2051010344, 1694977983, 788402852, 1988089041, 2035558649, 1800063056, 1234412692, 2490862867, 417320514, 2415019489, 3374117797, 136034611, 898704236, 1247106941, 3923519397, 3563607190, 2454738671, 3522360389, 2672645476, 146828884, 3985140042, 4233949333, 1184742586, 860278824, 2815489967, 983483427, 3190081845, 3288865305, 3575181235, 1292151129, 4007823805, 4049420597, 3499391972, 1611182906, 1721268432, 2944249577, 2487212557, 789127738, 4027610014, 1057334138, 2902720905, 624), None), 'num_served': 5}\nAll items drawn after resetting are equal: True\n\n\nSince the state of the iterator is just a dictionary, you can serialize it along with your model weights and restore them to continue training from exact point where you have checkpointed it."
  },
  {
    "objectID": "posts/new-blog/index.html",
    "href": "posts/new-blog/index.html",
    "title": "Blog using Github pages",
    "section": "",
    "text": "Now you can just write a markdown file, and commit it, or even create a file online even in github’s source browser, and it is automatically generated into a blog post just after the commit is made.\nI didn’t notice this feature in github.com before. I thought we had to run jekyll locally to build static site, and commit it for github to serve stuff. So rather went for python static blog generator called pelican, but gave up because of it being a pain to generate blog every time. I even tried to setup travis-ci to automagically generate the blog, but it was slow and was still a pain.\nSo if you want to create a blog, just use github , it is quite easy to setup. clone a theme, edit _config.yml and blog on."
  }
]