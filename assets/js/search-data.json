{
  
    
        "post0": {
            "title": "Roam Research - Software for building a Second Brain",
            "content": "Introduction . - ![](https://roamresearch.com/assets/images/Roam-Group-min.png) - [Roam Research](roamresearch.com) is a revolutionary note-taking/knowledge management software. It is designed with the idea that data structure for a second brain should be associative (graph) rather than a rigid hierarchy. - It is meant for everyone who needs to manage their knowledge effectively. Their founder aims for the product to become more or less excel for knowledge management. - Professor Balaji Srinivas introduced it to me. I have been using this for about a month and fell in love with its features. - Canadian philosopher who predicted the web tells that &quot;Medium is the message&quot;. The properties of a medium in which communication occurs can impact society a lot. I believe that roam is one such tool that shifts the medium of note-taking in a groundbreaking manner. - The features of Roam allow you to arrange for the serendipity of ideas, unexpected connections with your past, present and future selves. - It is built on simple building blocks that come together (emergent property) to make the whole greater than the sum of its parts. Each building block of Roam might look simple if looked separately, but together they become very powerful. - Note: It is still in beta with a pricing yet to be announced. But if you try it I think you will share this sentiment. ![](https://scaledynamix.com/wp-content/uploads/2018/08/takemymoney.jpg) - Here is the [Roam white paper](https://roamresearch.com/#/v8/help/page/Vu1MmjinS) written by founders on a Public roam database about why they think Roam is revolutionary. # Pain points of most knowledge-management software - I have used popular note-taking software like evernote, One-note, Zoho notebook etc. There was always huge friction with these tools. Getting them to work for Note taking, journaling, and project management was such a pain. - The only tool that came close was org-mode in Emacs. I used it for the past one year. But even it had a lot of friction due to the reasons I will expand upon below. And emacs is most definitely not for popular use, it is only for the chosen few who are blessed enough to reject the cult of mouse. ## Failure of Files inside Folders way of organization. - Most note-taking software or even physical note-taking follow a file-folder system or a single hierarchy of bullets inside bullets (Outlining tools) for organizing notes. I will refer to this as &quot;files inside folder&quot; model going forward now. - This system creates nested hierarchy of categories, sub-categories and so on. Inside which your notes are put. - ### **Why this approach fails?** - You might have started taking notes/journaling etc for a few days and abandon it after some time. The following are the main reasons I think this happens. - **Friction caused by a static hierarchy of the folder system** - Every time you want to write a note you have to decide where to put it in the hierarchy. You have to ask yourselves which notebook/folder/file should I write this, for it to be useful? - **Poor Return in investment for good note-taking** - You painstakingly take notes or journal your exercise regime or note down something. But it never surfaces automatically when you write something related. - Most notes are passive and useless unless you look for them. - Most notes are hidden uselessly in the hierarchy where you put it in. ### Practical Scenario where folder model fails - Say you had a discussion with your friend about note-taking while sipping a coffee in a cafe. - You talk about some personal stuff which say you want to put in your journal. - S/he brings about about a new book which you find interesting and want to read. - Also, you really like the coffee in the cafe and want to note it down in your list of the favourite coffee shops. - **Now how do you take notes in this case?** - In a rigid hierarchy, for this note to be useful you have to file it under multiple places - &quot;To read books&quot;, &quot;Daily Journal&quot;, &quot;Meets&quot;, &quot;Friend&#39;s name&quot; - But that is highly impractical because of the effort/redundancy involved. You would have to copy paste same information in multiple places or manually create links in multiple pages. And if those pages don&#39;t exist you would have to create them. ## Simple Tags don&#39;t solve this problem - Most note-taking software provide tagging to solve this problem. - But this creates friction of adding tags to everything you write. - And more importantly the tags are flat. i.e. They have no hierarchy between them. - Tags solve searching for notes, but not disovery. This makes it problematic if you want to discover notes when you stumble through a specific context automatically. ## We think in a associative (Graphs) manner not as a rigid hierarchy (Trees) - Every thought/idea in our brain has a bunch of associations. - Associations like people who introduced us to the idea, what we want to do with it, associations with books we read about it, tasks we completed based on it, tasks we want to do, date in which we did it etc. - So this forms a &quot;graph&quot; (in computer science) where every idea is a node and is linked to many other ideas. - When you think of an idea naturally you get reminded of the stuff it is associated with. - But the problem is we forget stuff, which is the reason why we are doing note-taking in the first place. - **What if a note-taking software allows you to mirror how the brain works? ** # Building blocks of Roam that differentiate it - These are the fundamental set of features that make Roam what it is. Since showing is better than telling, I am including 1-minute videos of how each fundamental building blocks of Roam work. ## Basic Layout - &lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube-nocookie.com/embed/3SwQ4usbCX4&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen&gt;&lt;/iframe&gt; - Roam&#39;s page layout is like any other &quot;outlining&quot; tool. ie It has bullets which can be nested within each other infinitely and the bullets can be collapsed. - There is no folder system. All notes are be seen from &quot;All Pages&quot; view in left side-bar, but it is rarely needed because of way roam&#39;s navigation is organized. - Roam&#39;s home page is the Daily notes page where you can see pages titled by dates. This is the basic dumping ground for all your quick note-taking and journaling, daily tasks, habit tracking for the day. ## Friction Free Link Creation - &lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube-nocookie.com/embed/lHkMq3aqDtw&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen&gt;&lt;/iframe&gt; - Basic way roam allows linking between pages is typing the note title inside two square brackets &quot;[[]]&quot; which includes autocomplete/search to all notes page titles. - One Important thing here is if the note page doesn&#39;t exist a new page gets create. This would seem weird when coming from other applications. But it serves a big purpose you will see next. ## Bi-directional Linking - &lt;iframe width=&quot;854&quot; height=&quot;480&quot; src=&quot;https://www.youtube.com/embed/v9s3pusI1JQ&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen&gt;&lt;/iframe&gt; - This is the key feature of Roam. When a page is linked to, when you visit the page you can see all the places where it has been referred. - Examples: - Say you write &quot;I was reading this cool article on [[Deep Learning]]&quot; - The Deep Learning page will have a back link to all the places it has been mentioned. - So every page becomes akin to a tag, but associations between them form a dynamic/organic hierarchy. - &gt; &quot;Every page is a tag, and every tag is a page&quot; - [Nat Elison&#39;s blog on Roam](https://www.nateliason.com/blog/roam) - ## Un-linked References - &lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube-nocookie.com/embed/nROryUttSr0&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen&gt;&lt;/iframe&gt; - If you have already mentioned a topic in lots of notes, but didn&#39;t create a page for it and link them. Roam has got your back with its super cool un-linked references. When you create a new page, you can easily bulk link every other page that has mentions of the current page. ## Ability to refer or embed any block/bullet anywhere - &lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube-nocookie.com/embed/ZFbrdv-70ME&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen&gt;&lt;/iframe&gt; - You can link to or embed any block written anywhere in roam notes. - Roam prompts a search/autocomplete to any block you have written in all the pages. How awesome is that? - To create a block reference two open Parentheses `((` Or type `/Block Reference.` - You can also embed the entire block using Block embed. - Type `/Block Embed` - When embedding or linking to a block you can see the places it has been used by clicking on the number which appears at top right side of a block. ## All back-links and block embeds are editable - &lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube-nocookie.com/embed/qg9uS6LlCf0&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen&gt;&lt;/iframe&gt; - The coolest part of roam is all blocks (bullets) displayed by back-links and block embeds are editable. - You can edit embedded notes and back-links with no duplication. - So you easily remix (refactor?) notes by creating a new note with just embeds from multiple other notes in other places. ## Navigating with Full-text Search - &lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube-nocookie.com/embed/Al69VbgKVw0&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen&gt;&lt;/iframe&gt; - Roam provides full text search of all the blocks and titles. - You can create a brand new page which is not linked to any other page directly from search. ## Graph View - &lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube-nocookie.com/embed/OXqN4u7lKac&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen&gt;&lt;/iframe&gt; - Since every note is basically a node in graph, roam easily allows a bird&#39;s eye of your entire graph in the &quot;Graph Overview&quot; page. - A more helpful feature is ability to view what nodes current page has connections to and navigate visually. ## Side Bar - &lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube-nocookie.com/embed/7dASSNABtIo&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen&gt;&lt;/iframe&gt; - Roam sidebar allows you to open multiple notes at a time. - This is really useful when you want to aggregate knowledge across notes. ## Filters on Bi-directional Links - &lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube-nocookie.com/embed/BnwWdTnXlxU&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen&gt;&lt;/iframe&gt; - When you have too many bi-directional links in a page, you can filter them to include or exclude other links. # Building a second brain with Roam - In this section, I will describe how Roam can be used for multiple use-cases at once thereby building your second brain. ## Note taking - If you read a lot and want to retain the knowledge. Reading stuff and writing it in your own words will be a good way to test gaps in understanding. - This [article](https://www.nateliason.com/blog/smart-notes) a effective way to take smart notes using roam. - I found [this summary](https://fortelabs.co/blog/how-to-take-smart-notes) of a book called &quot;[[How to Take Smart Notes: One Simple Technique to Boost Writing, Learning and Thinking ‚Äì for Students, Academics and Nonfiction Book Writers]] really helpful. ## Journal - Writing a journal is a bread and (peanut) butter of roam. - Daily notes encourages you to write daily at anytime. - It feels really encouraging to journal in roam as unlike other apps because of the back-links. - Anything recorded will automatically get associated with all the topics. ## Task management - &lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube-nocookie.com/embed/asQ4RSjjCu4&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen&gt;&lt;/iframe&gt; - Roam supports basic todos, you can use links to link tasks to the date it has to be done using date-picker. - The key advantage here is your project management tasks can easily be linked to the meeting notes, research notes, and journal etc. - Getting things done (GTD) is a popular method to manage tasks. - It is very easy to implement that in roam with the aid of back-links. - You can read how to adopt GTD in roam [here](https://oliverschmid.space/posts/gtd-in-roam/). ## Bookmarks - &lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube-nocookie.com/embed/7b2AVCZOMnw&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen&gt;&lt;/iframe&gt; - You can easily put links into roam with creating tags/links. - Tags are same as &quot;[[]]&quot; links, just that the font is greyed out. ## Personal CRM - Personal crm is for maintaining a list of people, their contact, birthdays, how you met them or anything else you want to maintain about them. - In roam you can easily create pages for people, and refer it in your daily notes. So when you go to the person&#39;s page, you can see all the places h/she has been mentioned. ## Content Creation - For writing new content, you can easily remix stuff which you wrote across different pages in a new page. - So you will never have the feeling of starting at a blank page when you have done your research and taken notes on it. # Other useful features ## Query - &lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube-nocookie.com/embed/AlmhG6nTl9M&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen&gt;&lt;/iframe&gt; - This is an advanced feature, where you can query on your graph to show blocks that satisfy boolean conditions. Like show me all the blocks with todos with high priority. etc ## Shortcuts - ![](https://nimbus-screenshots.s3.amazonaws.com/s/823247391734eda9aec6dd9353f33a2b.png) - This is useful to keep the most important projects ## Tables, Diagrams, Kanban boards - Tables can be created using `/Table` command followed by nested bullets. - ![](https://nimbus-screenshots.s3.amazonaws.com/s/e3e5189d83b0ab1d85e5debe2d0d7207.png) ## Embedding Media - Embed tweet by just pasting a twitter link. - Type backslash followed by image markdown to insert a markdown for image embed. `/Image Markdown` - Alternatively images can be uploaded by pasting directly or with backslash command `/Upload a image` backslash command for it to be uploaded and the markdown inserted automatically. - For youtube videos, use `/Embed Youtube Video` command. - Interesting point here is all the embeds are markdown or markdown like plain text syntax. No proprietary garbage. ## Publishing your Roam Notes - You can share a note by its URL to public as read-only or even allow public edits. ## Sharing your second Brain to form a Hive mind - Currently you can share the entire roam database and collabrate with peers. - More fine-grained controls of sharing parts of the graph are in the works. ## Works offline (Progressive Web App) - The mobile apps are coming shortly, but the web app is designed so well that it can function offline. ## Exports to plain text - This is important for anyone who cares about not getting locked out of your data. - Since the product is in development better to take regular backups if you plan to use it. # Few of other thoughts about Roam - The concept of bi-directional editable media in Roam can be transposed to many other products with lots of unstructured text trapped in a rigid hierarchy. - Roam is a well-designed innovative product that leapfrogs over existing products. This I think is because of unorthodox thinking of its founders ([Conor White-Sulivan](https://twitter.com/Conaw) and Joshua Brown) to build useful tools for augmenting our brain. - Turing award winner and Computer Science legend Alan Kay once said: &quot;The best way to predict the future is to invent it.&quot;. He also thought that there is a lot more that can be done to make computers truly augmenting the intellect. I think these founders are trying to fulfill that dream. - Roam seems like an application that can give compounding returns of value for the knowledge put in it. This is totally refreshing in an age where apps suck your attention. - Finally, I am more into engineering than content writing for productivity software. This software was so good that I couldn&#39;t help writing this article for sharing it. .",
            "url": "https://saiprasanna.in/posts/roam-research-software-for-building-a-second-brain/",
            "relUrl": "/posts/roam-research-software-for-building-a-second-brain/",
            "date": " ‚Ä¢ Mar 25, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Neural Module Networks",
            "content": "Research Paper - https://arxiv.org/abs/1511.02799 . Authors - Jacob Andreas, Marcus Rohrbach, Trevor Darrell, Dan Klein . Key Idea . Parse questions of visual QA into a description of compositions of functions. These functions are neural networks called Neural Modules. Execute the neural networks and reweigh the resulting label using question representation. . . Task - Visual Question Answering . Given a question like ‚ÄúWhat color is the coffee mug?‚Äù and an image we want to predict the answer. . . Prior approaches . End to End neural networks Use a CNN to vectorize the image and RNN to vectorize the question and use a feed forward network to classify the answer. | This is a black box trying to answer in one shot. | . | Semantic Parsing approach Parse the question into logical expressions, image into logical representation of the world and use logic based reasoning to solve the problem. | This is more compositional. | . | . Motivation . Combine the representational capacity of neural nets and compositionality of symbolic approach. | So, ‚ÄúRather than thinking of question answering as a problem of learning a single function to map from questions and contexts to answers, it‚Äôs perhaps useful to think of it as a highly-multitask learning setting, where each problem instance is associated with a novel task, and the identity of that task is expressed only noisily in language.‚Äù | Simple example - ‚ÄúIs this a truck?‚Äù - Needs single task to be performed, namely truck or not classification. | Compositional example - ‚ÄúWhat is the object to the left of the tea pot?‚Äù - Needs one to find the teapot, detect object to its left, then classify the object. | . Architecture . Neural Modules . Identify set of modules that can be composed to solve all/most tasks. | Modules can be thought of as a function parametrized by a neural network, with a type signature. | Data Types - Image, Unnormalized attention map, labels | | | | | . | . Strings -&gt; Modules . Parsing Use few rules on dependency parse of the question to convert it into a structured query. | e.g. ‚ÄúIs there a circle next to a square?‚Äù -&gt; is(circle, next-to(square)) | . | Layout ‚ÄúAll leaves become attend modules, all internal nodes become re-attend or combine modules dependent on their arity, and root nodes become measure modules for yes/no questions and classify modules for all other question types.‚Äù The queries could come from anywhere not just natural language question. As long as they can be converted to a layout in the end. | . | . | . Answering . An RNN is used to process the question and predict a label directly without looking into the image. | This is combined with the final label from the root node of the Neural Modules using geometric mean to get the final result. | This is done for 2 reasons Syntactic Regularity/Prior When converting to structured query, certain syntactic elements are lost. | For e.g. What is in the sky? and What are in the sky? both result in what(fly). | But answer varies from kite to kites. | . | Semantic Regularity/Prior Some answers are unreasonable just by inspecting the question. For example, What colour is the bear? eliminates all non-colour answers. | . | . | . | . Benchmarks . They try this in vqa dataset - https://visualqa.org/ a huge dataset with natural images and questions with answers. | . | Since VQA doesn‚Äôt have many deep compositional questions, they use shapes a synthetically generated dataset. | . | . Examples . What colour is his tie? | | .",
            "url": "https://saiprasanna.in/posts/neural-module-networks/",
            "relUrl": "/posts/neural-module-networks/",
            "date": " ‚Ä¢ Mar 2, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "ACL 2019 Conference Summary",
            "content": "My colleague Ananda and I attended ACL 2019 conference at the enchanting city of Florence. All the accepted papers can be accessed here. Here‚Äôs the summary of interesting trends and also specific research work that caught my eye at the conference. A note of thanks to my employer at Zoho for sponsoring us to attend. . I wrote this summary an many months ago and forgot posting it. Better late than never I guess. . Grammatical Error Correction . Among the ACL workshops, Building Educational Applications (BEA) Workshop had a Grammar Error Correction competition. | . The system description papers for this competition were presented as posters in the conference. . Three tracks were present in the competition. Restricted track - Only organizer provided human labelled parallel (error and corrected sentence pairs) data can be used. (No restriction on synthetic data) Unrestricted track - Any data including private data can be used. Low Resource track - No human labelled data can be used. | Interestingly, the winning team (Edinburgh + Microsoft)‚Äôs submission for Track 1 also beat Track 2 without using additional restricted data. | Synthetic data generated by corrupting good grammatical sentences from news, books and wikipedia are the techniques used overall by top performing teams. | . Multi-Lingual Models . MultiLingual models is a hot area of research now. Earlier results where using single model to perform tasks on multiple languages has shown promising results. . Lots of papers on multi-lingual shared models were presented. | Paper - Choosing Transfer Languages for Cross-Lingual Learning | . Rise of Automated Metrics . Until recently, we compare model outputs with human written sentences for translation, summarization etc. This can artificially penalize models that generate sentences with equivalent meaning but not same words. There are couple of papers that train models to score quality of the output. Then use these model scores as reward for reinforcement learning. (FYI reinforcement learning is only used for fine tuning, none of the seq2seq models can be trained from scratch using it) . Paper - This Email Could Save Your Life: Introducing the Task of Email Subject Line Generation This paper uses automated score instead of typical NGram match (ROUGE) score for summarization task. | Paper - Beyond BLEU:Training Neural Machine Translation with Semantic Similarity | Paper - Sentence Mover‚Äôs Similarity: Automatic Evaluation for Multi-Sentence Texts | . Statistical Evaluation . If we have two architectures and couple of datasets, how to say empirically one is better than the other? Few questions are how to compare two models on the same dataset, across multiple datasets, across various hyperparameter configurations. Problems in applying frequentist tests on the metrics such as accuracy, f1-score etc are that assumptions such as Independent and Identically distributed (IID) cannot be made for deep learning datasets. So we cannot assume that the score the model gets in one dataset is ‚Äúindependent‚Äù of the score on another dataset. Statistical tests that don‚Äôt assume underlying distribution are needed. Recent statistical methods/tests to do so are being developed and some were presented at the conference. . Paper - Deep Dominance - How to Properly Compare Deep Neural Models | Paper - Bayes Test of Precision, Recall, and F1 Measure for Comparison of Two Natural Language Processing Models | . Bayesian Methods . Attended a very detailed tutorial on it. The presenter has summarized the evolution of research in this area and the current papers. Here‚Äôs link to the detailed slides for fellow Bayesians. | . Analyzing Neural Nets and Interpretability . There is an entire sub-fields of research into analyzing and interpreting neural networks. . BERTology . ‚ÄúBERT-ology‚Äù papers that explore what linguistic structures do pre-trained models like BERT learn. . Paper - What Does BERT Look at? An Analysis of BERT‚Äôs Attention | . BlackBoxNLP Workshop . An entire workshop devoted for analyzing what Neural Networks learn. . Paper - On the Realization of Compositionality in Neural Networks Interesting paper studying what is required for neural models to compose two very trivial functions. | Paper - GEval: Tool for Debugging NLP Datasets and Models | . Formal Languages Workshop . An entire small workshop devoted to finding what Formal Languages (Finite state Automata, etc) neural networks can learn. e.g. Can we reduce a RNN to Weighted Finite State Machine (which is far more interpretable, amenable to theory etc). Although this area sounds exciting to me, I was unable to attend it as I was in an another workshop. Slides from talk of Noah Smith‚Äôs talk on Rational Recurrences at this workshop. . Neuroscience and NLP . Neuroscience labs have started to use deep learning. An interesting conjunction of research in NLP and neuroscience research in correlating ANN representations with brain signals was presented. . Paper - Relating Simple Sentence Representations in Deep Neural Networks and the Brain The researchers try to find relationship between deep learning language representations and brain signals. Paper of interest is where they predict neural brain patterns using pre-trained ANN models like BERT. | . Language Emergence in Multi-Agent systems . In this frontier, people try train models to solve some task by communicating symbols. Researchers analyze the properties of language used by the agents to solve the task and how it compares with properties of human language. . Paper - Word-order Biases in Deep-agent Emergent Communication | . Conversational AI . Neural Models for selecting conversation from past history, detecting intent and slot fitting are all increasingly being deployed by companies. | PolyAI (a startup at Singapore shipping conversational AI) shared three interesting papers. Their slides are also interesting. | On a related note, Baidu has is doing impressive research and engineering on meeting transcription. They have a stack that does speech to text, translating the text as its spoken (a problem that needed separate research as the text would be incomplete), detecting english phrases being spoken (code switching) and then NLP over the transcribed text. | . Translation . Lots of new work on adapting translation models for low-resource languages. | Unsupervised translation, Multi-lingual translation models are few areas of research. | Unbabel a YC funded startup doing translation systems shared lots of interesting and important results. Slides from their talk. This company employs a hybrid system where human translators do ‚Äúpost-edits‚Äù on machine translations. And some of their system work in real-time. | . Contextual Search using Neural Representations at scale . This paper has demonstrated a system which does dense vector search on entire wikipedia for open domain QA. . Scaling search on neural vectors to do question answering on entire wikipedia on CPU - https://github.com/uwnlp/denspi . Demo - http://allgood.cs.washington.edu:15001/ .",
            "url": "https://saiprasanna.in/posts/ACL-2019-Conference-Summary/",
            "relUrl": "/posts/ACL-2019-Conference-Summary/",
            "date": " ‚Ä¢ Feb 10, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Semantic Legion",
            "content": "I am guilty of spamming people in the degree one of my network with too many links in topics that fancy the Legion of varied interests that haunt me. Following the suggestion of Ananda Seelan, I am consolidating my link blasts into a considated blog post format, thus begins the ‚ÄúSemantic Legion‚Äù. This exercise might help organize the ‚ÄúLegion‚Äù in my head and maybe lead to more focused blog posts. . ‚ÄúMy name is Legion, for we are many.‚Äù . Machine Learning . The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks . The lottery ticket hypothesis suggests that big Deep Neural Nets train better than smaller nets because they get lucky. Essentially like someone who has purchased more number of lottery tickets. . Prune a large neural network by zeroing the bottom x% of weights by magnitude. This can be done one shot or iteratively while training. | Reset the obtained subnetwork weights to the exact weights you randomly intialized before training the large neural network. | The pruned subnetwork converges to similar test error rate as the full network or even better in the same number of epochs. | The authors notice that if you were try some other initialization for the subnetwork or even sample from similar distribution it doesn‚Äôt work. Hence they hypothesise that the larger network essentially got lucky. | Though the subnetwork is smaller, computations will need sparse matrix multiplication optimizations to be faster. . Understanding the generalization of ‚Äòlottery tickets‚Äô in neural networks . Facebook extends the study and checks it for various architectures, tasks and optimizer setting. The lottery ticket phenomena seems to occur in most places. The lottery ticket subnetworks generalize across datasets. This blog post is a summary of multiple papers by Facebook AI group in analyzing this phenomena. . New Theory Cracks Open the Black Box of Deep Learning . Information bottleneck theory a hypothesis about how neural nets learn is creating some buzz. One of the claims is that the output of earlier layers have more mutual information with the inputs while final layer outputs have more mutual information with the outputs than the inputs. The information about input gets compressed in each layer. . . Lucy Reading-Ikkanda/Quanta Magazine; adapted from arXiv:1703.00810 [cs.LG] . Evolution of Representations in the Transformer . This is a great practical example of using information bottlenecks to analyze neural nets behaviour. This research (accompanied by inspirationally well written blog post) compares the evolution of representations in three different NLP encoder models. And in part explains some empirical findings such as why de-noising objective works better than casual language model objective or encoders from translation objective for transfer learning. . Universal Adversarial Triggers for Attacking and Analyzing NLP (Wallace et al. EMNLP 19) . This paper finds magic spells that make your NLP models malfunction. They find phrases that cause a specific model prediction when concatenated to ùò¢ùòØùò∫ input from a dataset. These phrases are reported to work across architectures for the same dataset. . Triggers cause: 1. GPT-2 to spew racism 2. SQuAD models to answer &quot;to kill american people&quot; for 72% of questions asking &quot;Why...&quot; 3. Classification models to drop from 90% accuracy to 1% . AllenNLP Interpret . This is a great set of features for interpretability added to AllenNLP library. . We present AllenNLP Interpret, a toolkit built on top of AllenNLP for interactive model interpretations. The toolkit makes it easy to apply gradient-based saliency maps and adversarial attacks to new models, as well as develop new interpretation methods. AllenNLP interpret contains three components: a suite of interpretation techniques applicable to most models, APIs for developing new interpretation methods (e.g., APIs to obtain input gradients), and reusable front-end components for visualizing the interpretation results. . The amazing thing here is with implementing a simple interface in your model predictor allows you to apply a suite of interpretability techniques for our models. . AIDungeon2 is here . This is a real fun application of langauge model generation. Nick Walton has adapted GPT2 to generate user guided ‚ÄúChoose your own‚Äù text RPG type games. Now you can try out anything you fancy by just issuing commands like ‚ÄúCast a spell to Reverse entropy‚Äù. A truly open world RPG with a AI dungeon master. The model weaves your actions to generalte plausible/surreal story continuations. Hacker News discussion about it. The nature of the model make them generate surreal dream like scenarios. There are glaring consistency issues in the generated story lines. This points to a symbolic gap that is yet to be filled. . . Source: aiweirdness.com . Controlling Text Generation with Plug and Play Language Models . On the topic of controlling language models, uber research has found a way to control the generation of models like GPT2 without fine-tuning. . Quantum Computing, Linear Algebra, Tools for Learning . Quantum Computing for the Very Curious . I wanted to try out Micheal Nielsen‚Äôs (of neuralnetworksanddeeplearning.com fame) Quantum computing article. This long-form educational article attempts a unique teaching method by embedding flash cards (anki cards) and reminding readers via email to revisit the cards. I got around doing it at behest of the amzing Professor Balaji (a teacher of mine) who gave this as an exercise to test Linear Algebra understanding. Prior knowledge of the truly abstract nature of linear algebra (basis, linear transformations, linear combinations) really helped me to grok the essay. . The learning approach taken by this article (embedding flash cards + reminders) article shows how computing medium can be extended to augment our understanding. This scratches the surface of Alan Kay‚Äôs vision of computers being tools that extend our mind. . Augmenting Long-term Memory . If you‚Äôre curious about spaced repition flash card approach to learn new math theorems, machine learning concepts etc Micheal has written extensively about it in the above link. . Anki Flash Cards with Spaced Repitition . The free app Anki is example of good software aimed at expanding our capabilites rather than popular objective of draining attention. It has web, desktop and mobile versions for creating Anki (flash) cards with spaced repitition tracking. I am in the process of adopting it for my learning. Not yet successful in integrating it fully, will blog more about my experience in future. . Polar App . Related learning tool I found is Polar. . ‚ÄúA powerful document manager for web pages, textbooks, PDFs, and anything you want to read. Supports tagging, annotation, highlighting and keeps track of your reading progress.‚Äù . It doesn‚Äôt have a firefox extension yet. But it allows creating anki cards that can be synced to Anki app from web highlights. This helps in creating a learning expereince like the quantum computing blog for any document. . Philosophy . Would aliens understand lambda calculus? . Platonism vs Aristotelianism is an age old debate in philosophy. Professor Balaji (a teacher of mine) had a strong notion that the current mathematics we have is strongly influenced by our spatio-visual sense. Stumbled upon the above post which makes similar claims. It claims that certain cognitive priors are necessary to converge upon ideas which some consider as universal. . I don‚Äôt know enough to lean on any side of the debate heavily. But my intution lies with universality/platonism of physics, mathematics and computatability. I think even if Alien‚Äôs use some other metaphors to arrive at Lambda Calculus, the underlying notion of universal computability (if correct) will be the same. . New AI Strategy Mimics How Brains Learn to Smell . I am now exploring search systems over neural net generated representation (vector spaces). This generally involves approximate methods such as Locality senstive Hashing. The method described in this post was interestingly derived from the sense of smell of fruit-flies. This lends some weight top the notion that our cognitive reliance on certain senses (vision) makes some ideas intutive, but exploring outside it can expand our horizons. (Purely my speculation to be taken with a grain of salt.) . Programming Languages . Type State Pattern . To eliminate errors make them impossible in runtime is a mantra I stand behind. Programming Patterns that are finally entering mainstream (after stewing in the academic functional world) such as Optional are moving errors to compile time. Among the patterns, type state caught my eye. Using rust‚Äôs borrow checker and other langauge features allows one to build compile time state machines. They can be as simple as allowing the compiler to disallow methods such as read on file references that are closed. Or it can be taken one step beyond to write a full blown state machines that track the current state in compile time. ie Say you have an API that needs a handshake to be performed before sending, you can ensure in compile time that the ‚Äúsend‚Äù method can be called only after ‚Äúhandshake‚Äù is called. How awesome is that. . Why Monads matter? . This article explains what the usally hyped functional programming concept of monad solves for a imperative programmer. I have not dived deeply into any functional language yet. Seeing how even weakly adopted fucntional programming concepts such as Optionals (algebraic data types) and Optional Chaining (which is a monad) makes me question what is the cost with which the programming world is ignoring Functional paradigmn. Are the functional languages difficult to learn, or is it exposure bias towards imperative languages? Or do we need the functional abstractions to be put in better terms for people to grok them? Only time will tell. .",
            "url": "https://saiprasanna.in/posts/semantic-legion-1/",
            "relUrl": "/posts/semantic-legion-1/",
            "date": " ‚Ä¢ Dec 8, 2019"
        }
        
    
  
    
        ,"post4": {
            "title": "SemEval 2019 - Semi-Supervised Domain Adaptation for Suggestion mining",
            "content": "SemEval Workshop regularly has been conducting tasks in NLP to evaluate the progress in the field. . I and my colleague Ananda Seelan participated in this year SemEval‚Äôs Suggestion mining task (Task 9). Here is our submission to be published in NAACL 2019 proceedings, and the code is on github. . This blog is a summary of the key techniques and ideas which influenced this work. . Suggestion Mining Task . The suggestion mining task in brief is a text classification task to find whether a sentence contains a suggestion. . Example, . Suggestion - It would be nice if they had vegan options. | Non Suggestion - This restaurant has good vegan options. | . About 8k sentences scrapped from technical forumns were provided as training data. The task was divided into two subtasks. . Subtask A - Evaluation on same domain - technical forums posts. | Subtask B - Evaluation on out of domain - hotel reviews. | . The catch for subtask B is human labelled data in hotel reviews domain is not allowed for training. Our model was placed third place in the leaderboard for Subtask B. . Key Techniques . We used simple convolutional neural networks for text classification. And we applied transfer learning and semi-supervised learning for the tasks. . Transfer Learning . The current trend in machine learning for NLP is to using pre-trained language models. We used google‚Äôs recently published BERT model as our representation layer. Take a look at http://jalammar.github.io/illustrated-bert/ for a good description of how pre-trained models work for NLP. . Semi-Supervised Learning . In ACL 2018 conference Melbourne, I attended two talks which impacted the work in this paper. One was Sebastien Ruder‚Äôs talk on Strong baselines for semi-supervised learning in NLP. The conclusion of Sebastian Ruder, Barbara Plank (2018) was that classic machine learning techniques for semi-supervised learning such as Tri-Training prove as strong baseline in NLP with neural nets. Sebastien has a very accessible and thorough blog post explaining the techniques. They have also made their code available on github. . We applied a variant of tri-training. We use three models of the same architecture trained initially on data bootstrap sampled from the tech reviews data. The three models are used to iteratively label unlabelled data from hotel reviews domain. Agreement of labels between two models is used as way to select sentences to be added to next iteration of training. Pseudo-code and detailed explanations can be found in the paper. Or you might as well look to the code, as its way simpler than a dry description of it might suggest. . Statistical Significance . The other work published and presented in ACL 2018 that influenced this paper is Rotem Dror‚Äôs ‚ÄúThe Hitchhiker‚Äôs Guide to Statistical Significance in NLP‚Äù. . We report confidence intervals for five random seeds for all our experiments. And we also do pair-wise significance testing via McNemar‚Äôs test to evaluate whether pair-wise model performance on the test set vary significantly. . Metrics . . McNemar‚Äôs Test . .",
            "url": "https://saiprasanna.in/posts/semeval-19-semi-supervised-domain-adaptation-for-suggestion-mining/",
            "relUrl": "/posts/semeval-19-semi-supervised-domain-adaptation-for-suggestion-mining/",
            "date": " ‚Ä¢ Apr 7, 2019"
        }
        
    
  
    
        ,"post5": {
            "title": "God's own Programming Language",
            "content": "Found this interesting blog post explores why many programmers hold a high regard for an ancient programming language which you might not have heard about or use daily. . For God wrote in Lisp code When he filled the leaves with green. The fractal flowers and recursive roots: The most lovely hack I‚Äôve seen. And when I ponder snowflakes, never finding two the same, I know God likes a language with its own four-letter name. . Poem from twobithistory.org . XKCD Comics . Also a good read are the posts on LISP by Paul Graham of YCombinator/HackerNews fame. . How to attain Nirvana with the God‚Äôs own language? . To attain programming nirvana - Start reading the SICP Book. . SICP (Structure and Interpretation of Computer Programs) is a introduction to computer science book. It can change how you view even simple constructs we use for code (like loops, if, etc). A book that will teach timeless concepts in programming which you would never come across easily. I started reading it ages ago, haven‚Äôt completed it , the first few chapters themselves were sufficiently mind blowing. . Original book . A modern interactive version Now you can run the examples of SICP book God‚Äôs own language in the browser with godforsaken javascript. And laugh morosely on the irony of running LISP in a half baked language (javascript) which was inspired from it. . A Distilled version with illustrations - Smaller, condensed version. . To use an analogy, if SICP were about automobiles, it would be for the person who wants to know how cars work, how they are built, and how one might design fuel-efficient, safe, reliable vehicles for the 21st century. The people who hate SICP are the ones who just want to know how to drive their car on the highway, just like everyone else. - Peter Norvig on SICP . (How to Write a (Lisp) Interpreter (in Python)) . Follow this guide to implement your own LISP Interpreter in an hundred lines of python. You might think, ‚ÄúIs he crazy to ask a language beginner to implement the interpreter before learning it?‚Äù Answer is while I am partly crazy LISP is not, it has the simplest structure of all programming languages. It‚Äôs just lists duh! (LiSt Processing). . LISP for AI . You can practise LISP for Artificial Intelligence algorithms with Peter Norvig‚Äôs book on ‚ÄúParadigms of Artificial Intelligence Programming‚Äù. . Say you don‚Äôt want Nirvana, but something more pragmatic . You can learn Clojure to use God‚Äôs Language to do some real world wizardry. Clojure is a form of LISP which is modern, functional and runs on JVM. Brave Clojure is one of the best sources out there for Clojure. For front end development/nodejs there is clojure-script which compiles down to javascript. . Learning Clojure is the best way you can improve as a programmer because it introduces you to powerful concepts implemented in a simple, cohesive, and practical language. You learn Clojure here. Therefore, Brave Clojure is your very best friend when it comes to programming.‚Äù And lo, the syllogism was born! . XKCD Comics . A Note and a Zen Koan . Use any language that solves your problem and learn about others which have different paradigms conceptually like LISP, etc when you find the time. It a enjoyable exercise if you are curious about digging deeply into what makes computers tick. . As Master Foo says says, . Master Foo once said to a visiting programmer: ‚ÄúThere is more Unix-nature in one line of shell script than there is in ten thousand lines of C.‚Äù . The programmer, who was very proud of his mastery of C, said: ‚ÄúHow can this be? C is the language in which the very kernel of Unix is implemented!‚Äù . Master Foo replied: ‚ÄúThat is so. Nevertheless, there is more Unix-nature in one line of shell script than there is in ten thousand lines of C.‚Äù . The programmer grew distressed. ‚ÄúBut through the C language we experience the enlightenment of the Patriarch Ritchie! We become as one with the operating system and the machine, reaping matchless performance!‚Äù . Master Foo replied: ‚ÄúAll that you say is true. But there is still more Unix-nature in one line of shell script than there is in ten thousand lines of C.‚Äù . The programmer scoffed at Master Foo and rose to depart. But Master Foo nodded to his student Nubi, who wrote a line of shell script on a nearby whiteboard, and said: ‚ÄúMaster programmer, consider this pipeline. Implemented in pure C, would it not span ten thousand lines?‚Äù . The programmer muttered through his beard, contemplating what Nubi had written. Finally he agreed that it was so. . ‚ÄúAnd how many hours would you require to implement and debug that C program?‚Äù asked Nubi. . ‚ÄúMany,‚Äù admitted the visiting programmer. ‚ÄúBut only a fool would spend the time to do that when so many more worthy tasks await him.‚Äù . ‚ÄúAnd who better understands the Unix-nature?‚Äù Master Foo asked. ‚ÄúIs it he who writes the ten thousand lines, or he who, perceiving the emptiness of the task, gains merit by not coding?‚Äù . Upon hearing this, the programmer was enlightened. . For more funny hacker koans, visit here and here. . I am currently learning emacs-lisp for I have become a convert/evangelizer of the Church of Emacs on a Starship called Spacemacs. But that‚Äôs a post/sermon (;P) for another time. .",
            "url": "https://saiprasanna.in/posts/gods-own-programming-language/",
            "relUrl": "/posts/gods-own-programming-language/",
            "date": " ‚Ä¢ Apr 4, 2019"
        }
        
    
  
    
        ,"post6": {
            "title": "Learning Longer-term Dependencies in RNNs with Auxiliary Losses (ACL 2018)",
            "content": "Paper by Trieu H. Trinh, Andrew M. Dai, Minh-Thang Luong, Quoc V. Le . TLDR; RNNs can learn very long sequences when auxiliary loss for predicting input sequence (forward and backward) from different timesteps is added. . Problem being addressed . Recurrent neural nets in theory can learn arbitrarily long sequences, but in practice suffer from problems like vanishing gradients etc. Techniques to reduce vanishing gradients problem like LSTM alone don‚Äôt work for very long sequences. . RNN has a better tradeoff when it comes to memory requirements compared to CNN based networks or vanilla Transformer nets. When processing very large sequences this becomes important. . Proposed Method . Take the hidden state of main RNN used for a given task at sampled timesteps, use another RNN at sampled intervals, and try to predict the input sequence to certain time steps. Truncated BPTT (Back propagation through time) to few timesteps would give a new loss. . Evaluation and results . Evaluation is done on MNIST, CIFAR-10, Stanford dog dataset is given as sequence of pixels to an RNN with classification being the target. Since the pixels are flattened to sequential input, spatial location information is now across whole range of the sequence, requiring long dependencies to be formed to get good results. The authors also test it on character based classification on dbpedia. This technique achieves very significant results on long sequences compared to existing LSTMs, Transformers etc. . Opinions . This paper makes a significant experiment to improve a crucial behavior of RNNs on long sequences. The ablation study is well done. This auxiliary loss reminded me of the World models paper where the task of predicting future states improves current tasks output. .",
            "url": "https://saiprasanna.in/posts/learning-long-term-dependencies-rnn/",
            "relUrl": "/posts/learning-long-term-dependencies-rnn/",
            "date": " ‚Ä¢ Jun 5, 2018"
        }
        
    
  
    
        ,"post7": {
            "title": "Neural Open Information Extraction (ACL 2018)",
            "content": "Paper by Lei Cui, Furu Wei, Ming Zhou . TLDR; Models Open information extraction as Sequence to Sequence problem using neural nets. . What is Open information extraction? . Open Information Extraction aims to extract one or more (Entity 1, Relationship, Entity 2) tuples from sentences. . Example . &quot;Deep learning is a subfield of machine learning.&quot; | v (Deep learning, is a subfield of , machine learning) . Existing methods use handcrafted rules written on syntatic parsers which have poor perfomance and suffer from cascading of errors. This paper applies neural networks to get better accuracy and alleviate errors. . How is the problem modeled? . Sequence 2 Sequence task, where you take a source sentence as input and output the information tuple as sequence separated by special tokens (open arg1, arg2, arg3 and close arg1, arg2 arg3). Currently the model is trained for single tuple extraction. . ‚ÄúDeep learning is a subfield of machine learning.‚Äù -&gt; ‚Äú Deep learning is a subfield of machine learning ‚Äù . The paper uses a LSTM based Sequence to Sequence model with attention. The source and target vocabulary is same. If unknown word target is found, the model forms the target probability vector by placing attention on of source sequence as probability of the corresponding source words occurring. . What are the benchmarks? . Evaluating on a large benchmark dataset, this model gets 0.473 AUC (Area under curve) for Precision-Recall on top 5 predictions of this model (I think generated from beam search) has better performance , significantly higher than existing systems. Among existing systems OpenIE has the best score of 0.373 AUC . . One observation of authors is only 11 % of the predictions of Neural model matches with Open IE (rule based) model, but the performance is higher. This could be due to neural model generalizing on some of the patterns hard to capture by rules. .",
            "url": "https://saiprasanna.in/posts/neural-open-information-extraction/",
            "relUrl": "/posts/neural-open-information-extraction/",
            "date": " ‚Ä¢ Jun 4, 2018"
        }
        
    
  
    
        ,"post8": {
            "title": "Dependency Injection - What, Why and How?",
            "content": "We are going to explore dependency injection with emphasis on swift iOS development. But the concept applies to most object oriented languages. We will also see some practical considerations on applying DI in iOS environment. This article is result of my deep dive into implementing DI, and learning about various practical, theoretical aspects of it. . What is Dependency Injection? . ‚ÄúDependency Injection‚Äù is a 25-dollar term for a 5-cent concept‚Äù. . is a often repeated maxim regarding DI. . In its essence DI means wherever possible, replace object creation inside a piece of code by providing it from outside that piece of code. Hence the term. . Constructor, Property, Method are where we usually do object creation, and that can be replaced from outside. So we end up with 3 types of injection. . Constructor Injection | Property Injection | Method Injection | We will explore they 3 types in ‚ÄúHow ..?‚Äù section. . Why Dependency Injection? . Let‚Äôs dwelve into this with help of a scenario. . Scenario - Koala Koder! . Say you have an app with a user login. A user model struct/class encapsulates the logged in user data. . Suppose you are a Koala Koder( a programmer who is as lazy as a koala bear). You perhaps made a solution which is quick and dirty. Store the user model inside NSUserDefaults, and fetch it via properties. And as we all know how apple loves its singleton classes, we follow them making UserModel a singleton. . class UserModel { static let sharedInstance = UserModel() var name: String { get { return NSUserDefaults.standard.string(forKey: &quot;userName&quot;) ?? &quot;&quot; } set { NSUserDefaults.standard.set(newValue, forKey:&quot;userName&quot;) } } func greet() -&gt; String { return &quot; (name), Vannakam :)&quot; } } . We write our app with this usermodel in mind, and UserModel.sharedInstance is everywhere in our app. . . Problems, problems everywhere‚Ä¶ . 1. Unit Tester Vader strikes! . A senior developer suddenly turns to the dark side, and starts ranting about unit testing. He/She will not let apps which are not unit tested pass the code review. . . 2. A wild New Use case appears! . And if that isn‚Äôt enough a new use case should be supported. Our app should now support multiple users. . . 3. ‚ÄúLets move to &lt;insert any serialization library here&gt;‚Äù . Now we also reached a point where userdefaults didn‚Äôt scale and wish to migrate to new data serialization method. Now even our getters and setters inside the UserProfile is not safe. . Why are there problems? . So we find ourselves in deep trouble. Lets analyze why so. . 1. Singletons are hard to test . Now if you want to unit test a viewcontroller that uses this singleton. . class UserProfile: UIViewController { func viewDidLoad() { greetingLabel.text = UserModel.sharedInstance.greet() //... } } . In unit testing you create a UserProfile object, call viewDidLoad or other methods manually. Now you have to verify whether UserModel.sharedInstance.greet() was called. . Since UserModel.sharedInstance is immutable, either we can‚Äôt replace it with our mock class extending UserModel, which overrides greetUser, and sets a flag which can be checked. So our testing coverage comes down. . 2. Instantiation inside our code constraints creates strong coupling, creating harmful constraints . In our scenario, by using a singleton, we tied our codebase to a single UserModel but now our app needs multiple user models. So in general, using singletons will make it hard to adapt to new use cases. What you thought as a singleton suddenly is not so single anymore. . But consider that we didn‚Äôt use singleton, but instantiated UserModel, by calling UserModel() wherever we needed it. . class UserProfile: UIViewController { let userModel = UserModel() func viewDidLoad() { userNameLabel.text = userModel //... } } . We can‚Äôt support our new usecase of multiple users, without userprofile class knowing about multi-usermodel, or some other global allowing UserModel() to return the correct usermodel, these solutions are ugly hacks, which add complexity by either giving too much knowledge to classes or using globals and forgoing object oriented encapsulation. . 3. Concrete Type usage creates strong coupling . Consider the UserProfile, it uses NSUserDefaults, now suppose we move to coredata to save our data, we are again in trouble because of using singleton inside. Our UserProfile rather needed only just a way to serialize some data, it didn‚Äôt need to know about WHAT we use for serialization. This is the key insight to keep in mind when thinking about dependency injection. . DI to the rescue . DI helps to solve the variety of issues that we face above, with regard to Unit Testablity, Singletons and strong coupling we face above. . How to do Dependency Injection? . Constructor Injection . So we will be injecting a serializer into UserModel via constructor. . Move dependency to constructor, and if possible make it a interface/protocol type instead of concrete class/struct . So we remove singleton access to userdefaults. And rather pass a serializer protocol which has methods we require for serialization to constructor. . protocol Serializing { func string(forKey defaultName: String) -&gt; String? func set(_ value: Any?, forKey defaultName: String) } class UserModel { static let serializer: Serializing init(_ serializer: Serializing) { self.serializer = serializer } var name: String { get { return serializer.string(forKey: &quot;userName&quot;) ?? &quot;&quot; } set { serializer.setObject(newValue, forKey:&quot;userName&quot;) } } } . Since we are Koala Koder, we just make the Serializing protocol methods to same ones in NSUserdefaults, so we can make NSUserDefaults conform easily by . extension NSUserDefaults: Serializing {} . . . Now to use user defaults as our serializer, we do the following. . UserModel(serializer: NSUserDefaults.standard) . For our fancy multiple user use case we can also do this. (Note: did this in a hurry, it may have edge cases, just providing it as a illustration) . struct MultiUserSerializer: Serializing { let serializer: Serializing init(_ serializer: Serializing) { self.serializer = serializer } // So we fetch the current currentUserId and use that to prefix stored data private func fetchCurrentUserId() -&gt; String { return serializer.string(forKey: &quot;CURRENT_USER_ID&quot;) ?? &quot;0&quot; } func string(forKey defaultName: String) -&gt; String? { return serializer.string(forKey: fetchCurrentUserId() + &quot;:&quot; + defaultName) } func set(_ value: Any?, forKey defaultName: String) { return serializer.set(value, forKey defaultName: fetchCurrentUserId() + &quot;:&quot; + defaultName) } } //Instantiate using userdefaults, assume we implemented contextFetcher somewhere else let multiUserSerializer = MultiUserSerializer(serializer: NSUserDefaults.standard) let userModel = UserModel(serializer: multiUserSerializer) . So now userModel fetches data, and sets data to the current userID without even knowing about it. We could also use something other than NSUserDefaults.standard to serialize the data in top level. . The point is by removing replacing the concrete dependency NSUserDefaults.standard out of UserModel and swapping it to Serializing protocol we can now satisfy the new usecase of multi user modelling easily. This is the core idea behind of loose coupling. . Also we can now unit test User Model by just passing a Mock implementation of Serializing . Dependency injection works even with only concrete types. . Sometimes you don‚Äôt have the time, or are sure that concrete type used will not have to be changed. You can still DI the concrete type without bothering with protocol creation, just for the sake of it. . For example: . class UserProfile: UIViewController { let userModel: UserModel // This works only if you don&#39;t use storyboards init(userModel: UserModel) { self.userModel = userModel } func viewDidLoad() { userNameLabel.text = userModel.name } } . We move UserModel creation out of the controller, without creating any protocol for UserModel properties. . We still gain advantages of unit testability using ordinary mock objects, and also we are free to use our multi user serialized UserModel , hence making UserProfile support multi user model without changing any logic in user profile. . (But if you have to do something to notify changes in data, that is seperate topic) . let multiUserSerializer = MultiUserSerializer(serializer: NSUserDefaults.standard) let multiUserModel = UserModel(serializer: multiUserSerializer) let userProfile = UserProfile(userModel: multiUserModel) . My constructor is a monster now¬†:( . A common problem which you will come across is, your UIViewController (if you don‚Äôt use storyboards) or any class where you do DI becomes a to big. . class UserProfile: UIViewController { init(userModel: UserModel, apiService: APIService, x: XService, y:YService, z: ZService, ....) . This is a good thing, it points out clearly that your class is violating Single Responsibility rule . Single responsibility rule states that a class should have singe responsibility. . We can solve this by moving some of the current dependencies to a new class, and pass the new class as dependency to UserProfile. . Guess what if we do this we properly we would be re-inventing design patterns l MVVM(Nodel View ViewModel) or MVP. Whew! DI just solved Huge ViewController problem!!. . DI can easily help refactoring code to better design, in a gradual manner. . Property Injection . We seemed to have solved all our above problems, why bother with more types of injections? . If you don‚Äôt control the creation of a object, your best bet is property injection. But prefer constructor injection if that‚Äôs not the case. . Sometimes you don‚Äôt create objects of classes, some messy framework does it. For example, if you use Storyboards, you can‚Äôt do stuff like let userProfileViewController = UserProfile(multiUserModel). You would have to refactor to something like . class UserProfile: UIViewController { var userModel: UserModel! func viewDidLoad() { userNameLabel.text = userModel.name } } let multiUserSerializer = MultiUserSerializer(serializer: NSUserDefaults.standard) let multiUserModel = UserModel(serializer: multiUserSerializer) let storyboard = UIStoryboard(name: &quot;main&quot;, bundle: nil) let userProfile = storyboard.instantiateViewController(withIdentifier: &quot;UserProfile&quot;) as! UserProfile userProfile.userModel = multiUserModel . By using implicitly unwrapped Optional property, and setting it from outside, we achive the same effects of constructor injection. But is not perfect, as userModel property can be mutated from outside. Butthis is as good as it can get. . Method Injection . Method injection is just replacing instantiating inside method by one of its parameters. . // Before injection func someMethod() { let x = X() let y = x.something() + 10 return y } // After injection func someMethod(_ x: XService) { let y = x.something() + 10 return y } . Runtime Injection - Factory pattern . Sometimes you want to create a object of a particular class in runtime. But you want to use only protocol type(or a super type) instead of actual implementation (or subclass). Let‚Äôs say you need a networking service, which you set based on a user action, but as you are going to need it in runtime, you may think that you can‚Äôt inject it. . But what you can do is inject a factory Networking object or closure, that constructs it in runtime. This factory can fill the dependency of the concrete Networking class, so your class can be unaware of this. . protocol Networking { // Some methods .. } class ProxyNetworking: Networking { init(a: A) { } } class NormalNetworking: Networking { init(a: A) { } } // Injection Via Factory Class class NetworkingFactory { let a: A init(a: A) { self.a = A } func create(_ withProxy: Bool) -&gt; Networking { if (withProxy) { return ProxyNetworking(a: a) } else { return NormalNetworking(a: a) } } } class Some :UIViewController { // This has to be injected via property injection var networkingFactory: NetworkingFactory! var networking: Networking? @IBOutlet weak var proxySwitch: UISwitch! @IBAction func userTappedSubmit(sender: UIButton) { networking = networkingFactory.create(withProxy: proxySwitch.isOn) } } // Injection using closure class Some :UIViewController { // This has to be injected via property injection var networkingFactory: ((Bool) -&gt; Networking)! var networking: Networking? @IBOutlet weak var proxySwitch: UISwitch! @IBAction func userTappedSubmit(sender: UIButton) { networking = networkingFactory(proxySwitch.isOn) } } . Dependency injection - Containers &amp; Frameworks . There are Dependency Injection frameworks that make the job of dependency injection easier. You may say, ‚Äúwhoa Sai! wait,Do we really need a dependency injection framework as a dependency? Can‚Äôt it be done ‚Äú. . When we examine what we are doing with DI, we are building a graph with our concrete types as nodes, and their dependencies linking them. If we do this completely, all dependencies will originate from a root object. . Without a framework, we will be doing a lot of copy paste coding. If our app uses networking protocol type in multiple areas, we have to type out the same concrete implementation everywhere, and fill out every dependency of networking class everywhere. . For example: . class A { var propertyInjectionVar :V! init(networking: Networking, ...) } // To create A we have to create networking and also fill out any property injection vars it needs class NetworkService: Networking { init(x: X, y: Y, z: Z) {} } // Now networking will inturn have its dependencies , which inturn have more .. // So to create A, You have to create NetworkService, X, Y, Z A(networking NetworkService(x: X(), y: Y(), z: Z())) . To make this process easier we can build something to store list of dependency type, and their concrete implementation. We will have a table of mappings. . Dependency Type Implementation type . Networking | NetworkService | . X | XService | . Y | YService | . Z | ZSubClass | . A | A | . We can register a protocol Dependency Type to concrete implementation, like Networking, X, Y to NetworkService, XService, YService respectively. Or map concrete type to its concrete implementation which can be exactly same type, like A, or subclass like mapping Z to ZSubClass. Now what the container does is when A has to be created, it auto resolves each dependency of A from the table. . You can either implement a container, or use a DI Framework which does that for you. . Containers also allow you to autofill property injection, handle lifecycle of dependencies like marking them as singleton, so that your whole container has only one object of that type created and other fancy features to make your life easy. . A key point to remember is that your classes should not depend on DI framework container ie its not good idea to pass the container to your class as a dependency. . Dip Framework - Swift . Among the DI frameworks exiting now for swift, I recommend Dip. Dip has some nifty features to make your DI pain free. . Features of Dip . Scopes. Dip supports 5 different scopes (or life cycle strategies): Unique, Shared, Singleton, EagerSingleton, WeakSingleton; | Auto-wiring &amp; Auto-injection. Dip can infer your components‚Äô dependencies injected in constructor and automatically resolve them as well as dependencies injected with properties. | Resolving optionals. Dip is able to resolve constructor or property dependencies defined as optionals. | Type forwarding. You can register the same factory to resolve different types implemeted by a single class. | Circular dependencies. Dip will be able to resolve circular dependencies if you will follow some simple rules; | Storyboards integration. You can easily use Dip along with storyboards and Xibs without ever referencing container in your view controller‚Äôs code; | Named definitions. You can register different factories for the same protocol or type by registering them with tags; | Runtime arguments. You can register factories that accept up to 6 runtime arguments (and extend it if you need); | Easy configuration &amp; Code generation. No complex containers hierarchy, no unneeded functionality. Tired of writing all registrations by hand? There is a cool code generator that will create them for you. The only thing you need is to annotate your code with some comments. | Weakly typed components. Dip can resolve ‚Äúweak‚Äù types when they are unknown at compile time. | Thread safety. Registering and resolving components is thread safe; | Helpful error messages and configuration validation. You can validate your container configuration. If something can not be resolved at runtime Dip throws an error that completely describes the issue; | . Annotations in Dip . Java has good frameworks like Dagger, and Guice that use annotations to make the job even simpler compared to swift. Dip allows you to leave annotations of dependencies in comments and also generate code for DI from it. How cool is that? . Poor man‚Äôs DI in Swift . Though I recommend the framework approach, supposing you don‚Äôt want to use framework initially and still want the to do DI, Fear not. You can use Swift‚Äôs default parameters to do constructor/method injection, and just use variable properties for property injection. You can maintain a Seperate DI singleton and fill dependencies using that. . class UserModel { static let serializer: Serializing init(_ serializer: Serializing = PoormanDIContainer.instance.getSerializer()) { // Poor man DI self.serializer = serializer } var name: String { get { return serializer.string(forKey: &quot;userName&quot;) ?? &quot;&quot; } set { serializer.setObject(newValue, forKey:&quot;userName&quot;) } } } class PoorManDIContainer { let instance = PoorManDIContainer() func getSerializer() -&gt; Serializing { // If Serializer has some dependencies, it will again use default constructor to obtain it from PoorManDIContainer return Serializer() } } . But if you use the above method, beware of circular dependencies. . Conclusion . So in conclusion DI is great. It allows you to progressively make your code better remove singletons, make your code modular, testable and also allow you to evolve good design patterns. Try it out in your existing code base, it will be one of the easiest way to refactor legacy OOP code, without modifying internal logic initially.If you have any doubts, suggestions, constructive criticisms, comment below. .",
            "url": "https://saiprasanna.in/posts/dependency-injection-what-why-and-how/",
            "relUrl": "/posts/dependency-injection-what-why-and-how/",
            "date": " ‚Ä¢ Mar 20, 2017"
        }
        
    
  
    
        ,"post9": {
            "title": "Manjaro Linux - My current daily driver arch based distro",
            "content": "The partition having arch in my secondary hard disk had died. The invisible grime which settles in your mind because of using non free windows and osX at home and work had started to bother me. . And as I got a new mechanical keyboard (Reddragon KUMARA, cheap as cherry MX patents on mechanical switches have died out) and a gaming mouse (Logitech G402) recently, I wanted to use my desktop as main programming machine, instead of my old macbook air. . . So to inevitably I was going to reinstall GNU/Linux.Choosing a new linux distro as you all know leads to paralysis due to infinite choice. . Of all the distros that I had installed, I love Arch Linux the most. Arch with its rolling update model, and the infinite repositories of aur, always hit the sweet spot. But I was feeling a bit lazy, as in Arch Linux one has to setup everything from scratch (which I recommend at least once), and for some reason my font settings was always bad in vanilla Arch Setup. I know that is not exactly an insurmountable problem, but as I said before, was feeling lazy. . After a love/hate relationship with mac OS, I really wanted a good GUI experience. I am KDE kind of a guy, and love KDE plasma environment. I shopped around a bit, and found elementaryOS based on new desktop environment called the Pantheon. Thought I would give it a shot, though it wasn‚Äôt Arch based. But eventually decided against it, because it didn‚Äôt seem very customizable, which is rather the point of elementaryOS. . So my endless distro search, ended up with deciding between variants based on arch linux. Manjaro, Antergos , ApricityOS, I eventually decided to install Manjaro, because I heard that other two are basically GUI installers of Arch, while Manjaro was a distro based on arch with customizations to make it a more smooth ride. . . Boy, Manjaro with KDE, had refreshing feel, it felt like what Linux mint was when the first time I installed it. And propriety driver support was out of the box. I would recommend Manjaro as daily development OS, where you want arch, but don‚Äôt feel like configuring it perfectly. .",
            "url": "https://saiprasanna.in/posts/Manjaro-Linux/",
            "relUrl": "/posts/Manjaro-Linux/",
            "date": " ‚Ä¢ Mar 17, 2017"
        }
        
    
  
    
        ,"post10": {
            "title": "Crashes are Optional! && Write Less, Do More",
            "content": "It was a boring tuesday/wednesday afternoon, co-worker Giridhar pinged me. He was the organizer for Swift India meetups. He invited me to give a talk. As I am in trying to doing things that I haven‚Äôt done before, I accepted readily. . About 60-66 people turned out for the meetup. It was fun and great learning experience to share what you know with others. It also exposed where I have to concentrate to develop my public speaking skills. . Here are the slides for the talk. I will try to put up the playground file later. .",
            "url": "https://saiprasanna.in/posts/crashes-are-optional/",
            "relUrl": "/posts/crashes-are-optional/",
            "date": " ‚Ä¢ Jan 29, 2017"
        }
        
    
  
    
        ,"post11": {
            "title": "Open for Collaboration, Closed for Disturbance",
            "content": "I have been working at open office environment for past 1 and a half years. The first office was a semi open one where workspaces had some degree of separation, not exactly a cubicle, honey comb like structure where there is circle of 4 people at center, and 6 or so people around. The one thing I liked about our office is that there are no corner offices for managers. Even CEO had to sit with others. Though this is my first job, I can appreciate the egalitarian and practical aspects of this. There was some degree of isolation to do deep work (though I seriously crave for more), while preserving the collaborative aspect of open office. . But after few months we moved to new workspace, things changed to the worse. It was the dreaded Complete open office plan, our building consists of floors with more than 200 people in each, and there is no separation between rows of tables. It is a complete nightmare, taking the good design to extreme and making it completely hard to bear. . This made me think about O of S.O.L.I.D principles of software design, which suggest that a good design should be open for extension and closed for modification.I think this applies allegorically to office layout plan. . Some companies have moved to ditching offices completely, but there are many still not taking such a drastic step. So the principle which I arrive on workplace design, Open for Collaboration and Closed for Disturbance. A good office design should make it accessible for people to collaborate and avoid creating unnecessary old age hierarchy bullshit that came in form of corner office perks. At the same time it should provide the much needed c isolation to get shit done. In general the design should at least have some barrier between people.It wouldn‚Äôt hurt to provide some ‚ÄúDeep think rooms‚Äù which can function as isolation chambers which can be opposite of meeting rooms. . Many studies are pointing to decreased level of productivity in open office plans. It is common sense that deep work requires some degree of isolation, which completely open spaces hardly provides. Every interruption completely derails the thought process which were happening till then, productivity. . Lets look at some facts,studies,opinions which point out to the decrease in productivity, happiness, and involvement in complete open office: . 1) http://blog.ninlabs.com/2013/01/programmer-interrupted/ . Based on a analysis of 10,000 programming sessions recorded from 86 programmers using Eclipse and Visual Studio and a survey of 414 programmers (Parnin:10), we found: A programmer takes between 10‚Äì15 minutes to start editing code after resuming work from an interruption. When interrupted during an edit of a method, only 10% of times did a programmer resume work in less than a minute. A programmer is likely to get just one uninterrupted 2-hour session in a day We also looked at some of the ways programmers coped with interruption: Most sessions programmers navigated to several locations to rebuild context before resuming an edit. Programmers insert intentional compile errors to force a ‚Äúroadblock‚Äù reminder. A source diff is seen as a last resort way to recover state but can be cumbersome to review . 2) http://www.forbes.com/sites/davidburkus/2016/06/21/why-your-open-office-workspace-doesnt-work/ . Thus while noise was a problem, the greater noise level didn‚Äôt appear to be from all of the collective collaboration buzzing around the open room. The researchers then took their analysis one step further, using regression to calculate how important each dimension was to employees‚Äô overall satisfaction. One of the dimensions most strongly related to overall satisfaction was ease of interaction, despite the fact that it was judged to be no better or worse in open office plans than in private offices. In other words, the desire for more collaboration among employees was shared by all, but those in open office plans may not have found it to be worth all of the stress and distraction from the bombardment of noise. . 3) http://www.newyorker.com/business/currency/the-open-office-trap . But the most problematic aspect of the open office may be physical rather than psychological: simple noise. In laboratory settings, noise has been repeatedly tiedto reduced cognitive performance. The psychologist Nick Perham, who studies the effect of sound on how we think, has found that office commotion impairsworkers‚Äô ability to recall information, and even to do basic arithmetic. Listening to music to block out the office intrusion doesn‚Äôt help: even that, Perham found, impairs our mental acuity. Exposure to noise in an office may also take a toll on the health of employees. In a study by the Cornell University psychologists Gary Evans and Dana Johnson, clerical workers who were exposed to open-office noise for three hours had increased levels of epinephrine‚Ää‚Äî‚Ääa hormone that we often call adrenaline, associated with the so-called fight-or-flight response. What‚Äôs more, Evans and Johnson discovered that people in noisy environments made fewer ergonomic adjustments than they would in private, causing increased physical strain. The subjects subsequently attempted to solve fewer puzzles than they had after working in a quiet environment; in other words, they became less motivated and less creative. . 4) http://www.inc.com/geoffrey-james/why-your-company-will-benefit-from-getting-rid-of-open-office-spaces-first-90.html . They decrease productivity. Contrary to popular belief, open offices don‚Äôt increase collaboration or make people more productive. An Exeter University study showed they create a 32 percent drop in ‚Äúworkers‚Äô well-being‚Äù and 15 percent reduction in productivity. .They create time-consuming distractions.Office workers lose an average of 86 minutes per day due to distractions associated with open-plan offices. As a result, many employees are ‚Äúunmotivated, unproductive, and overly stressed,‚Äù according to the study funded by Steelcase. They make employees sick.A study at Queensland University of Technology‚Äôs Institute of Health and Biomedical Innovation found that working in environments without offices ‚Äúcaus[es] high levels of stress, conflict, high blood pressure, and a high staff turnover.‚Äù This comic describes it in a funny way on what is the cost of interruptions on programmers. This can apply to any field that involves some thinking to be done at work. . .",
            "url": "https://saiprasanna.in/posts/open-for-collaboration-closed-for-disturbance-rethinking-open-office-layout/",
            "relUrl": "/posts/open-for-collaboration-closed-for-disturbance-rethinking-open-office-layout/",
            "date": " ‚Ä¢ Jan 6, 2017"
        }
        
    
  
    
        ,"post12": {
            "title": "Function Currying, Composition in Redux Middleware",
            "content": "I am new to functional programming paradigm, though I do understand something about closures ,pure functions and have used some programming structures related to functional concepts like map, reduce, blocks in my iOS applications at work, I don‚Äôt have much experience with functional paradigm. . I started learning about react js, and eventually ended up learning about Redux. I would assume you to have basic knowledge about redux , if not read about it here. It is very well documented, and the whole api is small, If you are feeling adventurous read the redux source code, it is a few hundred lines. . Redux seems to be a more elegant implementation of Flux methodology without some components of flux like multiple stores communication via a dispatcher which is made redundant in redux pattern. . So as I was playing with redux by implementing a small application mentioned in this tutorial , I came across something called middlewares. These middleware intercept the actions to store and do some extra processing on it. I couldn‚Äôt wrap my head around how a middleware for handling Promises mentioned in that post worked. . export default function promiseMiddleware() { return next =&gt; action =&gt; { const { promise, type, ...rest } = action; if (!promise) return next(action); const SUCCESS = type; const REQUEST = type + &#39;_REQUEST&#39;; const FAILURE = type + &#39;_FAILURE&#39;; next({ ...rest, type: REQUEST }); return promise .then(res =&gt; { next({ ...rest, res, type: SUCCESS }); return true; }) .catch(error =&gt; { next({ ...rest, error, type: FAILURE }); // Another benefit is being able to log all failures here console.log(error); return false; }); }; } . It seemed to use functional magic which somehow allowed waiting for a promise to resolve and then dispatch the action or call next middleware. S o I was wondering how that worked and came across this excellent post titled ‚ÄúUnderstanding Redux Middleware‚Äù. I encourage you to check it out here if you haven‚Äôt already. This post is meant to clarify some things in it a bit further. From that article I came to understand that middleware functions got composed in the following fashion . Middleware1(Middleware2(‚Ä¶)) . Then by chance I was reading through documentation of redux-logger middleware which said that for it to log state , actions properly from it has to be passed as parameter after any asynchronous middleware. So from what I read in redux source for applyMiddleware I gathered that logger middleware will be a parameter for Async middleware for it to log correctly, Assume we do this , then . applyMiddleware(PromiseMiddleware, LoggerMiddleware) . will compose them in this order . PromiseMiddleware(LoggerMiddleware) . This seemed to make less sense as I wrongly thought that promise middleware will wait for LoggerMiddeware to process and then execute. . But after meditating on the redux source code ,creating curried functions and composing them , I got it. The actual execution order of middleware when dispatch action occurs, starts from PromiseMiddleware, which resolves the promise and only then calls the LoggerMiddleware. . This is because LoggerMiddleware is a curried function , so it doesn‚Äôt return any value and it is simply a function that is passed to Promisemiddleware. The Promisemiddleware takes up the action, resolves it, and inside ‚Äúthen‚Äù of promise calls the LoggerMiddleware using . next({ ...rest, res, type: SUCCESS }); . and hence passes the action to LoggerMiddleware Function. The PromiseMiddleware can refer to the next function inside it using the properly named param ‚Äúnext‚Äù. Now lets make some curried functions with asynchronous operations and uncurry them with parameters . Type these in js console We are first creating a curried function that takes another function as input and also some params. . var asyncFunction = nextFunction =&gt; params =&gt; window.setTimeout( function() { alert(&quot;In Async Callback&quot;); nextFunction(params); }, 1000); . This is similar to the promise middleware. asyncFunction takes in parameter a function called nextFunction. From the signature of nextFunction(params) used inside , we can get it that it simply takes a object. Now let us create a function to pass into async . var alertFunction = parameters =&gt; alert(parameter); . alertFunction simply alerts the parameter passed to it. COMPOSING Alert and asyncFunction manually, and calling the result of composition . var composedFunction = asyncFunction(alertFunction); composedFuntion(1); . Output Alert:- In Async Callback Alert:- 1 . VOILA! Our alertFunction executes inside the async callback To clarify replacing the nextFunction with body of alertFunction . function params =&gt; window.setTimeout(function() { alert(&quot;In Async Handler&quot;); // nextFunction(params) becomes alert(params); }, 1000) . I couldn‚Äôt grok async code in middleware because I was making similar mistake as in thinking that as alertFunction is innermost function in composition and so its body would execute first.But in actuality it doesn‚Äôt evaluate to value and hence the order of execution starts from outermost function . . This seems trivial after seeing the above example but it wrecked my understanding of middleware code. if G is a function and F is higher order function , ie F(someInputFunction) is also a function , then when you ‚Äúuncurry‚Äù by calling F(G)(x) the evaluation begins from operations defined inside/ to perform F, and operations defined inside/to perform G may or may not be used by F. . So we can wrapping asynchronous operations in a chain of composed functions, define them in seemingly synchronous way. And I think this is how Promises are actually implemented, will have to read about that later. . Anyway this really excites me, how mind bending functional jui jutsu can accomplish a lot with less code. Will post more when I learn more.. Eager to hear your experience in learning functional programming inception. . Reposted from medium.com .",
            "url": "https://saiprasanna.in/posts/function-currying-and-composition/",
            "relUrl": "/posts/function-currying-and-composition/",
            "date": " ‚Ä¢ Jul 10, 2016"
        }
        
    
  
    
        ,"post13": {
            "title": "Creation and Consumption",
            "content": "The amount of content available for consumption in our digital age is staggering. Movies, TV, video games, videos, blogs, emails, chat messages all compete for our limited time. . I think mindful consumption is of great importance. We can start being aware of what our consumption inlets are. Just the awareness of this can create great impact on how we choose to spend our time. . I am not arguing against consuming media, but for having healthy awareness on wether it has negative effects on one‚Äôs creative spirit. It is important in my opinion to keep the act of creation balanced with consumption. . To contemplate, and engage actively with the world through some outlet is a form of habit I hope we can get into. This outlet can be anything, writing, music, coding for open source, some personal project etc. . I think it is more fulfilling to move towards a balanced consumption to creation ratio. .",
            "url": "https://saiprasanna.in/posts/creation-and-consumption/",
            "relUrl": "/posts/creation-and-consumption/",
            "date": " ‚Ä¢ Jul 10, 2016"
        }
        
    
  
    
        ,"post14": {
            "title": "Speed up iOS dev using XCode Injection Plugin",
            "content": "Injection for XCode. . . One of the most boring/unproductive part of any development cycle is waiting for your project to compile. And if you have done iOS development, you know how much time is wasted for XCode to recompile the project. And even if compile time is less, you have to follow a bunch of taps, long presses etc to get to the desired app state before even testing your changes. This plugin will reduces reduce these steps considerably. As you change your code , you can inject the new class definition using this XCode plugin. It recompiles just changed file, and injects it into the live running app. The great thing is it works for real device as well. . Set up . Install Alcatraz package manager for XCode . Alcatraz allows you to install and remove XCode plugins hazzle free | Open using Alcatraz with package manager option in projects menu. | Search for injection plugin, and install it. Restart XCode after installations | To make it work in real device you need to click on Product -&gt; Injection Plugin -&gt; Patch Project for Injection. It will add couple of lines to your main.m of your project. If it is a swift project just create a empty main.m and do the above. | Inject Code . Run your project, make some changes to the code in a file, press ^ + = . | Your changed code in that file will get compiled and injected into the app live. | Now you can simply have to somehow make your program create new object of the changed class to see the changes. For example, tap back button and then again go the view. | There are few limitations on what can be injected, refer the Injection for XCode‚Äôs github project. And a small limitation currently is it wont work if you have more than 128 source files in your project due to a XCode limitation. Follow this and other issues in github issue tracker. | You can also set it up to automatically inject changes by enabling File Watcher in Product-&gt; Injection Plugin -&gt; Tunable App Parameters | If you want it to make changes visible as you inject the changes in you have to modify your normal code. You have to listen for callbacks after injection , and reload the view or do something else.Read the instructions. | This plugin will add a new folder to your project which you can add to .gitignore to avoid source control. . It is great that the developer has open sourced it. To know how it does its magic see here The author has released it under ‚Äúnagware‚Äù license, where he requests you to pay after using it for two weeks. .",
            "url": "https://saiprasanna.in/posts/speed-up-ios-development/",
            "relUrl": "/posts/speed-up-ios-development/",
            "date": " ‚Ä¢ Apr 17, 2016"
        }
        
    
  
    
        ,"post15": {
            "title": "MIT OCW 6.006 Algorithms Course",
            "content": "MIT OCW course is one among the best introductory algorithm courses online. It introduces techniques to analyse, and understand how algorithms work. . MIT OCW 6.006 Introduction to Algorithms Fall 2011 course is packed with awesome content. The open course ware website contains video recording of the MIT course conducted at MIT and all the accompanying notes, assignments, test content. It does not contain interactive content like udacity courses, but the content is in different level.You need to know python to do this course‚Äôs assignments. If you know how to code in any language , you can pick up basics of python in a day or two as it has simple syntax. . Prof. Erik Demaine ,Prof. Srinivas Devadas do a great job introducing concepts of algorithms and its analysis in the main lectures. Victor Costan does exemplary job in recitation videos in explaining the concepts introduced by Erik and Srini in easily understandable way. So don‚Äôt miss out on recitation if you plan to look into this course. . The assignments are one of the fun parts of the whole course. They allow you to see how efficient algorithms can really make a difference in running time of your code. These guys have done a awesome job in designing each assignment in such a way that you can visualise the impact of algorithms. For example in assignment 3 where you are supposed to write code to detect crossings among wires, they have designed the assignment in such a way that you can see the result of your code running in the browser. . The course videos are also available in youtube as playlist. Checkout the first video. . .",
            "url": "https://saiprasanna.in/posts/MIT-OCW-Algorithms/",
            "relUrl": "/posts/MIT-OCW-Algorithms/",
            "date": " ‚Ä¢ Apr 2, 2016"
        }
        
    
  
    
        ,"post16": {
            "title": "Blog using Github pages",
            "content": "Github.com allows us to upload just templates, config and theme for jekyll static blog generator, and it generates a static website for you. . Now you can just write a markdown file, and commit it, or even create a file online even in github‚Äôs source browser, and it is automatically generated into a blog post just after the commit is made. . I didn‚Äôt notice this feature in github.com before. I thought we had to run jekyll locally to build static site, and commit it for github to serve stuff. So rather went for python static blog generator called pelican, but gave up because of it being a pain to generate blog every time. I even tried to setup travis-ci to automagically generate the blog, but it was slow and was still a pain. . So if you want to create a blog, just use github , it is quite easy to setup. clone a theme, edit _config.yml and blog on. .",
            "url": "https://saiprasanna.in/posts/new-blog/",
            "relUrl": "/posts/new-blog/",
            "date": " ‚Ä¢ Mar 22, 2016"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "&lt;img src=&quot;assets/images/bio-photo.jpg&quot;, style=&quot;display: block;margin-left: auto; margin-right: auto; width: 50%;border-radius:50%&quot; width=&quot;320&quot; height=&quot;320&quot;/&gt; . I‚Äôm Sai Prasanna. I work as a machine learning engineer focusing currently on NLP applications. I also do research in machine learning. . Research Interests . Natural Language Processing | Reinforcement Learning | Representation Learning | Semi-Supervised Learning | Interpretability | Grammatical Error Correction | Language Models | Vector Search | Machine Learning | .",
          "url": "https://saiprasanna.in/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://saiprasanna.in/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}