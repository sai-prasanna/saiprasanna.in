{
  
    
        "post0": {
            "title": "Efficient Dynamic Batching of Large Datasets with Infinibatch",
            "content": "We will explore how to efficiently batch large datasets with varied sequence length for training using infinibatch. The focus will be on solving multiple challenges associated with this and making it work with dataloader abstraction in pytorch library. Though our focus is on pytorch, Infinibatch is a pure python library agnostic of the deep learning library. . This post was inspired by this thread on twitter. Late, but I think exactly what you are asking for: https://t.co/gIyBzf5yoEBy colleagues from my team in MS. . &mdash; Marcin Junczys-Dowmunt (Marian NMT) (@marian_nmt) August 10, 2020 . Note: We will use wikitext-103 dataset as an example. It&#8217;s a dataset with sentences from wikipedia. It has 103,227,021 word level tokens in it&#8217;s training split. It is used only for illustration, the techniques discussed here are can work for far larger dataset sizes. . !wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip !unzip wikitext-103-raw-v1.zip . . --2020-09-14 11:31:28-- https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.88.158 Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.88.158|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 191984949 (183M) [application/zip] Saving to: ‘wikitext-103-raw-v1.zip’ wikitext-103-raw-v1 100%[===================&gt;] 183.09M 58.0MB/s in 3.2s 2020-09-14 11:31:31 (58.0 MB/s) - ‘wikitext-103-raw-v1.zip’ saved [191984949/191984949] Archive: wikitext-103-raw-v1.zip creating: wikitext-103-raw/ inflating: wikitext-103-raw/wiki.test.raw inflating: wikitext-103-raw/wiki.valid.raw inflating: wikitext-103-raw/wiki.train.raw . Challenges in efficiently processing large datasets . 1. Loading and shuffling large datasets . For large datasets, loading the entire data into memory might not be possible. If we were to sample fully random batches we need to do random access on huge dataset. Depending on the disk latency this might be unfeasible. . To solve this we can do the following. . Shard the data into chunks larger than single instances so that it reduces the disk access. | Shuffle the chunks and load few of them and shuffle the data loaded from the chunks. | If we shard the pieces into too big chunks we might end up loosing statistical power in our training updates as we are essentially reducing the randomness of our samples used for training. But we can&#39;t shard them too small either as that wouldn&#39;t solve our disk access problem. . We need a flexible approach would make it easy to control how much data is to be loaded into memory for shuffling. To address this challenge in isolation, you can refer dataset sharding logic in NVIDIA&#39;s MEGATRON language model training code. But infinibatch solves it in a more generalized manner along with our other challenges. . 2. Dynamic Batching . NLP datasets generally have samples which are of varied lengths. When we batch the data for training on devices like GPU, we are forced to make them into n-dimensional tensors with fixed dimension. The most common type of input for NLP models is of the shape Mini-batch size x Sequence length. The sequence length is either a fixed value or is the length of longest sequence in that batch. The shorter sequences in the minii-batch are generally padded with a padding token. These padding tokens are wasteful in terms of computation as they don&#39;t do anything useful. . Some tutorials and examples you would find for pre-processing data would pad batches to a pre-determined sequence length independent of the elements in each batch. This is fully wasteful as many batches would have all the members less than the pre-determined length. . A better option would be to pad the elements of each batch to the sequence length which is maximum in that batch. This dynamic padding can improve efficiency but it doesn&#39;t solve the entire problem. Let&#39;s see why with an example. . Tokenization and length distribution . Let&#39;s implement a typical dynamic padding workflow with pytorch dataloader and a subword level tokenizer. We use BERT-base-cased tokenizer from huggingface&#39;s transformers library. This tokenizes words to subwords. The BERT model was pre-trained with maximum subword length of 512. We can theoretically use sequence lengths larger than that but for our purposes we will leave it as such at 512. . We will use torch&#39;s dataset and dataloader abstraction for this. It will as both an illustration of real world usage and is convinent as it helps avoid having to entire tokenized dataset in memory. We still have to load the sentences into memory once. This is not a problem for small datasets, but for very large corpuses it&#39;s a big problem as mentioned before. . #hide_output !pip install git+https://github.com/microsoft/infinibatch.git transformers torch from typing import Dict, List import torch from torch.utils.data import Dataset, DataLoader import tqdm from transformers import AutoTokenizer, PreTrainedTokenizerFast import numpy as np import matplotlib.pyplot as plt import seaborn as sns import torch %matplotlib inline . . class Wiki103(Dataset): def __init__(self, tokenizer: PreTrainedTokenizerFast, data_path=&quot;wikitext-103-raw/wiki.train.raw&quot;, max_length: int = 512): self.tokenizer = tokenizer with open(data_path) as dp: # We are self.sentences = [sentence.strip() for sentence in dp if len(sentence.strip()) &gt; 2] self.max_length = max_length def __len__(self): return len(self.sentences) def __getitem__(self, i) -&gt; Dict[str, torch.Tensor]: return self.tokenizer(self.sentences[i], max_length=self.max_length, truncation=True) . tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-cased&quot;, use_fast=True) wiki_dataset = Wiki103(tokenizer=tokenizer) sequence_lengths = [] for example in tqdm.tqdm(iter(wiki_dataset)): sequence_lengths.append(len(example[&quot;input_ids&quot;])) . . 1164464it [04:51, 4000.17it/s] . By plotting the truncated Subword sequence length vs Frequency we see a distribution with a large variance. . with plt.style.context(&#39;fivethirtyeight&#39;): plt.figure(figsize=[7,5]) n, bins, patches = plt.hist(x=sequence_lengths, bins=50, color=&#39;#0504aa&#39;,alpha=0.7, rwidth=0.85) plt.grid(axis=&#39;y&#39;, alpha=0.75) plt.xlabel(&#39;Subword Sequence Length&#39;,fontsize=15) plt.ylabel(&#39;Frequency&#39;,fontsize=15) plt.xticks(fontsize=15) plt.yticks(fontsize=15) plt.ylabel(&#39;Frequency&#39;,fontsize=15) plt.title(&#39;Sequence Length Distribution&#39;,fontsize=15) plt.show() . . Dynamic Padding . From the above graph we can intuit that if we draw random samples from the data to form a mini-batch, we would have few examples which are significantly longer than the rest. This would mean that we would add a lot of padding tokens. This holds even if we clean the very short length instances as noise. . Let&#39;s implement dynamic padding and measure how much. We can use torch&#39;s DataLoader abstraction to do efficient batching with multi-processing. Since our tokenized outputs are of different lengths we have to implement a collate function to pad them dynamically together. We can pass the tokenizer.pad function implemented in huggingface&#39;s tokenizer as the collate function. . def collate_fn(examples: List[Dict[str, torch.Tensor]]) -&gt; Dict[str, torch.Tensor]: # Since huggingface has already implemented this, this function is just to illustrate what a collator does. return tokenizer.pad(examples, return_tensors=&#39;pt&#39;) dataloader = DataLoader(dataset=wiki_dataset, batch_size=32, collate_fn=collate_fn) . Let&#39;s assume that we can use maximum a batch size of 32 for max sequence length of 512 for our model in our training hardware without out-of-memory errors. The tokens per batch would be 512 * 32 = 16384. We can now compute how much of it is padding tokens and what is the distribution of the batch&#39;s sequence length(which depends on the maximum element in the batch). . total_tokens = 0 padding_tokens = 0 batch_lengths = [] for batch in tqdm.tqdm(iter(dataloader)): batched_input_ids = batch[&quot;input_ids&quot;] batch_lengths.append(batched_input_ids.shape[1]) total_tokens += batched_input_ids.numel() padding_tokens += batched_input_ids[batched_input_ids == tokenizer.pad_token_id].numel() . 100%|██████████| 36390/36390 [06:35&lt;00:00, 91.92it/s] . print(f&quot;Total Batches : {len(iter(dataloader))}&quot;) print(f&quot;Padding Tokens : {padding_tokens}&quot;) print(f&quot;Input Tokens : {total_tokens - padding_tokens}&quot;) print(f&quot;Total Tokens : {total_tokens}&quot;) print(f&quot;Padding Tokens % : {(padding_tokens*100)/total_tokens}&quot;) . Total Batches : 36390 Padding Tokens : 244072396 Input Tokens : 119699332 Total Tokens : 363771728 Padding Tokens % : 67.09493267712108 . Surprise, surprise, 67% of our net tokens are padding tokens. This would imply that of all the computations that we do, only 33% of is done for useful work. This starkly highlights the problem with static batch lengths even when accounting for dynamic padding. . Let&#39;s also plot the distribution of batch lengths. . with plt.style.context(&#39;fivethirtyeight&#39;): plt.figure(figsize=[7,5]) n, bins, patches = plt.hist(x=batch_lengths, bins=50, color=&#39;#0504aa&#39;,alpha=0.7, rwidth=0.85) plt.grid(axis=&#39;y&#39;, alpha=0.75) plt.xlabel(&#39;Batch Sequence Length&#39;,fontsize=15) plt.ylabel(&#39;Frequency&#39;,fontsize=15) plt.xticks(fontsize=15) plt.yticks(fontsize=15) plt.ylabel(&#39;Frequency&#39;,fontsize=15) plt.title(&#39;Static Batch - Dynamic Padding Length Distribution&#39;,fontsize=15) plt.show() . . As batches are randomly sampled, we see a normal distribution as we can should expect by the Central Limit Theorem. The frequency in the final bin is deviant because we have a significant number of sentences which we had truncated, hence batches with them will have the maximum sequence length. . General approach to dynamic batching . Instead of drawing samples in random, had we sorted our dataset by length, then we can form batches by packing similar length sequences together into a batch till we reach the maximum number of tokens that we can fit. The maximum number of tokens that can be packed can be derived approximately from our previous memory limit static_batch x max_sequence_length. This allows us to pack more instances in one batch without much padding because the sequences would be of similar lengths after sorting. . We can&#39;t sort the entire dataset because machine learning training is based on the assumption that our instances are drawn independently from an identical distribution (IID). If we were to sort the entire dataset this breaks the assumption as our samples are no longer drawn independently from each other. If sentence length were a confounding factor then the model might fit on this spurious correlation. . We have a trade-off here between statistical power derived from randomization of our samples and lesser error in gradient updates derived from larger batch sizes if we batch dynamically. . Generally, we can have a positive trade off by sampling a window of instances and sorting withing the window and forming batches. . The Dataset we implemented above is a map-style dataset. It implements length and random access to each individual data sample with index (__getitem__). The sampling into batches is taken care of a sampler passed to DataLoader. . I don&#39;t think there is a clean way to implement a map-style dataset and a collate function such that we get batches with dynamic batch sizes but same number of tokens per batch. This comes from the basic mismatch of number of dynamic batches which you can form keeps changing based on the larger window you sample. . So it turns out that we have to do all the shuffling, windowing, sorting and batching inside a iterable-style IterableDataset dataset abstraction. These features are implemented by infinibatch. . 3. Checkpointing . In large datasets, it&#39;s typical not to wait for an entire epoch to checkpoint your model to recover from failures. So to be able to recover and continue training in a deterministic manner, such that it converges to same state if the failure hadn&#39;t occured, we have to checkpoint the random state that controls the order in which our samples are generated. . Infinibatch to the rescue . Infinibatch is a library of checkpointable iterators for randomized data loading of massive data sets in deep neural network training. . It is aimed at simplify the processing of large datasets. It is a collection of pure python classes that implement __iter__ interface. They can be composed inside one another easily and the final composed iterator can be checkpointed as a single entity.You can checkout it&#39;s basic tutorial here. We will use it to address the listed challenges piece by piece and then finally make it work inside IterableDataset and DataLoader abstractions. We will also see the tricks needed to make it work distributed data parallel training. . 1. Loading and shuffling large datasets . Following the infinibatch tutorial, we divide our dataset into multiple gzip chunks of 10000 sentences each. . !mkdir -p wikitext-103-chunks !split -a 4 --lines 10000 --numeric-suffixes --filter &#39;gzip &gt; wikitext-103-chunks/$FILE.txt.gz&#39; wikitext-103-raw/wiki.train.raw train. . We can now create an iterator using infinibatch with a function that can deserialize a shard. Infinibatch takes care of loading multiple in a shuffled order. We can control the amount of deserialized individual examples from the shards be buffered using buffer_size parameter. The library returns a python iterable. We can call next(iterable) or iterate with a for to get the examples. . Note: Passing train=True creates an infinite iterator that cycles after a full run on the dataset. The chunked_dataset_iterator method returns a composition of iterators, you can refer the source code here . import gzip, glob from functools import partial from infinibatch import datasets, iterators def read_chunk(path): with open(path, &quot;rb&quot;) as fp: lines = gzip.decompress(fp.read()).decode(encoding=&#39;utf-8&#39;).splitlines() lines = [sentence.strip() for sentence in lines if len(sentence.strip()) &gt; 2] return iter(lines) sentence_it = datasets.chunked_dataset_iterator( chunk_refs = glob.glob(&#39;wikitext-103-chunks/train.*.txt.gz&#39;), read_chunk_fn = read_chunk, buffer_size = 100000, seed = 1337, train=True, shuffle=True) print(next(sentence_it)) . = = = Oxides and hydroxides = = = . Tensorize our dataset with a map iterator . We can now compose our tokenizer upon our sentence iterator. Infinibatch has two ways of doing this, . MapIterator | ParallelMapIterator | If you use pytorch and need multiprocessing to do costly transformations over your data on the fly, use the ParallelMap and set the num_processes with what you would have with num_workers. And set num_workers=0 in your dataloader. . tokenize_fn = partial(tokenizer, max_length=512, truncation=True) features_it = iterators.ParallelMapIterator( source_iterator=sentence_it, num_processes=4, num_items_per_process=1000, transform=tokenize_fn ) next(features_it) . {&#39;input_ids&#39;: [101, 134, 134, 134, 152, 8745, 4704, 1105, 177, 19694, 8745, 4704, 134, 134, 134, 102], &#39;token_type_ids&#39;: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], &#39;attention_mask&#39;: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]} . 2. Dynamic Batching . Now comes the magic of dynamic batching with BucketedReadaheadBatchIterator. Let&#39;s fix the maximum tokens per batch to 32 * 512 = 16384. This iterator allows you to compute dynamic batch size by iteratively applying a user given function over the current longest example (with length computed by user function) in a sorted read_ahead window. This window is sorted and batches are formed by using the user provided batch_size function iteratively. . Example . Say we want 50 tokens per batch. If we set a read ahead window of 6. Assume we fetch six items [a, b, c, d, e, f] in the first read ahead window. . Sequence id Length . a | 50 | . b | 30 | . c | 20 | . d | 20 | . e | 30 | . f | 20 | . First we sort this window with lengths in decreasing order. The sort order is stable. This preserves the shuffling of equal sized elements from previous iterator. So for our example it would be [a, b, e, c, d, f] . Now we can Compute the dynamic batch sizes by applying the function batch_size iteratively till the window is exhausted. Assume our function is lambda longest_instance: 60 // len(longest_instance). Then applying it once we get first longest item a, current batch size will be 60 //50 = 1. The next longest item remaining can be used to calculate the size of the next batch and so on. So we will end up with [a], [b, e], [c, d, f]. Each of them will have 60 tokens. . You can take a look at the code that does this computation here. . tokens_per_batch = 32 * 512 batches_it = iterators.BucketedReadaheadBatchIterator( source_iterator=features_it, # read_ahead is the number of items to be read from previous iterator, # these are sorted and over which dynamic batches are formed. read_ahead=10000, # key determines the length used to sort and choose the longest remaining record. key=lambda example: len(example[&#39;input_ids&#39;]), # Determines the dynamic batch size batch_size=lambda longest_example: tokens_per_batch // len(longest_example[&#39;input_ids&#39;]), seed=0 ) dynamic_batch_wo_padding = next(batches_it) print(f&quot;Dynamic batch size: {len(dynamic_batch_wo_padding)}&quot;) dynamic_batch_wo_padding = next(batches_it) print(f&quot;Dynamic batch size: {len(dynamic_batch_wo_padding)}&quot;) print(dynamic_batch_wo_padding[:2]) . Dynamic batch size: 133 Dynamic batch size: 99 [{&#39;input_ids&#39;: [101, 1109, 10437, 10745, 1159, 1108, 1103, 1211, 10543, 7631, 1121, 19475, 119, 23364, 1276, 1103, 10745, 1551, 107, 1119, 27944, 107, 1105, 6315, 1115, 1103, 1591, 1329, 1160, 10437, 9307, 1939, 1104, 1141, 117, 1112, 4836, 10437, 24295, 2624, 1108, 2320, 1107, 1103, 1342, 119, 5512, 5912, 1116, 9279, 1276, 1103, 24295, 2624, 1104, 107, 1544, 170, 5955, 107, 22593, 27643, 27725, 170, 107, 12178, 107, 1113, 1103, 20694, 23676, 119, 26835, 3798, 1276, 1103, 107, 3321, 107, 2971, 1104, 10437, 24295, 2624, 1106, 1129, 1103, 1342, 112, 188, 2026, 3282, 4197, 117, 1112, 1218, 1112, 1103, 1263, 10745, 1551, 1115, 4977, 1122, 119, 8746, 2202, 1115, 21105, 1158, 1551, 1127, 107, 17562, 3345, 107, 1496, 1106, 1103, 12177, 10437, 2469, 1158, 119, 2061, 2202, 1115, 1103, 1342, 1125, 170, 10437, 2469, 9285, 107, 1177, 2213, 107, 1115, 1122, 1108, 1593, 4763, 1106, 2469, 22493, 1219, 11716, 117, 1112, 1103, 16408, 1733, 1766, 2230, 1108, 1579, 170, 1248, 1481, 4315, 10322, 5172, 119, 102], &#39;token_type_ids&#39;: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], &#39;attention_mask&#39;: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {&#39;input_ids&#39;: [101, 2651, 4385, 1127, 1211, 1510, 1276, 8910, 15364, 1158, 12354, 2928, 1105, 1103, 15553, 1104, 5400, 15246, 117, 1105, 11561, 1317, 3050, 131, 172, 16717, 3848, 6126, 117, 16358, 2881, 5018, 117, 23639, 11239, 5326, 6126, 117, 1105, 7812, 21212, 119, 140, 16717, 3848, 2116, 1110, 1103, 7764, 1271, 1111, 16307, 172, 16717, 3447, 1105, 1143, 17670, 4199, 131, 18249, 1105, 4600, 5511, 1113, 1499, 1104, 170, 2095, 119, 9800, 2881, 5018, 1127, 4122, 9417, 1116, 1115, 11479, 2894, 1103, 2095, 117, 3525, 15024, 1106, 5211, 1120, 117, 1137, 3968, 4546, 1113, 117, 19450, 1120, 1103, 2259, 1104, 1103, 2095, 1443, 1515, 1106, 8290, 1679, 24755, 1361, 1193, 1166, 1103, 172, 16717, 3848, 6126, 117, 8267, 15952, 2310, 1106, 1231, 6163, 26264, 1183, 1783, 119, 6603, 11239, 5326, 6126, 1127, 2576, 25344, 1113, 1499, 1104, 170, 2095, 1114, 18199, 1115, 2148, 4546, 1106, 1129, 2434, 1113, 1126, 3437, 1120, 1103, 2259, 1104, 1103, 2095, 1107, 170, 1861, 4633, 1106, 16358, 2881, 5018, 119, 102], &#39;token_type_ids&#39;: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], &#39;attention_mask&#39;: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}] . Now we can collate our examples and see how much this scheme has saved us. Since a training iterator is infinite, we will recreate our iterators with a non-infinite iterator. . sentence_it_finite = datasets.chunked_dataset_iterator( chunk_refs = glob.glob(&#39;wikitext-103-chunks/train.*.txt.gz&#39;), read_chunk_fn = read_chunk, buffer_size = 100000, seed = 1337, train=False, shuffle=False ) features_it_finite = iterators.ParallelMapIterator( source_iterator=sentence_it_finite, num_processes=4, num_items_per_process=1000, transform=tokenize_fn ) batches_it_finite = iterators.BucketedReadaheadBatchIterator( source_iterator=features_it_finite, read_ahead=10000, # Determines the window for the bucket which # will be sorted and converted to batches. key=lambda example: len(example[&#39;input_ids&#39;]), # Determines the length used # to sort and choose the longest remaining record. batch_size=lambda longest: tokens_per_batch // len(longest[&#39;input_ids&#39;]), # Determines the dynamic batch size seed=0 ) collate_fn = partial(tokenizer.pad, return_tensors=&#39;pt&#39;) tensors_it_finite = iterators.MapIterator( batches_it_finite, transform=collate_fn ) . total_batches_dynamic = 0 total_tokens_dynamic = 0 padding_tokens_dynamic = 0 batch_lengths_dynamic = [] for batch in tqdm.tqdm(tensors_it_finite): total_batches_dynamic += 1 batched_input_ids = batch[&quot;input_ids&quot;] batch_lengths_dynamic.append(batched_input_ids.shape[1]) total_tokens_dynamic += batched_input_ids.numel() padding_tokens_dynamic += batched_input_ids[batched_input_ids == tokenizer.pad_token_id].numel() . 7645it [08:45, 14.54it/s] . print(f&quot;Total Batches : {total_batches_dynamic}&quot;) # Seeing the tqdm stats. print(f&quot;Padding Tokens : {padding_tokens_dynamic}&quot;) print(f&quot;Input Tokens : {total_tokens_dynamic - padding_tokens_dynamic}&quot;) print(f&quot;Total Tokens : {total_tokens_dynamic}&quot;) print(f&quot;Padding Tokens % : {(padding_tokens_dynamic*100)/total_tokens_dynamic}&quot;) . Total Batches : 7645 Padding Tokens : 3848626 Input Tokens : 119699332 Total Tokens : 123547958 Padding Tokens % : 3.1150866937031854 . We have reduced the % of padding tokens per epoch from 67% to just around 3%.The total batches needed to process it in the same max tokens per batch limitation hence got reduced nearly five times from 36390 to 7642. . The processing time is just one minute extra. I guess that might be due to IO, but you could try benchmarking that with more rigour. . Now, plotting the length distribution for dynamic batches. . with plt.style.context(&#39;fivethirtyeight&#39;): plt.figure(figsize=[7,5]) n, bins, patches = plt.hist(x=batch_lengths_dynamic, bins=50, color=&#39;#0504aa&#39;,alpha=0.7, rwidth=0.85) plt.grid(axis=&#39;y&#39;, alpha=0.75) plt.xlabel(&#39;Batch Sequence Length&#39;,fontsize=15) plt.ylabel(&#39;Frequency&#39;,fontsize=15) plt.xticks(fontsize=15) plt.yticks(fontsize=15) plt.ylabel(&#39;Frequency&#39;,fontsize=15) plt.title(&#39;Dynamic Batch - Dynamic Padding Length Distribution&#39;,fontsize=15) plt.show() . . We now see that the expected per batch sequence length has reduced from 300 to 200. . 3. Checkpointing . One cool feature of infinibatch is that you can checkpoint a particular state in which the composed iterators is at and restore (rewind?) it back to that state. This is very cool considering it works recursively on the composed iterators and even on infinite iterator. Let&#39;s recreate our iterators and check this out. . sentence_it = datasets.chunked_dataset_iterator( chunk_refs = glob.glob(&#39;wikitext-103-chunks/train.*.txt.gz&#39;), read_chunk_fn = read_chunk, buffer_size = 100000, seed = 1337, train=False, shuffle=False ) features_it = iterators.ParallelMapIterator( source_iterator=sentence_it, num_processes=4, num_items_per_process=1000, transform=tokenize_fn ) batches_it = iterators.BucketedReadaheadBatchIterator( source_iterator=features_it, read_ahead=10000, # Determines the window for the bucket which # will be sorted and converted to batches. key=lambda example: len(example[&#39;input_ids&#39;]), # Determines the length used # to sort and choose the longest remaining record. batch_size=lambda longest: tokens_per_batch // len(longest[&#39;input_ids&#39;]), # Determines the dynamic batch size seed=0 ) collate_fn = partial(tokenizer.pad, return_tensors=&#39;pt&#39;) tensors_it = iterators.MapIterator( batches_it, transform=collate_fn ) . . initial_state = tensors_it.getstate() print(&quot;Initial State of composed iterators&quot;, initial_state) # Draw 5 batches batches = [next(tensors_it) for _ in range(5)] print(f&quot;Current State after sampling 5 batches: {tensors_it.getstate()}&quot;) # Reset the Iterator tensors_it.setstate(initial_state) # Redraw 5 batches redraw_batches = [next(tensors_it) for _ in range(5)] print(f&quot;State after resampling 5 batches: {tensors_it.getstate()}&quot;) # Check equal all_equal = True for b1, b2 in zip(batches, redraw_batches): for k in b1: if torch.all(b1[k].eq(b2[k])): continue all_equal = False break if not all_equal: break print(f&quot;All items drawn after resetting are equal: {all_equal}&quot;) . Initial State of composed iterators {&#39;source_state&#39;: None, &#39;random_state&#39;: None, &#39;num_served&#39;: 0} Current State after sampling 5 batches: {&#39;source_state&#39;: {&#39;source_state&#39;: None, &#39;flattened_items_yielded&#39;: 0}, &#39;random_state&#39;: (3, (2147483648, 766982754, 497961170, 3952298588, 2331775348, 1811986599, 3100132149, 3188119873, 3937547222, 215718963, 3315684082, 2978012849, 2428261856, 1298227695, 1704729580, 54668373, 3285201915, 3285178464, 1552935063, 988471319, 3135387943, 1691402966, 2757551880, 416056905, 907387413, 1072924981, 33903495, 2168419592, 2429050353, 831159753, 430343641, 3315943586, 1761671042, 864453023, 334804929, 1627478028, 2596811275, 3468733638, 3994375553, 1457139722, 3139722021, 1334790738, 2656639915, 3535811098, 1464315470, 2397423927, 885719490, 1140895889, 3284299483, 2854516462, 2734973817, 147484763, 792049954, 114360641, 3345458839, 1159898878, 1410498733, 2242989638, 453922141, 1344019764, 413870456, 3089405849, 1494382840, 470157779, 4266372830, 2831181573, 1361928602, 1589253513, 1381373062, 753045124, 987032420, 781978839, 2953638767, 3258570111, 3006718191, 1675218601, 1854232715, 3655829819, 1731242722, 2192104666, 1736665161, 740150002, 1195833394, 1610203160, 159492766, 4041488705, 3128952632, 2867295744, 3272632449, 886824304, 1791482600, 221114776, 3867175393, 4020804062, 1077871826, 1298953503, 996366221, 4149754679, 2483052703, 2615558283, 274318093, 1716359450, 4099129961, 1026774175, 288240973, 1459347562, 2365566296, 3690105224, 3065780221, 2050634722, 2652606621, 3185241207, 3026457375, 3456165734, 1880121515, 3398461093, 1795638629, 2379692076, 608668379, 1261955525, 84456522, 1913485156, 106878280, 757183891, 2913957588, 160418091, 2025664758, 141497907, 1657818026, 3053760160, 672193054, 4157546743, 223046484, 1623470498, 1201972930, 675008814, 684162366, 1738776330, 3025656654, 159760723, 1908867305, 3933381342, 2545706671, 467196949, 1427819885, 842150314, 4032903454, 2140851898, 3269883445, 975813755, 4177392955, 1556690684, 2535611513, 462962732, 67591358, 1729610528, 2025206740, 3153739740, 3255032049, 4186226368, 1070144624, 3107867195, 1621006038, 63742485, 835629717, 3189842019, 3950227584, 3184714559, 841836938, 1685394870, 657939920, 766156242, 1412314179, 1048281639, 4037161120, 2044490307, 1923947830, 3900790422, 907554295, 276417304, 860658646, 3574201134, 3508771399, 2110232300, 1636296241, 1405006077, 1093408401, 3243057343, 1519791182, 1994660136, 3829840937, 2644974199, 957955566, 3487641161, 1646922510, 1907939989, 3836029453, 3429168778, 201307778, 72550089, 2464394982, 1695794191, 3344785682, 996786130, 3589457196, 1241754792, 1291082245, 4224603667, 1194379475, 2693491244, 881186965, 2705535111, 445306946, 440274268, 1980827733, 2482488861, 3205215943, 2119332222, 2928713046, 1418736938, 652581136, 2474070665, 2208621536, 4171251876, 2303664214, 443762656, 2981912989, 2199228311, 2652261633, 3166738494, 3443009210, 3498764432, 424010848, 4065487566, 2262993542, 1756076712, 1477098233, 2742171915, 306185806, 3610666541, 923091830, 1034267993, 2336668648, 1880719718, 676878038, 3788797208, 3763351494, 3985428106, 1101865631, 1130501258, 3672967388, 3432003530, 4124438011, 1660392285, 4025484827, 2108074566, 3815409682, 42955331, 3248965569, 1643835718, 1246665668, 1071162194, 3814069229, 115491158, 985096811, 3311029186, 2990827378, 3101633320, 1648574497, 1470117052, 174145027, 2019894819, 2035501481, 459104123, 3507464599, 2093352659, 3369174406, 618767835, 4009895756, 935587447, 3956987426, 33753995, 307782427, 2473424805, 1440371818, 2382619594, 2138695812, 3164510238, 1318650933, 2910086616, 3886677510, 566832801, 3718063320, 1559818704, 183047272, 1142362855, 26306548, 645536402, 3875596208, 2272778168, 3512733409, 1897046338, 38248886, 2570759766, 1806313150, 860304898, 2433450338, 4124013408, 1216634590, 1275388896, 1169566669, 652504502, 761221427, 1448403764, 3129135949, 2513214949, 1269533687, 2413509541, 1226750363, 2450740925, 4094137910, 945759293, 3636927736, 3178020081, 2509964157, 3878869300, 1848504895, 2018369720, 1579755740, 1023627943, 924838836, 2653160914, 1812804174, 1521323076, 4012390528, 1338763317, 2608655937, 16022784, 1672945066, 2177189646, 2944458483, 2213810972, 1369873847, 1224017670, 130901785, 3595066712, 2259115284, 3316038259, 455873927, 2917250465, 3599550610, 1502173758, 684943436, 3079863840, 3144992244, 942855823, 1771140188, 2118780653, 3411494225, 2711180217, 4239611184, 1371891067, 3398566397, 3105518599, 1310665701, 3345178451, 2959821156, 242241789, 2148966880, 3192740583, 404401893, 3605380577, 1446464038, 3920522056, 2577523013, 1079274576, 286634372, 1752710796, 2351075979, 981312309, 3410516352, 3468455736, 1938779182, 1592494371, 1533303080, 88045436, 438252489, 1220512168, 3487004938, 3724852871, 1073434882, 3728218947, 2977555283, 4105408406, 3553772656, 1462006821, 3917158017, 119003006, 3470530198, 3439192457, 2829375771, 3555715155, 32324691, 588735808, 1459221702, 803072782, 2699519868, 1530797005, 79738580, 671990400, 4289511388, 3207115447, 2584684068, 832698998, 760958416, 1217440464, 2517898131, 2418819938, 3629956222, 3445024962, 206619378, 365007395, 522114139, 1707954431, 540423623, 1786750801, 369253262, 4239016754, 147889201, 1637777773, 236798285, 2806120188, 586972608, 2201782716, 1323327827, 819485723, 406078680, 3407345698, 1537169369, 1821691865, 527271655, 3751827102, 1465426495, 3321682429, 2179672664, 401355478, 1068871880, 24609462, 1403522408, 2311580015, 1532058170, 3877815340, 1768430711, 1619755157, 2832904331, 475102697, 354987331, 3295386430, 2816873951, 1039415736, 363972779, 1499307670, 2895506264, 3746345349, 2678027234, 3251899088, 955392878, 2329157295, 1343358773, 309573887, 2410178377, 2843173466, 361132917, 1755816798, 1319204283, 609284796, 1998842567, 1892325921, 223190385, 1483015769, 2876023365, 3876009312, 3199738344, 491524099, 160383137, 1219178873, 3870310498, 1114580266, 4279604166, 855339774, 1983818547, 2297848784, 4118592947, 4084409863, 2225095054, 4215601993, 946447434, 4205503762, 146088676, 778046685, 1876936928, 3157333726, 2173097090, 3215738813, 4135448234, 1219619643, 1936128689, 2897130162, 3336043946, 3779039524, 4200886837, 1359380925, 3402593091, 3140713935, 50855190, 3122065768, 1501584468, 2512255124, 687125154, 2666013386, 837819715, 3057258172, 3653455791, 2868624990, 322131992, 42534870, 4036564806, 798099710, 3533853670, 190914037, 3726947981, 2601169403, 602059656, 1365668439, 1918780004, 394790500, 277566007, 3891847777, 3365421094, 3139612253, 1380519090, 1183088424, 4203794803, 3049949521, 4214159484, 3446206962, 1875544460, 3207220027, 3288287026, 913535288, 178159620, 1410694581, 4190575040, 880731713, 1427805121, 404869072, 3413191414, 2865934056, 2899472677, 4239222733, 688404529, 3923323887, 933651074, 1199453686, 642723732, 2850614853, 3104368451, 3054041024, 3129913503, 2805843726, 1829781129, 3479062313, 650272704, 4224852052, 4085038685, 2616580676, 1793860711, 585126334, 2995262791, 520446536, 3855655015, 1571815563, 2240778227, 2051010344, 1694977983, 788402852, 1988089041, 2035558649, 1800063056, 1234412692, 2490862867, 417320514, 2415019489, 3374117797, 136034611, 898704236, 1247106941, 3923519397, 3563607190, 2454738671, 3522360389, 2672645476, 146828884, 3985140042, 4233949333, 1184742586, 860278824, 2815489967, 983483427, 3190081845, 3288865305, 3575181235, 1292151129, 4007823805, 4049420597, 3499391972, 1611182906, 1721268432, 2944249577, 2487212557, 789127738, 4027610014, 1057334138, 2902720905, 624), None), &#39;num_served&#39;: 5} State after resampling 5 batches: {&#39;source_state&#39;: {&#39;source_state&#39;: None, &#39;flattened_items_yielded&#39;: 0}, &#39;random_state&#39;: (3, (2147483648, 766982754, 497961170, 3952298588, 2331775348, 1811986599, 3100132149, 3188119873, 3937547222, 215718963, 3315684082, 2978012849, 2428261856, 1298227695, 1704729580, 54668373, 3285201915, 3285178464, 1552935063, 988471319, 3135387943, 1691402966, 2757551880, 416056905, 907387413, 1072924981, 33903495, 2168419592, 2429050353, 831159753, 430343641, 3315943586, 1761671042, 864453023, 334804929, 1627478028, 2596811275, 3468733638, 3994375553, 1457139722, 3139722021, 1334790738, 2656639915, 3535811098, 1464315470, 2397423927, 885719490, 1140895889, 3284299483, 2854516462, 2734973817, 147484763, 792049954, 114360641, 3345458839, 1159898878, 1410498733, 2242989638, 453922141, 1344019764, 413870456, 3089405849, 1494382840, 470157779, 4266372830, 2831181573, 1361928602, 1589253513, 1381373062, 753045124, 987032420, 781978839, 2953638767, 3258570111, 3006718191, 1675218601, 1854232715, 3655829819, 1731242722, 2192104666, 1736665161, 740150002, 1195833394, 1610203160, 159492766, 4041488705, 3128952632, 2867295744, 3272632449, 886824304, 1791482600, 221114776, 3867175393, 4020804062, 1077871826, 1298953503, 996366221, 4149754679, 2483052703, 2615558283, 274318093, 1716359450, 4099129961, 1026774175, 288240973, 1459347562, 2365566296, 3690105224, 3065780221, 2050634722, 2652606621, 3185241207, 3026457375, 3456165734, 1880121515, 3398461093, 1795638629, 2379692076, 608668379, 1261955525, 84456522, 1913485156, 106878280, 757183891, 2913957588, 160418091, 2025664758, 141497907, 1657818026, 3053760160, 672193054, 4157546743, 223046484, 1623470498, 1201972930, 675008814, 684162366, 1738776330, 3025656654, 159760723, 1908867305, 3933381342, 2545706671, 467196949, 1427819885, 842150314, 4032903454, 2140851898, 3269883445, 975813755, 4177392955, 1556690684, 2535611513, 462962732, 67591358, 1729610528, 2025206740, 3153739740, 3255032049, 4186226368, 1070144624, 3107867195, 1621006038, 63742485, 835629717, 3189842019, 3950227584, 3184714559, 841836938, 1685394870, 657939920, 766156242, 1412314179, 1048281639, 4037161120, 2044490307, 1923947830, 3900790422, 907554295, 276417304, 860658646, 3574201134, 3508771399, 2110232300, 1636296241, 1405006077, 1093408401, 3243057343, 1519791182, 1994660136, 3829840937, 2644974199, 957955566, 3487641161, 1646922510, 1907939989, 3836029453, 3429168778, 201307778, 72550089, 2464394982, 1695794191, 3344785682, 996786130, 3589457196, 1241754792, 1291082245, 4224603667, 1194379475, 2693491244, 881186965, 2705535111, 445306946, 440274268, 1980827733, 2482488861, 3205215943, 2119332222, 2928713046, 1418736938, 652581136, 2474070665, 2208621536, 4171251876, 2303664214, 443762656, 2981912989, 2199228311, 2652261633, 3166738494, 3443009210, 3498764432, 424010848, 4065487566, 2262993542, 1756076712, 1477098233, 2742171915, 306185806, 3610666541, 923091830, 1034267993, 2336668648, 1880719718, 676878038, 3788797208, 3763351494, 3985428106, 1101865631, 1130501258, 3672967388, 3432003530, 4124438011, 1660392285, 4025484827, 2108074566, 3815409682, 42955331, 3248965569, 1643835718, 1246665668, 1071162194, 3814069229, 115491158, 985096811, 3311029186, 2990827378, 3101633320, 1648574497, 1470117052, 174145027, 2019894819, 2035501481, 459104123, 3507464599, 2093352659, 3369174406, 618767835, 4009895756, 935587447, 3956987426, 33753995, 307782427, 2473424805, 1440371818, 2382619594, 2138695812, 3164510238, 1318650933, 2910086616, 3886677510, 566832801, 3718063320, 1559818704, 183047272, 1142362855, 26306548, 645536402, 3875596208, 2272778168, 3512733409, 1897046338, 38248886, 2570759766, 1806313150, 860304898, 2433450338, 4124013408, 1216634590, 1275388896, 1169566669, 652504502, 761221427, 1448403764, 3129135949, 2513214949, 1269533687, 2413509541, 1226750363, 2450740925, 4094137910, 945759293, 3636927736, 3178020081, 2509964157, 3878869300, 1848504895, 2018369720, 1579755740, 1023627943, 924838836, 2653160914, 1812804174, 1521323076, 4012390528, 1338763317, 2608655937, 16022784, 1672945066, 2177189646, 2944458483, 2213810972, 1369873847, 1224017670, 130901785, 3595066712, 2259115284, 3316038259, 455873927, 2917250465, 3599550610, 1502173758, 684943436, 3079863840, 3144992244, 942855823, 1771140188, 2118780653, 3411494225, 2711180217, 4239611184, 1371891067, 3398566397, 3105518599, 1310665701, 3345178451, 2959821156, 242241789, 2148966880, 3192740583, 404401893, 3605380577, 1446464038, 3920522056, 2577523013, 1079274576, 286634372, 1752710796, 2351075979, 981312309, 3410516352, 3468455736, 1938779182, 1592494371, 1533303080, 88045436, 438252489, 1220512168, 3487004938, 3724852871, 1073434882, 3728218947, 2977555283, 4105408406, 3553772656, 1462006821, 3917158017, 119003006, 3470530198, 3439192457, 2829375771, 3555715155, 32324691, 588735808, 1459221702, 803072782, 2699519868, 1530797005, 79738580, 671990400, 4289511388, 3207115447, 2584684068, 832698998, 760958416, 1217440464, 2517898131, 2418819938, 3629956222, 3445024962, 206619378, 365007395, 522114139, 1707954431, 540423623, 1786750801, 369253262, 4239016754, 147889201, 1637777773, 236798285, 2806120188, 586972608, 2201782716, 1323327827, 819485723, 406078680, 3407345698, 1537169369, 1821691865, 527271655, 3751827102, 1465426495, 3321682429, 2179672664, 401355478, 1068871880, 24609462, 1403522408, 2311580015, 1532058170, 3877815340, 1768430711, 1619755157, 2832904331, 475102697, 354987331, 3295386430, 2816873951, 1039415736, 363972779, 1499307670, 2895506264, 3746345349, 2678027234, 3251899088, 955392878, 2329157295, 1343358773, 309573887, 2410178377, 2843173466, 361132917, 1755816798, 1319204283, 609284796, 1998842567, 1892325921, 223190385, 1483015769, 2876023365, 3876009312, 3199738344, 491524099, 160383137, 1219178873, 3870310498, 1114580266, 4279604166, 855339774, 1983818547, 2297848784, 4118592947, 4084409863, 2225095054, 4215601993, 946447434, 4205503762, 146088676, 778046685, 1876936928, 3157333726, 2173097090, 3215738813, 4135448234, 1219619643, 1936128689, 2897130162, 3336043946, 3779039524, 4200886837, 1359380925, 3402593091, 3140713935, 50855190, 3122065768, 1501584468, 2512255124, 687125154, 2666013386, 837819715, 3057258172, 3653455791, 2868624990, 322131992, 42534870, 4036564806, 798099710, 3533853670, 190914037, 3726947981, 2601169403, 602059656, 1365668439, 1918780004, 394790500, 277566007, 3891847777, 3365421094, 3139612253, 1380519090, 1183088424, 4203794803, 3049949521, 4214159484, 3446206962, 1875544460, 3207220027, 3288287026, 913535288, 178159620, 1410694581, 4190575040, 880731713, 1427805121, 404869072, 3413191414, 2865934056, 2899472677, 4239222733, 688404529, 3923323887, 933651074, 1199453686, 642723732, 2850614853, 3104368451, 3054041024, 3129913503, 2805843726, 1829781129, 3479062313, 650272704, 4224852052, 4085038685, 2616580676, 1793860711, 585126334, 2995262791, 520446536, 3855655015, 1571815563, 2240778227, 2051010344, 1694977983, 788402852, 1988089041, 2035558649, 1800063056, 1234412692, 2490862867, 417320514, 2415019489, 3374117797, 136034611, 898704236, 1247106941, 3923519397, 3563607190, 2454738671, 3522360389, 2672645476, 146828884, 3985140042, 4233949333, 1184742586, 860278824, 2815489967, 983483427, 3190081845, 3288865305, 3575181235, 1292151129, 4007823805, 4049420597, 3499391972, 1611182906, 1721268432, 2944249577, 2487212557, 789127738, 4027610014, 1057334138, 2902720905, 624), None), &#39;num_served&#39;: 5} All items drawn after resetting are equal: True . Since the state of the iterator is just a dictionary, you can serialize it along with your model weights and restore them to continue training from exact point where you have checkpointed it. . Making Infinibatch work with Pytorch Dataloaders . Infinibatch by its very nature can be used only with IterableDataset. The training iterator with shuffling is infinite, so you must limit the training batches to some n steps if you want to maintain the notion of &quot;epochs&quot; to start validation. Or you can eschew whole notion of epochs by validating every nth step or both. . Note: The multi processing workers of DataLoader should be set to zero, with num_workers=0. Rather use ParallelMapIterator to parallelize your pre-processing. . Note: While using IterableDataset in the typical multi-gpu DistributedDataParallel (ddp) setup, you should pass instance_rank and num_instances to have different slices of data distributed to different training devices. . Warning: When using finite iterators with ddp for validation set, if you split the data using instance_rank option, the validation can get stuck.This can happen either when your dataset is not divisible by number of ddp processes or doing dynamic batching caused an uneven number of batches produced for each instance. So it&#8217;s better to do the validation in one GPU setting instance_rank=0. This is a quick hack, if you find a better option please let me know in the comments. . from torch.utils.data import IterableDataset class IterableCheckpointedDataset(IterableDataset): &quot;&quot;&quot; Wraps a CheckpointableIterator into a PyTorch IterableDataset, which is recognized by its type by PyTorch&#39;s DataLoader class. &quot;&quot;&quot; def __init__(self, source: iterators.CheckpointableIterator, should_reset: bool): super().__init__() self._source = source self._source_state = source.getstate() self._should_reset = should_reset def __iter__(self): # this is called in the forked clone worker_info = torch.utils.data.get_worker_info() assert ( worker_info is None or worker_info.num_workers == 1 ) # not supported since we can&#39;t get at the checkpoint for each worker if self._should_reset: # For training, since it&#39;s infinite iterator, if we train for # `n` batches with total instances less than dataset size # it&#39;s better not to reset the iterator by itself will cycle back # with a new shuffle order when all the instances are iterated once. self._source.setstate(self._source_state) return self._source def create_wiki_dataloader(chunks_glob: str, tokenizer: PreTrainedTokenizerFast, is_train: bool, max_seq_len: int = 512, tokens_per_batch: int = 2 ** 16, num_workers: int = 4, buffer_size: int = 100000, seed: int = 1337) -&gt; DataLoader: sentence_it = datasets.chunked_dataset_iterator( chunk_refs = glob.glob(chunks_glob), read_chunk_fn = read_chunk, buffer_size = buffer_size, seed = seed, train=is_train, shuffle=is_train # Shuffle Only on Train ) tokenize_fn = partial(tokenizer, max_length=max_seq_len, truncation=True) features_it = iterators.ParallelMapIterator( source_iterator=sentence_it, num_processes=num_workers, num_items_per_process=1000, transform=tokenize_fn ) batches_it = iterators.BucketedReadaheadBatchIterator( source_iterator=features_it, read_ahead=10000, key=lambda example: len(example[&#39;input_ids&#39;]), batch_size=lambda longest: tokens_per_batch // len(longest[&#39;input_ids&#39;]), seed=seed ) collate_fn = partial(tokenizer.pad, return_tensors=&#39;pt&#39;) tensors_it = iterators.MapIterator( batches_it, transform=collate_fn ) dataset = IterableCheckpointedDataset( source=tensors_it, should_reset=not is_train #Reset only for validation ) return DataLoader(dataset, # Very important to set this to 0. num_workers=0, # Important as we have already batched. batch_size=1, # Since batch has only one member which has all the #tensors already collated, we just return it. collate_fn=lambda x: x[0] ) tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-cased&quot;, use_fast=True) train_loader = create_wiki_dataloader(&#39;wikitext-103-chunks/train.*.txt.gz&#39;, tokenizer=tokenizer, is_train=True) val_loader = create_wiki_dataloader(&#39;wikitext-103-chunks/train.*.txt.gz&#39;, tokenizer=tokenizer, is_train=False) #print(next(iter(train_loader))) #print(next(iter(val_loader))) .",
            "url": "https://saiprasanna.in/posts/efficient-dynamic-batching-of-large-datasets-with-infinibatch/",
            "relUrl": "/posts/efficient-dynamic-batching-of-large-datasets-with-infinibatch/",
            "date": " • Sep 13, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Roam Research - Software for building a Second Brain",
            "content": "Introduction . Roam Research is a revolutionary note-taking/knowledge management software. It is designed with the idea that data structure for a second brain should be associative (graph) rather than a rigid hierarchy. It is meant for everyone who needs to manage their knowledge effectively. Their founder aims for the product to become more or less excel for knowledge management. Professor Balaji Srinivas introduced it to me. I have been using this for about a month and fell in love with its features. Canadian philosopher who predicted the web tells that “Medium is the message”. The properties of a medium in which communication occurs can impact society a lot. I believe that roam is one such tool that shifts the medium of note-taking in a groundbreaking manner. The features of Roam allow you to arrange for the serendipity of ideas, unexpected connections with your past, present and future selves. It is built on simple building blocks that come together (emergent property) to make the whole greater than the sum of its parts. Each building block of Roam might look simple if looked separately, but together they become very powerful. Note: It is still in beta with a pricing yet to be announced. But if you try it I think you will share this sentiment. Here is the Roam white paper written by founders on a Public roam database about why they think Roam is revolutionary. . Pain points of most knowledge-management software . I have used popular note-taking software like evernote, One-note, Zoho notebook etc. There was always huge friction with these tools. Getting them to work for Note taking, journaling, and project management was such a pain. The only tool that came close was org-mode in Emacs. I used it for the past one year. But even it had a lot of friction due to the reasons I will expand upon below. And emacs is most definitely not for popular use, it is only for the chosen few who are blessed enough to reject the cult of mouse. . Failure of Files inside Folders way of organization. . Most note-taking software or even physical note-taking follow a file-folder system or a single hierarchy of bullets inside bullets (Outlining tools) for organizing notes. I will refer to this as “files inside folder” model going forward now. This system creates nested hierarchy of categories, sub-categories and so on. Inside which your notes are put. . Why this approach fails? . You might have started taking notes/journaling etc for a few days and abandon it after some time. The following are the main reasons I think this happens. . Friction caused by a static hierarchy of the folder system . Every time you want to write a note you have to decide where to put it in the hierarchy. You have to ask yourselves which notebook/folder/file should I write this, for it to be useful? . Poor Return in investment for good note-taking . You painstakingly take notes or journal your exercise regime or note down something. But it never surfaces automatically when you write something related. Most notes are passive and useless unless you look for them. Most notes are hidden uselessly in the hierarchy where you put it in. . Practical Scenario where folder model fails . Say you had a discussion with your friend about note-taking while sipping a coffee in a cafe. You talk about some personal stuff which say you want to put in your journal. S/he brings about about a new book which you find interesting and want to read. Also, you really like the coffee in the cafe and want to note it down in your list of the favourite coffee shops. . Now how do you take notes in this case? In a rigid hierarchy, for this note to be useful you have to file it under multiple places “To read books”, “Daily Journal”, “Meets”, “Friend’s name” But that is highly impractical because of the effort/redundancy involved. You would have to copy paste same information in multiple places or manually create links in multiple pages. And if those pages don’t exist you would have to create them. . Simple Tags don’t solve this problem . Most note-taking software provide tagging to solve this problem. But this creates friction of adding tags to everything you write. And more importantly the tags are flat. i.e. They have no hierarchy between them. Tags solve searching for notes, but not disovery. This makes it problematic if you want to discover notes when you stumble through a specific context automatically. . We think in a associative (Graphs) manner not as a rigid hierarchy (Trees) . Every thought/idea in our brain has a bunch of associations. Associations like people who introduced us to the idea, what we want to do with it, associations with books we read about it, tasks we completed based on it, tasks we want to do, date in which we did it etc. So this forms a “graph” (in computer science) where every idea is a node and is linked to many other ideas. When you think of an idea naturally you get reminded of the stuff it is associated with. But the problem is we forget stuff, which is the reason why we are doing note-taking in the first place. What if a note-taking software allows you to mirror how the brain works? . Building blocks of Roam that differentiate it . These are the fundamental set of features that make Roam what it is. Since showing is better than telling, I am including 1-minute videos of how each fundamental building blocks of Roam work. . Basic Layout . Roam’s page layout is like any other “outlining” tool. ie It has bullets which can be nested within each other infinitely and the bullets can be collapsed. There is no folder system. All notes are be seen from “All Pages” view in left side-bar, but it is rarely needed because of way roam’s navigation is organized. Roam’s home page is the Daily notes page where you can see pages titled by dates. This is the basic dumping ground for all your quick note-taking and journaling, daily tasks, habit tracking for the day. . Friction Free Link Creation . Basic way roam allows linking between pages is typing the note title inside two square brackets “[[]]” which includes autocomplete/search to all notes page titles. One Important thing here is if the note page doesn’t exist a new page gets create. This would seem weird when coming from other applications. But it serves a big purpose you will see next. . Bi-directional Linking . This is the key feature of Roam. When a page is linked to, when you visit the page you can see all the places where it has been referred. . Say you write “I was reading this cool article on [[Deep Learning]]”, The Deep Learning page will have a back link to all the places it has been mentioned. So every page becomes akin to a tag, but associations between them form a dynamic/organic hierarchy. . “Every page is a tag, and every tag is a page” - Nat Elison’s blog on Roam . Un-linked References . If you have already mentioned a topic in lots of notes, but didn’t create a page for it and link them. Roam has got your back with its super cool un-linked references. When you create a new page, you can easily bulk link every other page that has mentions of the current page. . Ability to refer or embed any block/bullet anywhere . You can link to or embed any block written anywhere in roam notes. Roam prompts a search/autocomplete to any block you have written in all the pages. How awesome is that? To create a block reference two open Parentheses (( Or type /Block Reference. You can also embed the entire block using Block embed. Type /Block Embed When embedding or linking to a block you can see the places it has been used by clicking on the number which appears at top right side of a block. . All back-links and block embeds are editable . The coolest part of roam is all blocks (bullets) displayed by back-links and block embeds are editable. You can edit embedded notes and back-links with no duplication. So you easily remix (refactor?) notes by creating a new note with just embeds from multiple other notes in other places. . Navigating with Full-text Search . Roam provides full text search of all the blocks and titles. You can create a brand new page which is not linked to any other page directly from search. . Graph View . Since every note is basically a node in graph, roam easily allows a bird’s eye of your entire graph in the “Graph Overview” page. A more helpful feature is ability to view what nodes current page has connections to and navigate visually. . Side Bar . Roam sidebar allows you to open multiple notes at a time. This is really useful when you want to aggregate knowledge across notes. . Filters on Bi-directional Links . When you have too many bi-directional links in a page, you can filter them to include or exclude other links. . Building a second brain with Roam . In this section, I will describe how Roam can be used for multiple use-cases at once thereby building your second brain. . Note taking . If you read a lot and want to retain the knowledge. Reading stuff and writing it in your own words will be a good way to test gaps in understanding. This article a effective way to take smart notes using roam. I found this summary of a book called “[[How to Take Smart Notes - Book]] really helpful. . Journal . Writing a journal is a bread and (peanut) butter of roam. Daily notes encourages you to write daily at anytime. It feels really encouraging to journal in roam as unlike other apps because of the back-links. Anything recorded will automatically get associated with all the topics. . Task management . Roam supports basic todos, you can use links to link tasks to the date it has to be done using date-picker. The key advantage here is your project management tasks can easily be linked to the meeting notes, research notes, and journal etc. Getting things done (GTD) is a popular method to manage tasks. It is very easy to implement that in roam with the aid of back-links. You can read how to adopt GTD in roam here. . Bookmarks . You can easily put links into roam with creating tags/links. Tags are same as “[[]]” links, just that the font is greyed out. . Personal CRM . Personal crm is for maintaining a list of people, their contact, birthdays, how you met them or anything else you want to maintain about them. In roam you can easily create pages for people, and refer it in your daily notes. So when you go to the person’s page, you can see all the places h/she has been mentioned. . Content Creation . For writing new content, you can easily remix stuff which you wrote across different pages in a new page. So you will never have the feeling of starting at a blank page when you have done your research and taken notes on it. . Other useful features . Query . This is an advanced feature, where you can query on your graph to show blocks that satisfy boolean conditions. Like show me all the blocks with todos with high priority etc. . Shortcuts . This is useful to keep the most important projects . Tables, Diagrams, Kanban boards . Tables can be created using /Table command followed by nested bullets. . Embedding Media . Embed tweet by just pasting a twitter link. Type backslash followed by image markdown to insert a markdown for image embed. /Image Markdown Alternatively images can be uploaded by pasting directly or with backslash command /Upload a image backslash command for it to be uploaded and the markdown inserted automatically. For youtube videos, use /Embed Youtube Video command. Interesting point here is all the embeds are markdown or markdown like plain text syntax. No proprietary garbage. . Publishing your Roam Notes . You can share a note by its URL to public as read-only or even allow public edits. . Sharing your second Brain to form a Hive mind . Currently you can share the entire roam database and collabrate with peers. More fine-grained controls of sharing parts of the graph are in the works. . Works offline (Progressive Web App) . The mobile apps are coming shortly, but the web app is designed so well that it can function offline. . Exports to plain text . This is important for anyone who cares about not getting locked out of your data. Since the product is in development better to take regular backups if you plan to use it. . Conclusion . The concept of bi-directional editable media in Roam can be transposed to many other products with lots of unstructured text trapped in a rigid hierarchy. Roam is a well-designed innovative product that leapfrogs over existing products. This I think is because of unorthodox thinking of its founders (Conor White-Sulivan and Joshua Brown) to build useful tools for augmenting our brain. Turing award winner and Computer Science legend Alan Kay once said: “The best way to predict the future is to invent it.”. He also thought that there is a lot more that can be done to make computers truly augmenting the intellect. I think these founders are trying to fulfil that dream. Roam seems like an application that can give compounding returns of value for the knowledge put in it. This is totally refreshing in an age where apps suck your attention. Finally, I am more into engineering than content writing for productivity software. This software was so good that I couldn’t help writing this article to share it with others. .",
            "url": "https://saiprasanna.in/posts/roam-research-software-for-building-a-second-brain/",
            "relUrl": "/posts/roam-research-software-for-building-a-second-brain/",
            "date": " • Mar 25, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Neural Module Networks",
            "content": "Research Paper - https://arxiv.org/abs/1511.02799 . Authors - Jacob Andreas, Marcus Rohrbach, Trevor Darrell, Dan Klein . Key Idea . Parse questions of visual QA into a description of compositions of functions. These functions are neural networks called Neural Modules. Execute the neural networks and reweigh the resulting label using question representation. . Task - Visual Question Answering . Given a question like “What color is the coffee mug?” and an image we want to predict the answer. . Prior approaches . End to End neural networks . Use a CNN to vectorize the image and RNN to vectorize the question and use a feed forward network to classify the answer. This is a black box trying to answer in one shot. . Semantic Parsing approach . Parse the question into logical expressions, image into logical representation of the world and use logic based reasoning to solve the problem. This is more compositional. . Motivation . Combine the representational capacity of neural nets and compositionality of symbolic approach. . “Rather than thinking of question answering as a problem of learning a single function to map from questions and contexts to answers, it’s perhaps useful to think of it as a highly-multitask learning setting, where each problem instance is associated with a novel task, and the identity of that task is expressed only noisily in language.” . Simple example - “Is this a truck?” - Needs single task to be performed, namely truck or not classification. . Compositional example - “What is the object to the left of the tea pot?” - Needs one to find the teapot, detect object to its left, then classify the object. . Architecture . Neural Modules . Identify set of modules that can be composed to solve all/most tasks. Modules can be thought of as a function parametrized by a neural network, with a type signature. Data Types - Image, Unnormalized attention map, labels . Strings -&gt; Modules . Parsing Use few rules on dependency parse of the question to convert it into a structured query. e.g. “Is there a circle next to a square?” -&gt; is(circle, next-to(square)) Layout “All leaves become attend modules, all internal nodes become re-attend or combine modules dependent on their arity, and root nodes become measure modules for yes/no questions and classify modules for all other question types.” The queries could come from anywhere not just natural language question. As long as they can be converted to a layout in the end. . Answering . An RNN is used to process the question and predict a label directly without looking into the image. This is combined with the final label from the root node of the Neural Modules using geometric mean to get the final result. This is done for 2 reasons Syntactic Regularity/Prior When converting to structured query, certain syntactic elements are lost. For e.g. What is in the sky? and What are in the sky? both result in what(fly). But answer varies from kite to kites. Semantic Regularity/Prior Some answers are unreasonable just by inspecting the question. For example, What colour is the bear? eliminates all non-colour answers. . Benchmarks . They try this in vqa dataset - https://visualqa.org/ a huge dataset with natural images and questions with answers. Since VQA doesn’t have many deep compositional questions, they use shapes a synthetically generated dataset. . Examples . What colour is his tie? .",
            "url": "https://saiprasanna.in/posts/neural-module-networks/",
            "relUrl": "/posts/neural-module-networks/",
            "date": " • Mar 2, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "ACL 2019 Conference Summary",
            "content": "My colleague Ananda and I attended ACL 2019 conference at the enchanting city of Florence. All the accepted papers can be accessed here. Here’s the summary of interesting trends and also specific research work that caught my eye at the conference. A note of thanks to my employer at Zoho for sponsoring us to attend. . I wrote this summary an many months ago and forgot posting it. Better late than never I guess. . Grammatical Error Correction . Among the ACL workshops, Building Educational Applications (BEA) Workshop had a Grammar Error Correction competition. | . The system description papers for this competition were presented as posters in the conference. . Three tracks were present in the competition. Restricted track - Only organizer provided human labelled parallel (error and corrected sentence pairs) data can be used. (No restriction on synthetic data) Unrestricted track - Any data including private data can be used. Low Resource track - No human labelled data can be used. | Interestingly, the winning team (Edinburgh + Microsoft)’s submission for Track 1 also beat Track 2 without using additional restricted data. | Synthetic data generated by corrupting good grammatical sentences from news, books and wikipedia are the techniques used overall by top performing teams. | . Multi-Lingual Models . MultiLingual models is a hot area of research now. Earlier results where using single model to perform tasks on multiple languages has shown promising results. . Lots of papers on multi-lingual shared models were presented. | Paper - Choosing Transfer Languages for Cross-Lingual Learning | . Rise of Automated Metrics . Until recently, we compare model outputs with human written sentences for translation, summarization etc. This can artificially penalize models that generate sentences with equivalent meaning but not same words. There are couple of papers that train models to score quality of the output. Then use these model scores as reward for reinforcement learning. (FYI reinforcement learning is only used for fine tuning, none of the seq2seq models can be trained from scratch using it) . Paper - This Email Could Save Your Life: Introducing the Task of Email Subject Line Generation This paper uses automated score instead of typical NGram match (ROUGE) score for summarization task. | Paper - Beyond BLEU:Training Neural Machine Translation with Semantic Similarity | Paper - Sentence Mover’s Similarity: Automatic Evaluation for Multi-Sentence Texts | . Statistical Evaluation . If we have two architectures and couple of datasets, how to say empirically one is better than the other? Few questions are how to compare two models on the same dataset, across multiple datasets, across various hyperparameter configurations. Problems in applying frequentist tests on the metrics such as accuracy, f1-score etc are that assumptions such as Independent and Identically distributed (IID) cannot be made for deep learning datasets. So we cannot assume that the score the model gets in one dataset is “independent” of the score on another dataset. Statistical tests that don’t assume underlying distribution are needed. Recent statistical methods/tests to do so are being developed and some were presented at the conference. . Paper - Deep Dominance - How to Properly Compare Deep Neural Models | Paper - Bayes Test of Precision, Recall, and F1 Measure for Comparison of Two Natural Language Processing Models | . Bayesian Methods . Attended a very detailed tutorial on it. The presenter has summarized the evolution of research in this area and the current papers. Here’s link to the detailed slides for fellow Bayesians. | . Analyzing Neural Nets and Interpretability . There is an entire sub-fields of research into analyzing and interpreting neural networks. . BERTology . “BERT-ology” papers that explore what linguistic structures do pre-trained models like BERT learn. . Paper - What Does BERT Look at? An Analysis of BERT’s Attention | . BlackBoxNLP Workshop . An entire workshop devoted for analyzing what Neural Networks learn. . Paper - On the Realization of Compositionality in Neural Networks Interesting paper studying what is required for neural models to compose two very trivial functions. | Paper - GEval: Tool for Debugging NLP Datasets and Models | . Formal Languages Workshop . An entire small workshop devoted to finding what Formal Languages (Finite state Automata, etc) neural networks can learn. e.g. Can we reduce a RNN to Weighted Finite State Machine (which is far more interpretable, amenable to theory etc). Although this area sounds exciting to me, I was unable to attend it as I was in an another workshop. Slides from talk of Noah Smith’s talk on Rational Recurrences at this workshop. . Neuroscience and NLP . Neuroscience labs have started to use deep learning. An interesting conjunction of research in NLP and neuroscience research in correlating ANN representations with brain signals was presented. . Paper - Relating Simple Sentence Representations in Deep Neural Networks and the Brain The researchers try to find relationship between deep learning language representations and brain signals. Paper of interest is where they predict neural brain patterns using pre-trained ANN models like BERT. | . Language Emergence in Multi-Agent systems . In this frontier, people try train models to solve some task by communicating symbols. Researchers analyze the properties of language used by the agents to solve the task and how it compares with properties of human language. . Paper - Word-order Biases in Deep-agent Emergent Communication | . Conversational AI . Neural Models for selecting conversation from past history, detecting intent and slot fitting are all increasingly being deployed by companies. | PolyAI (a startup at Singapore shipping conversational AI) shared three interesting papers. Their slides are also interesting. | On a related note, Baidu has is doing impressive research and engineering on meeting transcription. They have a stack that does speech to text, translating the text as its spoken (a problem that needed separate research as the text would be incomplete), detecting english phrases being spoken (code switching) and then NLP over the transcribed text. | . Translation . Lots of new work on adapting translation models for low-resource languages. | Unsupervised translation, Multi-lingual translation models are few areas of research. | Unbabel a YC funded startup doing translation systems shared lots of interesting and important results. Slides from their talk. This company employs a hybrid system where human translators do “post-edits” on machine translations. And some of their system work in real-time. | . Contextual Search using Neural Representations at scale . This paper has demonstrated a system which does dense vector search on entire wikipedia for open domain QA. . Scaling search on neural vectors to do question answering on entire wikipedia on CPU - https://github.com/uwnlp/denspi . Demo - http://allgood.cs.washington.edu:15001/ .",
            "url": "https://saiprasanna.in/posts/ACL-2019-Conference-Summary/",
            "relUrl": "/posts/ACL-2019-Conference-Summary/",
            "date": " • Feb 10, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Semantic Legion",
            "content": "I am guilty of spamming people in the degree one of my network with too many links in topics that fancy the Legion of varied interests that haunt me. Following the suggestion of Ananda Seelan, I am consolidating my link blasts into a considated blog post format, thus begins the “Semantic Legion”. This exercise might help organize the “Legion” in my head and maybe lead to more focused blog posts. . “My name is Legion, for we are many.” . Machine Learning . The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks . The lottery ticket hypothesis suggests that big Deep Neural Nets train better than smaller nets because they get lucky. Essentially like someone who has purchased more number of lottery tickets. . Prune a large neural network by zeroing the bottom x% of weights by magnitude. This can be done one shot or iteratively while training. | Reset the obtained subnetwork weights to the exact weights you randomly intialized before training the large neural network. | The pruned subnetwork converges to similar test error rate as the full network or even better in the same number of epochs. | The authors notice that if you were try some other initialization for the subnetwork or even sample from similar distribution it doesn’t work. Hence they hypothesise that the larger network essentially got lucky. | Though the subnetwork is smaller, computations will need sparse matrix multiplication optimizations to be faster. . Understanding the generalization of ‘lottery tickets’ in neural networks . Facebook extends the study and checks it for various architectures, tasks and optimizer setting. The lottery ticket phenomena seems to occur in most places. The lottery ticket subnetworks generalize across datasets. This blog post is a summary of multiple papers by Facebook AI group in analyzing this phenomena. . New Theory Cracks Open the Black Box of Deep Learning . Information bottleneck theory a hypothesis about how neural nets learn is creating some buzz. One of the claims is that the output of earlier layers have more mutual information with the inputs while final layer outputs have more mutual information with the outputs than the inputs. The information about input gets compressed in each layer. . . Lucy Reading-Ikkanda/Quanta Magazine; adapted from arXiv:1703.00810 [cs.LG] . Evolution of Representations in the Transformer . This is a great practical example of using information bottlenecks to analyze neural nets behaviour. This research (accompanied by inspirationally well written blog post) compares the evolution of representations in three different NLP encoder models. And in part explains some empirical findings such as why de-noising objective works better than casual language model objective or encoders from translation objective for transfer learning. . Universal Adversarial Triggers for Attacking and Analyzing NLP (Wallace et al. EMNLP 19) . This paper finds magic spells that make your NLP models malfunction. They find phrases that cause a specific model prediction when concatenated to 𝘢𝘯𝘺 input from a dataset. These phrases are reported to work across architectures for the same dataset. . Triggers cause: 1. GPT-2 to spew racism 2. SQuAD models to answer &quot;to kill american people&quot; for 72% of questions asking &quot;Why...&quot; 3. Classification models to drop from 90% accuracy to 1% . AllenNLP Interpret . This is a great set of features for interpretability added to AllenNLP library. . We present AllenNLP Interpret, a toolkit built on top of AllenNLP for interactive model interpretations. The toolkit makes it easy to apply gradient-based saliency maps and adversarial attacks to new models, as well as develop new interpretation methods. AllenNLP interpret contains three components: a suite of interpretation techniques applicable to most models, APIs for developing new interpretation methods (e.g., APIs to obtain input gradients), and reusable front-end components for visualizing the interpretation results. . The amazing thing here is with implementing a simple interface in your model predictor allows you to apply a suite of interpretability techniques for our models. . AIDungeon2 is here . This is a real fun application of langauge model generation. Nick Walton has adapted GPT2 to generate user guided “Choose your own” text RPG type games. Now you can try out anything you fancy by just issuing commands like “Cast a spell to Reverse entropy”. A truly open world RPG with a AI dungeon master. The model weaves your actions to generalte plausible/surreal story continuations. Hacker News discussion about it. The nature of the model make them generate surreal dream like scenarios. There are glaring consistency issues in the generated story lines. This points to a symbolic gap that is yet to be filled. . . Source: aiweirdness.com . Controlling Text Generation with Plug and Play Language Models . On the topic of controlling language models, uber research has found a way to control the generation of models like GPT2 without fine-tuning. . Quantum Computing, Linear Algebra, Tools for Learning . Quantum Computing for the Very Curious . I wanted to try out Micheal Nielsen’s (of neuralnetworksanddeeplearning.com fame) Quantum computing article. This long-form educational article attempts a unique teaching method by embedding flash cards (anki cards) and reminding readers via email to revisit the cards. I got around doing it at behest of the amzing Professor Balaji (a teacher of mine) who gave this as an exercise to test Linear Algebra understanding. Prior knowledge of the truly abstract nature of linear algebra (basis, linear transformations, linear combinations) really helped me to grok the essay. . The learning approach taken by this article (embedding flash cards + reminders) article shows how computing medium can be extended to augment our understanding. This scratches the surface of Alan Kay’s vision of computers being tools that extend our mind. . Augmenting Long-term Memory . If you’re curious about spaced repition flash card approach to learn new math theorems, machine learning concepts etc Micheal has written extensively about it in the above link. . Anki Flash Cards with Spaced Repitition . The free app Anki is example of good software aimed at expanding our capabilites rather than popular objective of draining attention. It has web, desktop and mobile versions for creating Anki (flash) cards with spaced repitition tracking. I am in the process of adopting it for my learning. Not yet successful in integrating it fully, will blog more about my experience in future. . Polar App . Related learning tool I found is Polar. . “A powerful document manager for web pages, textbooks, PDFs, and anything you want to read. Supports tagging, annotation, highlighting and keeps track of your reading progress.” . It doesn’t have a firefox extension yet. But it allows creating anki cards that can be synced to Anki app from web highlights. This helps in creating a learning expereince like the quantum computing blog for any document. . Philosophy . Would aliens understand lambda calculus? . Platonism vs Aristotelianism is an age old debate in philosophy. Professor Balaji (a teacher of mine) had a strong notion that the current mathematics we have is strongly influenced by our spatio-visual sense. Stumbled upon the above post which makes similar claims. It claims that certain cognitive priors are necessary to converge upon ideas which some consider as universal. . I don’t know enough to lean on any side of the debate heavily. But my intution lies with universality/platonism of physics, mathematics and computatability. I think even if Alien’s use some other metaphors to arrive at Lambda Calculus, the underlying notion of universal computability (if correct) will be the same. . New AI Strategy Mimics How Brains Learn to Smell . I am now exploring search systems over neural net generated representation (vector spaces). This generally involves approximate methods such as Locality senstive Hashing. The method described in this post was interestingly derived from the sense of smell of fruit-flies. This lends some weight top the notion that our cognitive reliance on certain senses (vision) makes some ideas intutive, but exploring outside it can expand our horizons. (Purely my speculation to be taken with a grain of salt.) . Programming Languages . Type State Pattern . To eliminate errors make them impossible in runtime is a mantra I stand behind. Programming Patterns that are finally entering mainstream (after stewing in the academic functional world) such as Optional are moving errors to compile time. Among the patterns, type state caught my eye. Using rust’s borrow checker and other langauge features allows one to build compile time state machines. They can be as simple as allowing the compiler to disallow methods such as read on file references that are closed. Or it can be taken one step beyond to write a full blown state machines that track the current state in compile time. ie Say you have an API that needs a handshake to be performed before sending, you can ensure in compile time that the “send” method can be called only after “handshake” is called. How awesome is that. . Why Monads matter? . This article explains what the usally hyped functional programming concept of monad solves for a imperative programmer. I have not dived deeply into any functional language yet. Seeing how even weakly adopted fucntional programming concepts such as Optionals (algebraic data types) and Optional Chaining (which is a monad) makes me question what is the cost with which the programming world is ignoring Functional paradigmn. Are the functional languages difficult to learn, or is it exposure bias towards imperative languages? Or do we need the functional abstractions to be put in better terms for people to grok them? Only time will tell. .",
            "url": "https://saiprasanna.in/posts/semantic-legion-1/",
            "relUrl": "/posts/semantic-legion-1/",
            "date": " • Dec 8, 2019"
        }
        
    
  
    
        ,"post5": {
            "title": "SemEval 2019 - Semi-Supervised Domain Adaptation for Suggestion mining",
            "content": "SemEval Workshop regularly has been conducting tasks in NLP to evaluate the progress in the field. . I and my colleague Ananda Seelan participated in this year SemEval’s Suggestion mining task (Task 9). Here is our submission to be published in NAACL 2019 proceedings, and the code is on github. . This blog is a summary of the key techniques and ideas which influenced this work. . Suggestion Mining Task . The suggestion mining task in brief is a text classification task to find whether a sentence contains a suggestion. . Example, . Suggestion - It would be nice if they had vegan options. | Non Suggestion - This restaurant has good vegan options. | . About 8k sentences scrapped from technical forumns were provided as training data. The task was divided into two subtasks. . Subtask A - Evaluation on same domain - technical forums posts. | Subtask B - Evaluation on out of domain - hotel reviews. | . The catch for subtask B is human labelled data in hotel reviews domain is not allowed for training. Our model was placed third place in the leaderboard for Subtask B. . Key Techniques . We used simple convolutional neural networks for text classification. And we applied transfer learning and semi-supervised learning for the tasks. . Transfer Learning . The current trend in machine learning for NLP is to using pre-trained language models. We used google’s recently published BERT model as our representation layer. Take a look at http://jalammar.github.io/illustrated-bert/ for a good description of how pre-trained models work for NLP. . Semi-Supervised Learning . In ACL 2018 conference Melbourne, I attended two talks which impacted the work in this paper. One was Sebastien Ruder’s talk on Strong baselines for semi-supervised learning in NLP. The conclusion of Sebastian Ruder, Barbara Plank (2018) was that classic machine learning techniques for semi-supervised learning such as Tri-Training prove as strong baseline in NLP with neural nets. Sebastien has a very accessible and thorough blog post explaining the techniques. They have also made their code available on github. . We applied a variant of tri-training. We use three models of the same architecture trained initially on data bootstrap sampled from the tech reviews data. The three models are used to iteratively label unlabelled data from hotel reviews domain. Agreement of labels between two models is used as way to select sentences to be added to next iteration of training. Pseudo-code and detailed explanations can be found in the paper. Or you might as well look to the code, as its way simpler than a dry description of it might suggest. . Statistical Significance . The other work published and presented in ACL 2018 that influenced this paper is Rotem Dror’s “The Hitchhiker’s Guide to Statistical Significance in NLP”. . We report confidence intervals for five random seeds for all our experiments. And we also do pair-wise significance testing via McNemar’s test to evaluate whether pair-wise model performance on the test set vary significantly. . Metrics . . McNemar’s Test . .",
            "url": "https://saiprasanna.in/posts/semeval-19-semi-supervised-domain-adaptation-for-suggestion-mining/",
            "relUrl": "/posts/semeval-19-semi-supervised-domain-adaptation-for-suggestion-mining/",
            "date": " • Apr 7, 2019"
        }
        
    
  
    
        ,"post6": {
            "title": "God's own Programming Language",
            "content": "Found this interesting blog post explores why many programmers hold a high regard for an ancient programming language which you might not have heard about or use daily. . For God wrote in Lisp code When he filled the leaves with green. The fractal flowers and recursive roots: The most lovely hack I’ve seen. And when I ponder snowflakes, never finding two the same, I know God likes a language with its own four-letter name. . Poem from twobithistory.org . XKCD Comics . Also a good read are the posts on LISP by Paul Graham of YCombinator/HackerNews fame. . How to attain Nirvana with the God’s own language? . To attain programming nirvana - Start reading the SICP Book. . SICP (Structure and Interpretation of Computer Programs) is a introduction to computer science book. It can change how you view even simple constructs we use for code (like loops, if, etc). A book that will teach timeless concepts in programming which you would never come across easily. I started reading it ages ago, haven’t completed it , the first few chapters themselves were sufficiently mind blowing. . Original book . A modern interactive version Now you can run the examples of SICP book God’s own language in the browser with godforsaken javascript. And laugh morosely on the irony of running LISP in a half baked language (javascript) which was inspired from it. . A Distilled version with illustrations - Smaller, condensed version. . To use an analogy, if SICP were about automobiles, it would be for the person who wants to know how cars work, how they are built, and how one might design fuel-efficient, safe, reliable vehicles for the 21st century. The people who hate SICP are the ones who just want to know how to drive their car on the highway, just like everyone else. - Peter Norvig on SICP . (How to Write a (Lisp) Interpreter (in Python)) . Follow this guide to implement your own LISP Interpreter in an hundred lines of python. You might think, “Is he crazy to ask a language beginner to implement the interpreter before learning it?” Answer is while I am partly crazy LISP is not, it has the simplest structure of all programming languages. It’s just lists duh! (LiSt Processing). . LISP for AI . You can practise LISP for Artificial Intelligence algorithms with Peter Norvig’s book on “Paradigms of Artificial Intelligence Programming”. . Say you don’t want Nirvana, but something more pragmatic . You can learn Clojure to use God’s Language to do some real world wizardry. Clojure is a form of LISP which is modern, functional and runs on JVM. Brave Clojure is one of the best sources out there for Clojure. For front end development/nodejs there is clojure-script which compiles down to javascript. . Learning Clojure is the best way you can improve as a programmer because it introduces you to powerful concepts implemented in a simple, cohesive, and practical language. You learn Clojure here. Therefore, Brave Clojure is your very best friend when it comes to programming.” And lo, the syllogism was born! . XKCD Comics . A Note and a Zen Koan . Use any language that solves your problem and learn about others which have different paradigms conceptually like LISP, etc when you find the time. It a enjoyable exercise if you are curious about digging deeply into what makes computers tick. . As Master Foo says says, . Master Foo once said to a visiting programmer: “There is more Unix-nature in one line of shell script than there is in ten thousand lines of C.” . The programmer, who was very proud of his mastery of C, said: “How can this be? C is the language in which the very kernel of Unix is implemented!” . Master Foo replied: “That is so. Nevertheless, there is more Unix-nature in one line of shell script than there is in ten thousand lines of C.” . The programmer grew distressed. “But through the C language we experience the enlightenment of the Patriarch Ritchie! We become as one with the operating system and the machine, reaping matchless performance!” . Master Foo replied: “All that you say is true. But there is still more Unix-nature in one line of shell script than there is in ten thousand lines of C.” . The programmer scoffed at Master Foo and rose to depart. But Master Foo nodded to his student Nubi, who wrote a line of shell script on a nearby whiteboard, and said: “Master programmer, consider this pipeline. Implemented in pure C, would it not span ten thousand lines?” . The programmer muttered through his beard, contemplating what Nubi had written. Finally he agreed that it was so. . “And how many hours would you require to implement and debug that C program?” asked Nubi. . “Many,” admitted the visiting programmer. “But only a fool would spend the time to do that when so many more worthy tasks await him.” . “And who better understands the Unix-nature?” Master Foo asked. “Is it he who writes the ten thousand lines, or he who, perceiving the emptiness of the task, gains merit by not coding?” . Upon hearing this, the programmer was enlightened. . For more funny hacker koans, visit here and here. . I am currently learning emacs-lisp for I have become a convert/evangelizer of the Church of Emacs on a Starship called Spacemacs. But that’s a post/sermon (;P) for another time. .",
            "url": "https://saiprasanna.in/posts/gods-own-programming-language/",
            "relUrl": "/posts/gods-own-programming-language/",
            "date": " • Apr 4, 2019"
        }
        
    
  
    
        ,"post7": {
            "title": "Learning Longer-term Dependencies in RNNs with Auxiliary Losses (ACL 2018)",
            "content": "Paper by Trieu H. Trinh, Andrew M. Dai, Minh-Thang Luong, Quoc V. Le . TLDR; RNNs can learn very long sequences when auxiliary loss for predicting input sequence (forward and backward) from different timesteps is added. . Problem being addressed . Recurrent neural nets in theory can learn arbitrarily long sequences, but in practice suffer from problems like vanishing gradients etc. Techniques to reduce vanishing gradients problem like LSTM alone don’t work for very long sequences. . RNN has a better tradeoff when it comes to memory requirements compared to CNN based networks or vanilla Transformer nets. When processing very large sequences this becomes important. . Proposed Method . Take the hidden state of main RNN used for a given task at sampled timesteps, use another RNN at sampled intervals, and try to predict the input sequence to certain time steps. Truncated BPTT (Back propagation through time) to few timesteps would give a new loss. . Evaluation and results . Evaluation is done on MNIST, CIFAR-10, Stanford dog dataset is given as sequence of pixels to an RNN with classification being the target. Since the pixels are flattened to sequential input, spatial location information is now across whole range of the sequence, requiring long dependencies to be formed to get good results. The authors also test it on character based classification on dbpedia. This technique achieves very significant results on long sequences compared to existing LSTMs, Transformers etc. . Opinions . This paper makes a significant experiment to improve a crucial behavior of RNNs on long sequences. The ablation study is well done. This auxiliary loss reminded me of the World models paper where the task of predicting future states improves current tasks output. .",
            "url": "https://saiprasanna.in/posts/learning-long-term-dependencies-rnn/",
            "relUrl": "/posts/learning-long-term-dependencies-rnn/",
            "date": " • Jun 5, 2018"
        }
        
    
  
    
        ,"post8": {
            "title": "Neural Open Information Extraction (ACL 2018)",
            "content": "Paper by Lei Cui, Furu Wei, Ming Zhou . TLDR; Models Open information extraction as Sequence to Sequence problem using neural nets. . What is Open information extraction? . Open Information Extraction aims to extract one or more (Entity 1, Relationship, Entity 2) tuples from sentences. . Example . &quot;Deep learning is a subfield of machine learning.&quot; | v (Deep learning, is a subfield of , machine learning) . Existing methods use handcrafted rules written on syntatic parsers which have poor perfomance and suffer from cascading of errors. This paper applies neural networks to get better accuracy and alleviate errors. . How is the problem modeled? . Sequence 2 Sequence task, where you take a source sentence as input and output the information tuple as sequence separated by special tokens (open arg1, arg2, arg3 and close arg1, arg2 arg3). Currently the model is trained for single tuple extraction. . “Deep learning is a subfield of machine learning.” -&gt; “ Deep learning is a subfield of machine learning ” . The paper uses a LSTM based Sequence to Sequence model with attention. The source and target vocabulary is same. If unknown word target is found, the model forms the target probability vector by placing attention on of source sequence as probability of the corresponding source words occurring. . What are the benchmarks? . Evaluating on a large benchmark dataset, this model gets 0.473 AUC (Area under curve) for Precision-Recall on top 5 predictions of this model (I think generated from beam search) has better performance , significantly higher than existing systems. Among existing systems OpenIE has the best score of 0.373 AUC . . One observation of authors is only 11 % of the predictions of Neural model matches with Open IE (rule based) model, but the performance is higher. This could be due to neural model generalizing on some of the patterns hard to capture by rules. .",
            "url": "https://saiprasanna.in/posts/neural-open-information-extraction/",
            "relUrl": "/posts/neural-open-information-extraction/",
            "date": " • Jun 4, 2018"
        }
        
    
  
    
        ,"post9": {
            "title": "Dependency Injection - What, Why and How?",
            "content": "We are going to explore dependency injection with emphasis on swift iOS development. But the concept applies to most object oriented languages. We will also see some practical considerations on applying DI in iOS environment. This article is result of my deep dive into implementing DI, and learning about various practical, theoretical aspects of it. . What is Dependency Injection? . “Dependency Injection” is a 25-dollar term for a 5-cent concept”. . is a often repeated maxim regarding DI. . In its essence DI means wherever possible, replace object creation inside a piece of code by providing it from outside that piece of code. Hence the term. . Constructor, Property, Method are where we usually do object creation, and that can be replaced from outside. So we end up with 3 types of injection. . Constructor Injection | Property Injection | Method Injection | We will explore they 3 types in “How ..?” section. . Why Dependency Injection? . Let’s dwelve into this with help of a scenario. . Scenario - Koala Koder! . Say you have an app with a user login. A user model struct/class encapsulates the logged in user data. . Suppose you are a Koala Koder( a programmer who is as lazy as a koala bear). You perhaps made a solution which is quick and dirty. Store the user model inside NSUserDefaults, and fetch it via properties. And as we all know how apple loves its singleton classes, we follow them making UserModel a singleton. . class UserModel { static let sharedInstance = UserModel() var name: String { get { return NSUserDefaults.standard.string(forKey: &quot;userName&quot;) ?? &quot;&quot; } set { NSUserDefaults.standard.set(newValue, forKey:&quot;userName&quot;) } } func greet() -&gt; String { return &quot; (name), Vannakam :)&quot; } } . We write our app with this usermodel in mind, and UserModel.sharedInstance is everywhere in our app. . . Problems, problems everywhere… . 1. Unit Tester Vader strikes! . A senior developer suddenly turns to the dark side, and starts ranting about unit testing. He/She will not let apps which are not unit tested pass the code review. . . 2. A wild New Use case appears! . And if that isn’t enough a new use case should be supported. Our app should now support multiple users. . . 3. “Lets move to &lt;insert any serialization library here&gt;” . Now we also reached a point where userdefaults didn’t scale and wish to migrate to new data serialization method. Now even our getters and setters inside the UserProfile is not safe. . Why are there problems? . So we find ourselves in deep trouble. Lets analyze why so. . 1. Singletons are hard to test . Now if you want to unit test a viewcontroller that uses this singleton. . class UserProfile: UIViewController { func viewDidLoad() { greetingLabel.text = UserModel.sharedInstance.greet() //... } } . In unit testing you create a UserProfile object, call viewDidLoad or other methods manually. Now you have to verify whether UserModel.sharedInstance.greet() was called. . Since UserModel.sharedInstance is immutable, either we can’t replace it with our mock class extending UserModel, which overrides greetUser, and sets a flag which can be checked. So our testing coverage comes down. . 2. Instantiation inside our code constraints creates strong coupling, creating harmful constraints . In our scenario, by using a singleton, we tied our codebase to a single UserModel but now our app needs multiple user models. So in general, using singletons will make it hard to adapt to new use cases. What you thought as a singleton suddenly is not so single anymore. . But consider that we didn’t use singleton, but instantiated UserModel, by calling UserModel() wherever we needed it. . class UserProfile: UIViewController { let userModel = UserModel() func viewDidLoad() { userNameLabel.text = userModel //... } } . We can’t support our new usecase of multiple users, without userprofile class knowing about multi-usermodel, or some other global allowing UserModel() to return the correct usermodel, these solutions are ugly hacks, which add complexity by either giving too much knowledge to classes or using globals and forgoing object oriented encapsulation. . 3. Concrete Type usage creates strong coupling . Consider the UserProfile, it uses NSUserDefaults, now suppose we move to coredata to save our data, we are again in trouble because of using singleton inside. Our UserProfile rather needed only just a way to serialize some data, it didn’t need to know about WHAT we use for serialization. This is the key insight to keep in mind when thinking about dependency injection. . DI to the rescue . DI helps to solve the variety of issues that we face above, with regard to Unit Testablity, Singletons and strong coupling we face above. . How to do Dependency Injection? . Constructor Injection . So we will be injecting a serializer into UserModel via constructor. . Move dependency to constructor, and if possible make it a interface/protocol type instead of concrete class/struct . So we remove singleton access to userdefaults. And rather pass a serializer protocol which has methods we require for serialization to constructor. . protocol Serializing { func string(forKey defaultName: String) -&gt; String? func set(_ value: Any?, forKey defaultName: String) } class UserModel { static let serializer: Serializing init(_ serializer: Serializing) { self.serializer = serializer } var name: String { get { return serializer.string(forKey: &quot;userName&quot;) ?? &quot;&quot; } set { serializer.setObject(newValue, forKey:&quot;userName&quot;) } } } . Since we are Koala Koder, we just make the Serializing protocol methods to same ones in NSUserdefaults, so we can make NSUserDefaults conform easily by . extension NSUserDefaults: Serializing {} . . . Now to use user defaults as our serializer, we do the following. . UserModel(serializer: NSUserDefaults.standard) . For our fancy multiple user use case we can also do this. (Note: did this in a hurry, it may have edge cases, just providing it as a illustration) . struct MultiUserSerializer: Serializing { let serializer: Serializing init(_ serializer: Serializing) { self.serializer = serializer } // So we fetch the current currentUserId and use that to prefix stored data private func fetchCurrentUserId() -&gt; String { return serializer.string(forKey: &quot;CURRENT_USER_ID&quot;) ?? &quot;0&quot; } func string(forKey defaultName: String) -&gt; String? { return serializer.string(forKey: fetchCurrentUserId() + &quot;:&quot; + defaultName) } func set(_ value: Any?, forKey defaultName: String) { return serializer.set(value, forKey defaultName: fetchCurrentUserId() + &quot;:&quot; + defaultName) } } //Instantiate using userdefaults, assume we implemented contextFetcher somewhere else let multiUserSerializer = MultiUserSerializer(serializer: NSUserDefaults.standard) let userModel = UserModel(serializer: multiUserSerializer) . So now userModel fetches data, and sets data to the current userID without even knowing about it. We could also use something other than NSUserDefaults.standard to serialize the data in top level. . The point is by removing replacing the concrete dependency NSUserDefaults.standard out of UserModel and swapping it to Serializing protocol we can now satisfy the new usecase of multi user modelling easily. This is the core idea behind of loose coupling. . Also we can now unit test User Model by just passing a Mock implementation of Serializing . Dependency injection works even with only concrete types. . Sometimes you don’t have the time, or are sure that concrete type used will not have to be changed. You can still DI the concrete type without bothering with protocol creation, just for the sake of it. . For example: . class UserProfile: UIViewController { let userModel: UserModel // This works only if you don&#39;t use storyboards init(userModel: UserModel) { self.userModel = userModel } func viewDidLoad() { userNameLabel.text = userModel.name } } . We move UserModel creation out of the controller, without creating any protocol for UserModel properties. . We still gain advantages of unit testability using ordinary mock objects, and also we are free to use our multi user serialized UserModel , hence making UserProfile support multi user model without changing any logic in user profile. . (But if you have to do something to notify changes in data, that is seperate topic) . let multiUserSerializer = MultiUserSerializer(serializer: NSUserDefaults.standard) let multiUserModel = UserModel(serializer: multiUserSerializer) let userProfile = UserProfile(userModel: multiUserModel) . My constructor is a monster now :( . A common problem which you will come across is, your UIViewController (if you don’t use storyboards) or any class where you do DI becomes a to big. . class UserProfile: UIViewController { init(userModel: UserModel, apiService: APIService, x: XService, y:YService, z: ZService, ....) . This is a good thing, it points out clearly that your class is violating Single Responsibility rule . Single responsibility rule states that a class should have singe responsibility. . We can solve this by moving some of the current dependencies to a new class, and pass the new class as dependency to UserProfile. . Guess what if we do this we properly we would be re-inventing design patterns l MVVM(Nodel View ViewModel) or MVP. Whew! DI just solved Huge ViewController problem!!. . DI can easily help refactoring code to better design, in a gradual manner. . Property Injection . We seemed to have solved all our above problems, why bother with more types of injections? . If you don’t control the creation of a object, your best bet is property injection. But prefer constructor injection if that’s not the case. . Sometimes you don’t create objects of classes, some messy framework does it. For example, if you use Storyboards, you can’t do stuff like let userProfileViewController = UserProfile(multiUserModel). You would have to refactor to something like . class UserProfile: UIViewController { var userModel: UserModel! func viewDidLoad() { userNameLabel.text = userModel.name } } let multiUserSerializer = MultiUserSerializer(serializer: NSUserDefaults.standard) let multiUserModel = UserModel(serializer: multiUserSerializer) let storyboard = UIStoryboard(name: &quot;main&quot;, bundle: nil) let userProfile = storyboard.instantiateViewController(withIdentifier: &quot;UserProfile&quot;) as! UserProfile userProfile.userModel = multiUserModel . By using implicitly unwrapped Optional property, and setting it from outside, we achive the same effects of constructor injection. But is not perfect, as userModel property can be mutated from outside. Butthis is as good as it can get. . Method Injection . Method injection is just replacing instantiating inside method by one of its parameters. . // Before injection func someMethod() { let x = X() let y = x.something() + 10 return y } // After injection func someMethod(_ x: XService) { let y = x.something() + 10 return y } . Runtime Injection - Factory pattern . Sometimes you want to create a object of a particular class in runtime. But you want to use only protocol type(or a super type) instead of actual implementation (or subclass). Let’s say you need a networking service, which you set based on a user action, but as you are going to need it in runtime, you may think that you can’t inject it. . But what you can do is inject a factory Networking object or closure, that constructs it in runtime. This factory can fill the dependency of the concrete Networking class, so your class can be unaware of this. . protocol Networking { // Some methods .. } class ProxyNetworking: Networking { init(a: A) { } } class NormalNetworking: Networking { init(a: A) { } } // Injection Via Factory Class class NetworkingFactory { let a: A init(a: A) { self.a = A } func create(_ withProxy: Bool) -&gt; Networking { if (withProxy) { return ProxyNetworking(a: a) } else { return NormalNetworking(a: a) } } } class Some :UIViewController { // This has to be injected via property injection var networkingFactory: NetworkingFactory! var networking: Networking? @IBOutlet weak var proxySwitch: UISwitch! @IBAction func userTappedSubmit(sender: UIButton) { networking = networkingFactory.create(withProxy: proxySwitch.isOn) } } // Injection using closure class Some :UIViewController { // This has to be injected via property injection var networkingFactory: ((Bool) -&gt; Networking)! var networking: Networking? @IBOutlet weak var proxySwitch: UISwitch! @IBAction func userTappedSubmit(sender: UIButton) { networking = networkingFactory(proxySwitch.isOn) } } . Dependency injection - Containers &amp; Frameworks . There are Dependency Injection frameworks that make the job of dependency injection easier. You may say, “whoa Sai! wait,Do we really need a dependency injection framework as a dependency? Can’t it be done “. . When we examine what we are doing with DI, we are building a graph with our concrete types as nodes, and their dependencies linking them. If we do this completely, all dependencies will originate from a root object. . Without a framework, we will be doing a lot of copy paste coding. If our app uses networking protocol type in multiple areas, we have to type out the same concrete implementation everywhere, and fill out every dependency of networking class everywhere. . For example: . class A { var propertyInjectionVar :V! init(networking: Networking, ...) } // To create A we have to create networking and also fill out any property injection vars it needs class NetworkService: Networking { init(x: X, y: Y, z: Z) {} } // Now networking will inturn have its dependencies , which inturn have more .. // So to create A, You have to create NetworkService, X, Y, Z A(networking NetworkService(x: X(), y: Y(), z: Z())) . To make this process easier we can build something to store list of dependency type, and their concrete implementation. We will have a table of mappings. . Dependency Type Implementation type . Networking | NetworkService | . X | XService | . Y | YService | . Z | ZSubClass | . A | A | . We can register a protocol Dependency Type to concrete implementation, like Networking, X, Y to NetworkService, XService, YService respectively. Or map concrete type to its concrete implementation which can be exactly same type, like A, or subclass like mapping Z to ZSubClass. Now what the container does is when A has to be created, it auto resolves each dependency of A from the table. . You can either implement a container, or use a DI Framework which does that for you. . Containers also allow you to autofill property injection, handle lifecycle of dependencies like marking them as singleton, so that your whole container has only one object of that type created and other fancy features to make your life easy. . A key point to remember is that your classes should not depend on DI framework container ie its not good idea to pass the container to your class as a dependency. . Dip Framework - Swift . Among the DI frameworks exiting now for swift, I recommend Dip. Dip has some nifty features to make your DI pain free. . Features of Dip . Scopes. Dip supports 5 different scopes (or life cycle strategies): Unique, Shared, Singleton, EagerSingleton, WeakSingleton; | Auto-wiring &amp; Auto-injection. Dip can infer your components’ dependencies injected in constructor and automatically resolve them as well as dependencies injected with properties. | Resolving optionals. Dip is able to resolve constructor or property dependencies defined as optionals. | Type forwarding. You can register the same factory to resolve different types implemeted by a single class. | Circular dependencies. Dip will be able to resolve circular dependencies if you will follow some simple rules; | Storyboards integration. You can easily use Dip along with storyboards and Xibs without ever referencing container in your view controller’s code; | Named definitions. You can register different factories for the same protocol or type by registering them with tags; | Runtime arguments. You can register factories that accept up to 6 runtime arguments (and extend it if you need); | Easy configuration &amp; Code generation. No complex containers hierarchy, no unneeded functionality. Tired of writing all registrations by hand? There is a cool code generator that will create them for you. The only thing you need is to annotate your code with some comments. | Weakly typed components. Dip can resolve “weak” types when they are unknown at compile time. | Thread safety. Registering and resolving components is thread safe; | Helpful error messages and configuration validation. You can validate your container configuration. If something can not be resolved at runtime Dip throws an error that completely describes the issue; | . Annotations in Dip . Java has good frameworks like Dagger, and Guice that use annotations to make the job even simpler compared to swift. Dip allows you to leave annotations of dependencies in comments and also generate code for DI from it. How cool is that? . Poor man’s DI in Swift . Though I recommend the framework approach, supposing you don’t want to use framework initially and still want the to do DI, Fear not. You can use Swift’s default parameters to do constructor/method injection, and just use variable properties for property injection. You can maintain a Seperate DI singleton and fill dependencies using that. . class UserModel { static let serializer: Serializing init(_ serializer: Serializing = PoormanDIContainer.instance.getSerializer()) { // Poor man DI self.serializer = serializer } var name: String { get { return serializer.string(forKey: &quot;userName&quot;) ?? &quot;&quot; } set { serializer.setObject(newValue, forKey:&quot;userName&quot;) } } } class PoorManDIContainer { let instance = PoorManDIContainer() func getSerializer() -&gt; Serializing { // If Serializer has some dependencies, it will again use default constructor to obtain it from PoorManDIContainer return Serializer() } } . But if you use the above method, beware of circular dependencies. . Conclusion . So in conclusion DI is great. It allows you to progressively make your code better remove singletons, make your code modular, testable and also allow you to evolve good design patterns. Try it out in your existing code base, it will be one of the easiest way to refactor legacy OOP code, without modifying internal logic initially.If you have any doubts, suggestions, constructive criticisms, comment below. .",
            "url": "https://saiprasanna.in/posts/dependency-injection-what-why-and-how/",
            "relUrl": "/posts/dependency-injection-what-why-and-how/",
            "date": " • Mar 20, 2017"
        }
        
    
  
    
        ,"post10": {
            "title": "Manjaro Linux - My current daily driver arch based distro",
            "content": "The partition having arch in my secondary hard disk had died. The invisible grime which settles in your mind because of using non free windows and osX at home and work had started to bother me. . And as I got a new mechanical keyboard (Reddragon KUMARA, cheap as cherry MX patents on mechanical switches have died out) and a gaming mouse (Logitech G402) recently, I wanted to use my desktop as main programming machine, instead of my old macbook air. . . So to inevitably I was going to reinstall GNU/Linux.Choosing a new linux distro as you all know leads to paralysis due to infinite choice. . Of all the distros that I had installed, I love Arch Linux the most. Arch with its rolling update model, and the infinite repositories of aur, always hit the sweet spot. But I was feeling a bit lazy, as in Arch Linux one has to setup everything from scratch (which I recommend at least once), and for some reason my font settings was always bad in vanilla Arch Setup. I know that is not exactly an insurmountable problem, but as I said before, was feeling lazy. . After a love/hate relationship with mac OS, I really wanted a good GUI experience. I am KDE kind of a guy, and love KDE plasma environment. I shopped around a bit, and found elementaryOS based on new desktop environment called the Pantheon. Thought I would give it a shot, though it wasn’t Arch based. But eventually decided against it, because it didn’t seem very customizable, which is rather the point of elementaryOS. . So my endless distro search, ended up with deciding between variants based on arch linux. Manjaro, Antergos , ApricityOS, I eventually decided to install Manjaro, because I heard that other two are basically GUI installers of Arch, while Manjaro was a distro based on arch with customizations to make it a more smooth ride. . . Boy, Manjaro with KDE, had refreshing feel, it felt like what Linux mint was when the first time I installed it. And propriety driver support was out of the box. I would recommend Manjaro as daily development OS, where you want arch, but don’t feel like configuring it perfectly. .",
            "url": "https://saiprasanna.in/posts/Manjaro-Linux/",
            "relUrl": "/posts/Manjaro-Linux/",
            "date": " • Mar 17, 2017"
        }
        
    
  
    
        ,"post11": {
            "title": "Crashes are Optional! && Write Less, Do More",
            "content": "It was a boring tuesday/wednesday afternoon, co-worker Giridhar pinged me. He was the organizer for Swift India meetups. He invited me to give a talk. As I am in trying to doing things that I haven’t done before, I accepted readily. . About 60-66 people turned out for the meetup. It was fun and great learning experience to share what you know with others. It also exposed where I have to concentrate to develop my public speaking skills. . Here are the slides for the talk. I will try to put up the playground file later. .",
            "url": "https://saiprasanna.in/posts/crashes-are-optional/",
            "relUrl": "/posts/crashes-are-optional/",
            "date": " • Jan 29, 2017"
        }
        
    
  
    
        ,"post12": {
            "title": "Open for Collaboration, Closed for Disturbance",
            "content": "I have been working at open office environment for past 1 and a half years. The first office was a semi open one where workspaces had some degree of separation, not exactly a cubicle, honey comb like structure where there is circle of 4 people at center, and 6 or so people around. The one thing I liked about our office is that there are no corner offices for managers. Even CEO had to sit with others. Though this is my first job, I can appreciate the egalitarian and practical aspects of this. There was some degree of isolation to do deep work (though I seriously crave for more), while preserving the collaborative aspect of open office. . But after few months we moved to new workspace, things changed to the worse. It was the dreaded Complete open office plan, our building consists of floors with more than 200 people in each, and there is no separation between rows of tables. It is a complete nightmare, taking the good design to extreme and making it completely hard to bear. . This made me think about O of S.O.L.I.D principles of software design, which suggest that a good design should be open for extension and closed for modification.I think this applies allegorically to office layout plan. . Some companies have moved to ditching offices completely, but there are many still not taking such a drastic step. So the principle which I arrive on workplace design, Open for Collaboration and Closed for Disturbance. A good office design should make it accessible for people to collaborate and avoid creating unnecessary old age hierarchy bullshit that came in form of corner office perks. At the same time it should provide the much needed c isolation to get shit done. In general the design should at least have some barrier between people.It wouldn’t hurt to provide some “Deep think rooms” which can function as isolation chambers which can be opposite of meeting rooms. . Many studies are pointing to decreased level of productivity in open office plans. It is common sense that deep work requires some degree of isolation, which completely open spaces hardly provides. Every interruption completely derails the thought process which were happening till then, productivity. . Lets look at some facts,studies,opinions which point out to the decrease in productivity, happiness, and involvement in complete open office: . 1) http://blog.ninlabs.com/2013/01/programmer-interrupted/ . Based on a analysis of 10,000 programming sessions recorded from 86 programmers using Eclipse and Visual Studio and a survey of 414 programmers (Parnin:10), we found: A programmer takes between 10–15 minutes to start editing code after resuming work from an interruption. When interrupted during an edit of a method, only 10% of times did a programmer resume work in less than a minute. A programmer is likely to get just one uninterrupted 2-hour session in a day We also looked at some of the ways programmers coped with interruption: Most sessions programmers navigated to several locations to rebuild context before resuming an edit. Programmers insert intentional compile errors to force a “roadblock” reminder. A source diff is seen as a last resort way to recover state but can be cumbersome to review . 2) http://www.forbes.com/sites/davidburkus/2016/06/21/why-your-open-office-workspace-doesnt-work/ . Thus while noise was a problem, the greater noise level didn’t appear to be from all of the collective collaboration buzzing around the open room. The researchers then took their analysis one step further, using regression to calculate how important each dimension was to employees’ overall satisfaction. One of the dimensions most strongly related to overall satisfaction was ease of interaction, despite the fact that it was judged to be no better or worse in open office plans than in private offices. In other words, the desire for more collaboration among employees was shared by all, but those in open office plans may not have found it to be worth all of the stress and distraction from the bombardment of noise. . 3) http://www.newyorker.com/business/currency/the-open-office-trap . But the most problematic aspect of the open office may be physical rather than psychological: simple noise. In laboratory settings, noise has been repeatedly tiedto reduced cognitive performance. The psychologist Nick Perham, who studies the effect of sound on how we think, has found that office commotion impairsworkers’ ability to recall information, and even to do basic arithmetic. Listening to music to block out the office intrusion doesn’t help: even that, Perham found, impairs our mental acuity. Exposure to noise in an office may also take a toll on the health of employees. In a study by the Cornell University psychologists Gary Evans and Dana Johnson, clerical workers who were exposed to open-office noise for three hours had increased levels of epinephrine — a hormone that we often call adrenaline, associated with the so-called fight-or-flight response. What’s more, Evans and Johnson discovered that people in noisy environments made fewer ergonomic adjustments than they would in private, causing increased physical strain. The subjects subsequently attempted to solve fewer puzzles than they had after working in a quiet environment; in other words, they became less motivated and less creative. . 4) http://www.inc.com/geoffrey-james/why-your-company-will-benefit-from-getting-rid-of-open-office-spaces-first-90.html . They decrease productivity. Contrary to popular belief, open offices don’t increase collaboration or make people more productive. An Exeter University study showed they create a 32 percent drop in “workers’ well-being” and 15 percent reduction in productivity. .They create time-consuming distractions.Office workers lose an average of 86 minutes per day due to distractions associated with open-plan offices. As a result, many employees are “unmotivated, unproductive, and overly stressed,” according to the study funded by Steelcase. They make employees sick.A study at Queensland University of Technology’s Institute of Health and Biomedical Innovation found that working in environments without offices “caus[es] high levels of stress, conflict, high blood pressure, and a high staff turnover.” This comic describes it in a funny way on what is the cost of interruptions on programmers. This can apply to any field that involves some thinking to be done at work. . .",
            "url": "https://saiprasanna.in/posts/open-for-collaboration-closed-for-disturbance-rethinking-open-office-layout/",
            "relUrl": "/posts/open-for-collaboration-closed-for-disturbance-rethinking-open-office-layout/",
            "date": " • Jan 6, 2017"
        }
        
    
  
    
        ,"post13": {
            "title": "Function Currying, Composition in Redux Middleware",
            "content": "I am new to functional programming paradigm, though I do understand something about closures ,pure functions and have used some programming structures related to functional concepts like map, reduce, blocks in my iOS applications at work, I don’t have much experience with functional paradigm. . I started learning about react js, and eventually ended up learning about Redux. I would assume you to have basic knowledge about redux , if not read about it here. It is very well documented, and the whole api is small, If you are feeling adventurous read the redux source code, it is a few hundred lines. . Redux seems to be a more elegant implementation of Flux methodology without some components of flux like multiple stores communication via a dispatcher which is made redundant in redux pattern. . So as I was playing with redux by implementing a small application mentioned in this tutorial , I came across something called middlewares. These middleware intercept the actions to store and do some extra processing on it. I couldn’t wrap my head around how a middleware for handling Promises mentioned in that post worked. . export default function promiseMiddleware() { return next =&gt; action =&gt; { const { promise, type, ...rest } = action; if (!promise) return next(action); const SUCCESS = type; const REQUEST = type + &#39;_REQUEST&#39;; const FAILURE = type + &#39;_FAILURE&#39;; next({ ...rest, type: REQUEST }); return promise .then(res =&gt; { next({ ...rest, res, type: SUCCESS }); return true; }) .catch(error =&gt; { next({ ...rest, error, type: FAILURE }); // Another benefit is being able to log all failures here console.log(error); return false; }); }; } . It seemed to use functional magic which somehow allowed waiting for a promise to resolve and then dispatch the action or call next middleware. S o I was wondering how that worked and came across this excellent post titled “Understanding Redux Middleware”. I encourage you to check it out here if you haven’t already. This post is meant to clarify some things in it a bit further. From that article I came to understand that middleware functions got composed in the following fashion . Middleware1(Middleware2(…)) . Then by chance I was reading through documentation of redux-logger middleware which said that for it to log state , actions properly from it has to be passed as parameter after any asynchronous middleware. So from what I read in redux source for applyMiddleware I gathered that logger middleware will be a parameter for Async middleware for it to log correctly, Assume we do this , then . applyMiddleware(PromiseMiddleware, LoggerMiddleware) . will compose them in this order . PromiseMiddleware(LoggerMiddleware) . This seemed to make less sense as I wrongly thought that promise middleware will wait for LoggerMiddeware to process and then execute. . But after meditating on the redux source code ,creating curried functions and composing them , I got it. The actual execution order of middleware when dispatch action occurs, starts from PromiseMiddleware, which resolves the promise and only then calls the LoggerMiddleware. . This is because LoggerMiddleware is a curried function , so it doesn’t return any value and it is simply a function that is passed to Promisemiddleware. The Promisemiddleware takes up the action, resolves it, and inside “then” of promise calls the LoggerMiddleware using . next({ ...rest, res, type: SUCCESS }); . and hence passes the action to LoggerMiddleware Function. The PromiseMiddleware can refer to the next function inside it using the properly named param “next”. Now lets make some curried functions with asynchronous operations and uncurry them with parameters . Type these in js console We are first creating a curried function that takes another function as input and also some params. . var asyncFunction = nextFunction =&gt; params =&gt; window.setTimeout( function() { alert(&quot;In Async Callback&quot;); nextFunction(params); }, 1000); . This is similar to the promise middleware. asyncFunction takes in parameter a function called nextFunction. From the signature of nextFunction(params) used inside , we can get it that it simply takes a object. Now let us create a function to pass into async . var alertFunction = parameters =&gt; alert(parameter); . alertFunction simply alerts the parameter passed to it. COMPOSING Alert and asyncFunction manually, and calling the result of composition . var composedFunction = asyncFunction(alertFunction); composedFuntion(1); . Output Alert:- In Async Callback Alert:- 1 . VOILA! Our alertFunction executes inside the async callback To clarify replacing the nextFunction with body of alertFunction . function params =&gt; window.setTimeout(function() { alert(&quot;In Async Handler&quot;); // nextFunction(params) becomes alert(params); }, 1000) . I couldn’t grok async code in middleware because I was making similar mistake as in thinking that as alertFunction is innermost function in composition and so its body would execute first.But in actuality it doesn’t evaluate to value and hence the order of execution starts from outermost function . . This seems trivial after seeing the above example but it wrecked my understanding of middleware code. if G is a function and F is higher order function , ie F(someInputFunction) is also a function , then when you “uncurry” by calling F(G)(x) the evaluation begins from operations defined inside/ to perform F, and operations defined inside/to perform G may or may not be used by F. . So we can wrapping asynchronous operations in a chain of composed functions, define them in seemingly synchronous way. And I think this is how Promises are actually implemented, will have to read about that later. . Anyway this really excites me, how mind bending functional jui jutsu can accomplish a lot with less code. Will post more when I learn more.. Eager to hear your experience in learning functional programming inception. . Reposted from medium.com .",
            "url": "https://saiprasanna.in/posts/function-currying-and-composition/",
            "relUrl": "/posts/function-currying-and-composition/",
            "date": " • Jul 10, 2016"
        }
        
    
  
    
        ,"post14": {
            "title": "Creation and Consumption",
            "content": "The amount of content available for consumption in our digital age is staggering. Movies, TV, video games, videos, blogs, emails, chat messages all compete for our limited time. . I think mindful consumption is of great importance. We can start being aware of what our consumption inlets are. Just the awareness of this can create great impact on how we choose to spend our time. . I am not arguing against consuming media, but for having healthy awareness on wether it has negative effects on one’s creative spirit. It is important in my opinion to keep the act of creation balanced with consumption. . To contemplate, and engage actively with the world through some outlet is a form of habit I hope we can get into. This outlet can be anything, writing, music, coding for open source, some personal project etc. . I think it is more fulfilling to move towards a balanced consumption to creation ratio. .",
            "url": "https://saiprasanna.in/posts/creation-and-consumption/",
            "relUrl": "/posts/creation-and-consumption/",
            "date": " • Jul 10, 2016"
        }
        
    
  
    
        ,"post15": {
            "title": "Speed up iOS dev using XCode Injection Plugin",
            "content": "Injection for XCode. . . One of the most boring/unproductive part of any development cycle is waiting for your project to compile. And if you have done iOS development, you know how much time is wasted for XCode to recompile the project. And even if compile time is less, you have to follow a bunch of taps, long presses etc to get to the desired app state before even testing your changes. This plugin will reduces reduce these steps considerably. As you change your code , you can inject the new class definition using this XCode plugin. It recompiles just changed file, and injects it into the live running app. The great thing is it works for real device as well. . Set up . Install Alcatraz package manager for XCode . Alcatraz allows you to install and remove XCode plugins hazzle free | Open using Alcatraz with package manager option in projects menu. | Search for injection plugin, and install it. Restart XCode after installations | To make it work in real device you need to click on Product -&gt; Injection Plugin -&gt; Patch Project for Injection. It will add couple of lines to your main.m of your project. If it is a swift project just create a empty main.m and do the above. | Inject Code . Run your project, make some changes to the code in a file, press ^ + = . | Your changed code in that file will get compiled and injected into the app live. | Now you can simply have to somehow make your program create new object of the changed class to see the changes. For example, tap back button and then again go the view. | There are few limitations on what can be injected, refer the Injection for XCode’s github project. And a small limitation currently is it wont work if you have more than 128 source files in your project due to a XCode limitation. Follow this and other issues in github issue tracker. | You can also set it up to automatically inject changes by enabling File Watcher in Product-&gt; Injection Plugin -&gt; Tunable App Parameters | If you want it to make changes visible as you inject the changes in you have to modify your normal code. You have to listen for callbacks after injection , and reload the view or do something else.Read the instructions. | This plugin will add a new folder to your project which you can add to .gitignore to avoid source control. . It is great that the developer has open sourced it. To know how it does its magic see here The author has released it under “nagware” license, where he requests you to pay after using it for two weeks. .",
            "url": "https://saiprasanna.in/posts/speed-up-ios-development/",
            "relUrl": "/posts/speed-up-ios-development/",
            "date": " • Apr 17, 2016"
        }
        
    
  
    
        ,"post16": {
            "title": "MIT OCW 6.006 Algorithms Course",
            "content": "MIT OCW course is one among the best introductory algorithm courses online. It introduces techniques to analyse, and understand how algorithms work. . MIT OCW 6.006 Introduction to Algorithms Fall 2011 course is packed with awesome content. The open course ware website contains video recording of the MIT course conducted at MIT and all the accompanying notes, assignments, test content. It does not contain interactive content like udacity courses, but the content is in different level.You need to know python to do this course’s assignments. If you know how to code in any language , you can pick up basics of python in a day or two as it has simple syntax. . Prof. Erik Demaine ,Prof. Srinivas Devadas do a great job introducing concepts of algorithms and its analysis in the main lectures. Victor Costan does exemplary job in recitation videos in explaining the concepts introduced by Erik and Srini in easily understandable way. So don’t miss out on recitation if you plan to look into this course. . The assignments are one of the fun parts of the whole course. They allow you to see how efficient algorithms can really make a difference in running time of your code. These guys have done a awesome job in designing each assignment in such a way that you can visualise the impact of algorithms. For example in assignment 3 where you are supposed to write code to detect crossings among wires, they have designed the assignment in such a way that you can see the result of your code running in the browser. . The course videos are also available in youtube as playlist. Checkout the first video. . .",
            "url": "https://saiprasanna.in/posts/MIT-OCW-Algorithms/",
            "relUrl": "/posts/MIT-OCW-Algorithms/",
            "date": " • Apr 2, 2016"
        }
        
    
  
    
        ,"post17": {
            "title": "Blog using Github pages",
            "content": "Github.com allows us to upload just templates, config and theme for jekyll static blog generator, and it generates a static website for you. . Now you can just write a markdown file, and commit it, or even create a file online even in github’s source browser, and it is automatically generated into a blog post just after the commit is made. . I didn’t notice this feature in github.com before. I thought we had to run jekyll locally to build static site, and commit it for github to serve stuff. So rather went for python static blog generator called pelican, but gave up because of it being a pain to generate blog every time. I even tried to setup travis-ci to automagically generate the blog, but it was slow and was still a pain. . So if you want to create a blog, just use github , it is quite easy to setup. clone a theme, edit _config.yml and blog on. .",
            "url": "https://saiprasanna.in/posts/new-blog/",
            "relUrl": "/posts/new-blog/",
            "date": " • Mar 22, 2016"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "&lt;img src=&quot;/assets/images/bio-photo.jpg&quot;, style=&quot;display: block;margin-left: auto; margin-right: auto; border-radius:50%&quot; width=&quot;200&quot; height=&quot;200&quot;/&gt; . I’m Sai Prasanna. I work as a machine learning engineer focusing currently on NLP applications. I also do academic research in machine learning. . Research Interests . Natural Language Processing | Reinforcement Learning | Representation Learning | Semi-Supervised Learning | Interpretability | Grammatical Error Correction | Language Models | Vector Search | Machine Learning | .",
          "url": "https://saiprasanna.in/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://saiprasanna.in/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}